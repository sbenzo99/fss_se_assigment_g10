2025-10-03|||v4.57.0 Branch (#41310)
2025-09-11|||Push generation config along with checkpoints (#40804)
2025-09-11|||feature: Add robust token counting with padding exclusion  (#40416)
2025-09-10|||Move num_items_in_batch to correct device before accelerator.gather (#40773)
2025-09-08|||Fix np array typing (#40741)
2025-09-05|||Fix arguments (#40605)
2025-09-04|||Fix backward compatibility with accelerate in Trainer (#40668)
2025-09-03|||[CP] Add attention_mask to the buffer when the mask is causal  (#40619)
2025-08-26|||[Trainer] accelerate contextparallel support in trainer (#40205)
2025-08-25|||Fix label smoothing incompatibility with multi-label classification (#40296)
2025-08-21|||Fix: Only call Trainer.align_special_tokens if model has "config" attribute (#40322)
2025-08-20|||[`fix`] Pass adamw optimizer parameters to StableAdamW (#40184)
2025-08-15|||Fix typos (#40175)
2025-08-14|||Replace `self.tokenizer` by `self.processing_class` (#40119)
2025-08-13|||[trainer] handle case where EOS token is None in `generation_config` (#40127)
2025-08-12|||[trainer] ensure special tokens in model configs are aligned with tokenizer at train time (#38441)
2025-08-11|||guard on model.eval when using torch.compile + FSDP2 (#37413)
2025-08-05|||Fix eval thread fork bomb (#39717)
2025-08-03|||Refactor label name handling for PEFT models in Trainer class (#39265)
2025-07-30|||Simplify conditional code (#39781)
2025-07-29|||fix(trainer): Correct loss scaling for incomplete gradient accumulation steps (#39659)
2025-07-29|||Apply several ruff SIM rules   (#37283)
2025-07-28|||PATCH: add back n-dim device-mesh + fix tp trainer saving (#39693)
2025-07-25|||Add ep (#39501)
2025-07-23|||Fix typos and grammar issues in documentation and code (#39598)
2025-07-21|||Raise `TypeError` instead of ValueError for invalid types (#38660)
2025-07-16|||Fixes #39204: add fallback if get_base_model missing (#39226)
2025-07-16|||make the loss context manager easier to extend (#39321)
2025-07-16|||Add StableAdamW Optimizer  (#39446)
2025-07-07|||fix bug using FSDP V1 will lead to model device not properly set (#39177)
2025-07-05|||Refactor the way we handle outputs for new llamas and new models (#39120)
2025-07-03|||when delaying optimizer creation only prepare the model (#39152)
2025-06-27|||Fix: unprotected import of tp plugin (#39083)
2025-06-25|||fix gemma3 grad acc (#37208)
2025-06-20|||Remove `ALL_LAYERNORM_LAYERS` (#38922)
2025-06-19|||feat: add flexible Liger Kernel configuration to TrainingArguments (#38911)
2025-06-19|||Add support for specifying revisions when pushing to Hub via internal Trainer call (#36852)
2025-06-18|||More PYUP fixes (#38883)
2025-06-17|||No more Tuple, List, Dict (#38797)
2025-06-13|||Fix trainer.py not showing signature columns (#38465)
2025-06-13|||Use HF papers (#38184)
2025-06-11|||Better typing for num_items_in_batch (#38728)
2025-06-07|||remove ipex_optimize_model usage (#38632)
2025-06-06|||fix total batch size calculation in trainer (#38286)
2025-06-06|||fix: support grad clipping for TP through replicating non-sharded modules (#36132)
2025-06-05|||Remove `isort` from dependencies (#38616)
2025-06-02|||Fix amp deprecation issue (#38100)
2025-05-20|||Make `train_dataset` attribute in `_get_train_sampler` optional  (#38226)
2025-05-19|||Add adam_kwargs for Apollo Optimizer (#38168)
2025-05-19|||Refactor `get_XXX_dataloader` from Trainer (#38090)
2025-05-17|||fix bug in distributed loss test (#38166)
2025-05-16|||Fix import torchao.prototype.low_bit_optim since torchao  v0.11 (#38174)
2025-05-15|||[FIX] Save speed metrics to logs (#38136)
2025-05-12|||update seed_worker to set seed based on worker_id and rank (#37980)
2025-05-12|||Fix tot update in trainer (#37923)
2025-05-06|||fix FSDP + torch.compile bug when saving pretrained model  (#37725)
2025-04-30|||make sure lr is not a tensor (#37881)
2025-04-29|||Revert change that breaks on Torch 2.1 (#37531)
2025-04-28|||change XLA deprecated api (#37741)
2025-04-25|||Force torch>=2.6 with torch.load to avoid vulnerability issue (#37785)
2025-04-25|||Fix typos in strings and comments (#37784)
2025-04-24|||Fix load of rng state for resuming training from checkpoint (#37162)
2025-04-24|||fix: learning_rate logged as tensor causing save issue with deepspeed (#37704)
2025-04-22|||[fix] make legacy bnb code work (#37331)
2025-04-16|||Make Ignored Columns ValueError More Informative (#33299)
2025-04-11|||prevent creating a view/leaf param for low rank optimizers w FSDP (#37379)
2025-04-11|||Simplify soft dependencies and update the dummy-creation process (#36827)
2025-04-11|||Remove old code for  PyTorch,  Accelerator and tokenizers (#37234)
2025-04-10|||(Part 2) feat: allow for tp_size attr for tplizing the model (#37054)
2025-04-10|||fix: use mtime by default in Trainer._rotate_checkpoints with automatic fallback (#37260)
2025-04-09|||Apply torchfix to replace deprecated functions: `_pytree._register_pytree_node` and `torch.cpu.amp.autocast` (#37372)
2025-04-07|||byebye torch 2.0 (#37277)
2025-04-04|||Disable delay_optimizer_creation in `Trainer` to support fsdp2 (#37147)
2025-04-03|||Add new dim to `num_items_in_batch` if necessary (#36967)
2025-04-02|||Merge tensor operations with device transfer operations (#37097)
2025-04-01|||Only count num items in batch when needed (#36867)
2025-03-31|||Fix more inefficient PT operations (#37060)
2025-03-27|||Set weights_only in torch.load (#36991)
2025-03-27|||Fix typing for None valued variables (#37004)
2025-03-26|||Log the correct learning rate (#36973)
2025-03-21|||Remove call to `.item` in `get_batch_samples` (#36861)
2025-03-21|||FIX FSDP plugin update for QLoRA (#36720)
2025-03-20|||Use pyupgrade --py39-plus to improve code (#36843)
2025-03-20|||Saving `Trainer.collator.tokenizer` in when `Trainer.processing_class` is `None` (#36552)
2025-03-19|||Just import torch AdamW instead (#36177)
2025-03-14|||Fix/best model checkpoint fix (#35885)
2025-03-13|||fix: fsdp sharded state dict wont work for save_only_model knob (#36627)
2025-03-12|||HPU support (#36424)
2025-03-06|||chore: enhance message descriptions in parameters,comments,logs and docstrings (#36554)
2025-02-26|||fix: prevent model access error during Optuna hyperparameter tuning (#36395)
2025-02-25|||Update _get_eval_sampler to reflect Trainer.tokenizer is deprecation  self.tokenizer -> self.processing_class (#36315)
2025-02-19|||Fix callback handler reference (#36250)
2025-02-18|||feat: add support for tensor parallel training workflow with accelerate (#34194)
2025-02-14|||Fix the key name for _load_rng_state under torch.cuda (#36138)
2025-02-13|||fix training issues (#36158)
2025-02-12|||adding option to save/reload scaler (#34932)
2025-02-12|||Fix multi gpu loss sync condition, add doc and test (#35743)
2025-02-12||| Optim: APOLLO optimizer integration (#36062)
2025-02-12|||added warning to Trainer when label_names is not specified for PeftModel (#32085)
2025-02-12|||add RAdamScheduleFree optimizer (#35313)
2025-02-10|||Revert checkpoint tmp dir (#36112)
2025-02-07|||Fix OS err (#36094)
2025-02-06|||Save checkpoint to temporary directory to handle partial saves during failures (#35580)
2025-02-04|||layernorm_decay_fix (#35927)
2025-01-29|||fix iterator overflow when gradient accumulation is 1 (#35960)
2025-01-29|||Trainer Refactor: Part 1 (#35567)
2025-01-23|||Fix GA loss for Deepspeed (#35808)
2025-01-21|||Support adamw_torch_8bit (#34993)
2025-01-17|||Added liger_kernel compatibility with `PeftModel` (#35680)
2025-01-16|||Fix the bug that `Trainer` cannot correctly call `torch_jit_model_eval` (#35722)
2025-01-16|||Fix condition when GA loss bug fix is not performed (#35651)
2025-01-09|||Update doc for `metric_for_best_model` when `save_strategy="best"`. (#35389)
2025-01-08|||Pass correct `num_items_in_batch` value into the training_step function (#35438)
2025-01-06|||Add check for if num_items_in_batch is not None (#35102)
2024-12-27|||Fix `model_accepts_loss_kwargs` for timm model (#35257)
2024-12-26|||Drop inplace operation for loss computation with gradient accumulation (#35416)
2024-12-23|||Scale loss before backward (#35207)
2024-12-20|||Aurevoir PyTorch 1 (#35358)
2024-12-13|||don't use no_sync when deepspeed doesn't support it for certain zero stages (#35157)
2024-12-13|||Fix FSDP no longer working (#35212)
2024-12-11|||[PEFT] Better Trainer error when prompt learning with loading best model at the end (#35087)
2024-12-10|||Fix `num_items_in_batch` not being an integer (#35115)
2024-12-09|||Fix GA loss bugs and add unit test (#35121)
2024-12-05|||[`trainer`] fix the GA `model_accepts_loss_kwargs` (#34915)
2024-12-02|||add docstring example for compute_loss_func (#35020)
2024-11-29|||fix: double verbs (#35008)
2024-11-28|||Allow compressed-tensors quantized model to be trained (#34520)
2024-11-25|||Sum gathered input tokens (#34554)
2024-11-25|||Add safe_globals to resume training on PyTorch 2.6 (#34632)
2024-11-21|||Change logging level from warning to info for `max_steps` overriding `num_train_epochs` (#34810)
2024-11-20|||Fix hyperparameter search when optuna+deepseed (#34642)
2024-11-19|||Feature: print tokens per second during training (#34507)
2024-11-19|||Trainer hyperparameter search kwargs docs update (#34459)
2024-11-15|||Remove FSDP wrapping from sub-models. (#34452)
2024-11-15|||FSDP grad accum fix (#34645)
2024-11-04|||Update trainer for easier handling of accumulate, compile fixes, and proper reporting (#34511)
2024-10-31|||Fix step shifting when accumulate gradient (#33673)
2024-10-29|||Adding `optimizer_cls_and_kwargs` to `Trainer.__init__` (#34358)
2024-10-29|||enable average tokens across devices (#34373)
2024-10-29|||New option called `"best"` for `args.save_strategy`. (#31817)
2024-10-28|||exclude fsdp from delay_optimizer_creation (#34140)
2024-10-28|||Fix batch size handling in prediction_loop for DataLoaderShard (#34343)
2024-10-23|||Enable Gradient Accumulation fix across all models + trainer fully in forward() (#34283)
2024-10-23|||fix error in _get_eval_sampler when group_by_length enabled (#34237)
2024-10-17|||Enable users to use their own loss functions + deal with prefetching for grad accum (#34198)
2024-10-17|||Update `trainer._get_eval_sampler()` to support `group_by_length` arg (#33514)
2024-10-16|||Revert "Fix FSDP resume Initialization issue" (#34193)
2024-10-15|||Fix FSDP resume Initialization issue (#34032)
2024-10-10|||Default `synced_gpus` to `True` when using `FullyShardedDataParallel` (#33483)
2024-10-10|||Fix data_seed unused (#33731)
2024-10-08|||Fixup DeepSpeed things (#34007)
2024-10-07|||Enable customized optimizer for DeepSpeed (#32049)
2024-10-03|||add setter for trainer processor (#33911)
2024-10-03|||Update an keyerror on _save_check_point prevent confusion of missing â€¦ (#33832)
2024-10-02|||Handle Trainer `tokenizer` kwarg deprecation with decorator (#33887)
2024-10-02|||Trainer - deprecate tokenizer for processing_class (#32385)
2024-10-01|||Add include_loss_for_metrics (#33088)
2024-10-01|||Validate the eval dataset in advance. (#33743)
2024-09-27|||fix trainer tr_loss add error (#33651)
2024-09-25|||Add AdEMAMix optimizer (#33682)
2024-09-17|||Add revision to trainer push_to_hub (#33482)
2024-09-16|||Updated Trainer's liger-kernel integration to call correct patching API (#33502)
2024-09-13|||add a callback hook right before the optimizer step (#33444)
2024-09-09|||schedulefree optimizers (#30079)
2024-09-04|||Fix: multigpu training (#33271)
2024-09-03|||Only disallow DeepSpeed Zero-3 for auto bs finder (#31731)
2024-09-02|||docs: Replace package abbreviations with full name(`bitsandbytes`) in docstrings (#33230)
2024-09-02|||Fix: Suppressed 'use_reentrant=False' warning (#33208)
2024-08-27|||update torch req for 4-bit optimizer (#33144)
2024-08-27|||Update stateful_callbacks state before saving checkpoint (#32115)
2024-08-23|||Integrate Liger (Linkedin GPU Efficient Runtime) Kernel to Trainer (#32860)
2024-08-22|||FEAT / Trainer: Add adamw 4bit optimizer (#31865)
2024-08-20|||ðŸš¨ðŸš¨ðŸš¨ Update min version of accelerate to 0.26.0 (#32627)
2024-08-19|||Support save/load ckpt for XLA FSDP (#32311)
2024-08-16|||Revert PR 32299, flag users when Zero-3 was missed (#32851)
2024-08-14|||Support MUSA (Moore Threads GPU) backend in transformers (#31913)
2024-08-13|||Add support for GrokAdamW optimizer (#32521)
2024-08-12|||"to be not" -> "not to be" (#32636)
2024-08-06|||Migrate import checks not need accelerate, and be more clear on min versions (#32292)
2024-08-01|||Yell at the user if zero-3 init wasn't performed, but expected to have been done (#32299)
2024-07-30|||Docs: formatting nits (#32247)
2024-07-30|||Cast epochs_trained to int when resuming training (#32286)
2024-07-16|||Fix gather when collecting 'num_input_tokens_seen' (#31974)
2024-07-11|||Allow `Trainer.get_optimizer_cls_and_kwargs` to be overridden (#31875)
2024-07-10|||Push sharded checkpoint to hub when `push_to_hub=True` in `TrainingArguments` (#31808)
2024-07-05|||Exclude torch.compile time from metrics computation (#31443)
2024-07-05|||Add torch_empty_cache_steps to TrainingArguments (#31546)
2024-07-03|||[fix bug] logits's shape different from label's shape in preprocess_logits_for_metrics (#31447)
2024-07-03|||Add ignore_errors=True to trainer.py rmtree in _inner_training_loop (#31668)
2024-06-28|||add gather_use_object arguments (#31514)
2024-06-21|||Removed torch.cuda.empty_cache from train loop. (#31530)
2024-06-19|||Add valid columns check in _remove_unused_columns method (#31466)
2024-06-18|||Give more useful `metric_for_best_model` errors (#31450)
2024-06-17|||Support multiple validation datasets when `dataloader_persistent_workers=True` (#30627)
2024-06-12|||[Bug Fix] Renamed loss to losses to suppress UnboundLocalError (#31365)
2024-06-06|||Fix _save_tpu: use _maybe_convert_to_cpu instead of to cpu. (#31264)
2024-06-04|||Add new line switch before logging ***** Running {description} ***** (#31225)
2024-06-03|||Rename sanity_evaluation to eval_on_start (#31192)
2024-05-31|||[trainer] add sanity evaluation option  (#31146)
2024-05-29|||Add on_optimizer_step to callback options (#31095)
2024-05-28|||Fix typo in trainer.py (#31048)
2024-05-21|||FEAT / Trainer: LOMO optimizer support (#30178)
2024-05-14|||[T5] Adding `model_parallel = False` to `T5ForTokenClassification` and `MT5ForTokenClassification` (#30763)
2024-05-14|||PEFT: Access active_adapters as a property in Trainer (#30790)
2024-05-10|||PEFT / Trainer: Make use of `model.active_adapters()` instead of deprecated `model.active_adapter` whenever possible (#30738)
2024-05-06|||Trainer - add cache clearing and the option for batched eval metrics computation (#28769)
2024-05-06|||Trainer._load_from_checkpoint - support loading multiple Peft adapters (#30505)
2024-04-29|||Include safetensors as part of `_load_best_model` (#30553)
2024-04-26|||Remove skipping logic now that set_epoch exists (#30501)
2024-04-25|||Introduce Stateful Callbacks (#29666)
2024-04-24|||Non blocking support to torch DL's (#30465)
2024-04-24|||Enable fp16 on CPU (#30459)
2024-04-24|||Neuron: When save_safetensor=False, no need to move model to CPU (#29703)
2024-04-23|||Fix use_cache for xla fsdp (#30353)
2024-04-19|||Update unwrap from accelerate (#29933)
2024-04-17|||Add strategy to store results in evaluation loop (#30267)
2024-04-15|||round epoch only in console (#30237)
2024-04-11|||Guard XLA version imports (#30167)
2024-04-10|||Fix accelerate kwargs for versions <0.28.0 (#30086)
2024-04-09|||[Trainer] Undo #29896 (#30129)
2024-04-09|||[Trainer] Fix default data collator (#30142)
2024-04-09|||Add datasets.Dataset to Trainer's train_dataset and eval_dataset type hints (#30077)
2024-04-08|||Trainer / Core : Do not change init signature order (#30126)
2024-04-08|||Accept token in trainer.push_to_hub() (#30093)
2024-04-08|||Change log level to warning for num_train_epochs override (#30014)
2024-04-05|||[Trainer] Allow passing image processor (#29896)
2024-03-30|||Rework tests to compare trainer checkpoint args (#29883)
2024-03-28|||Allow GradientAccumulationPlugin to be configured from AcceleratorConfig (#29589)
2024-03-28|||add functions to inspect model and optimizer status to trainer.py (#29838)
2024-03-27|||add Cambricon MLUs support (#29627)
2024-03-26|||Add warnings if training args differ from checkpoint trainer state (#29255)
2024-03-25|||Fix the behavior of collecting 'num_input_tokens_seen' (#29099)
2024-03-22|||Fix type hint for train_dataset param of Trainer.__init__() to allow IterableDataset.  Issue 29678 (#29738)
2024-03-21|||Silence deprecations and use the DataLoaderConfig (#29779)
2024-03-20|||fix galore layerwise with frozen params (#29743)
2024-03-20|||fixed the issue of DPO trainer that using one node and mutiple GPUs and set the device_map='auto' (#29695)
2024-03-19|||FEAT / Optim: Add GaLore optimizer (#29588)
2024-03-14|||Fix TPU checkpointing inside Trainer (#29657)
2024-03-14|||[`PEFT`] Fix `save_pretrained` to make sure adapters weights are also saved on TPU (#29388)
2024-03-13|||Add support for FSDP+QLoRA and DeepSpeed ZeRO3+QLoRA (#29587)
2024-03-12|||fix error: TypeError: Object of type Tensor is not JSON serializable â€¦ (#29568)
2024-03-11|||Make torch xla available on GPU (#29334)
2024-03-08|||Fix eval thread fork bomb (#29538)
2024-03-07|||fix: Avoid error when fsdp_config is missing xla_fsdp_v2 (#29480)
2024-03-06|||Fix test failure on DeepSpeed (#29444)
2024-03-04|||Fix grad_norm unserializable tensor log failure (#29212)
2024-03-04|||ðŸš¨ Fully revert atomic checkpointing ðŸš¨ (#29370)
2024-03-04|||Workaround for #27758 to avoid ZeroDivisionError (#28756)
2024-03-01|||Fix deprecated arg issue (#29372)
2024-02-26|||Add feature extraction mapping for automatic metadata update (#28944)
2024-02-20|||FIX [`PEFT` / `Trainer` ] Handle better peft + quantized compiled models (#29055)
2024-02-20|||FEAT [`Trainer` / `bnb`]: Add RMSProp from `bitsandbytes` to HF `Trainer` (#29082)
2024-02-19|||storing & logging gradient norm in trainer (#27326)
2024-02-16|||`auto_find_batch_size` isn't yet supported with DeepSpeed/FSDP. Raise error accrodingly. (#29058)
2024-02-14|||FIX [`Trainer` / tags]: Fix trainer + tags when users do not pass `"tags"` to `trainer.push_to_hub()` (#29009)
2024-02-14|||[TPU] Support PyTorch/XLA FSDP via SPMD (#28949)
2024-02-14|||Introduce AcceleratorConfig dataclass (#28664)
2024-02-14|||ENH [`AutoQuantizer`]: enhance trainer + not supported quant methods (#28991)
2024-02-12|||Clean up staging tmp checkpoint directory (#28848)
2024-02-06|||Raise error when using `save_only_model` with `load_best_model_at_end` for DeepSpeed/FSDP (#28866)
2024-02-05|||Do not use mtime for checkpoint rotation. (#28862)
2024-02-02|||Reduce GPU memory usage when using FSDP+PEFT (#28830)
2024-01-31|||Resolve DeepSpeed cannot resume training with PeftModel (#28746)
2024-01-29|||Support saving only PEFT adapter in checkpoints when using PEFT + FSDP (#28297)
2024-01-26|||Fix `weights_only` (#28725)
2024-01-26|||support PeftMixedModel signature inspect (#28321)
2024-01-25|||[`chore`] Add missing space in warning (#28695)
2024-01-24|||Use save_safetensor to disable safe serialization for XLA (#28669)
2024-01-23|||add dataloader prefetch factor in training args and trainer (#28498)
2024-01-23|||Fix windows err with checkpoint race conditions (#28637)
2024-01-18|||Use `weights_only` only if torch >= 1.13 (#28506)
2024-01-15|||[`core`/ FEAT] Add the possibility to push custom tags using `PreTrainedModel` itself (#28405)
2024-01-11|||Byebye torch 1.10 (#28207)
2024-01-10|||Fix for checkpoint rename race condition (#28364)
2024-01-10|||Support `DeepSpeed` when using auto find batch size (#28088)
2024-01-02|||fix bug:divide by zero in _maybe_log_save_evaluate() (#28251)
2024-01-02|||Fix trainer saving safetensors: metadata is None (#28219)
2023-12-20|||move code to Trainer.evaluate to enable use of that function with multiple datasets (#27844)
2023-12-19|||When save a model on TPU, make a copy to be moved to CPU (#27993)
2023-12-18|||in peft finetune, only the trainable parameters need to be saved (#27825)
2023-12-16|||fix resuming from ckpt when using FSDP with FULL_STATE_DICT (#27891)
2023-12-16|||Fix bug for checkpoint saving on multi node training setting (#28078)
2023-12-15|||make torch.load a bit safer (#27282)
2023-12-13|||Fix bug with rotating checkpoints (#28009)
2023-12-12|||Support PeftModel signature inspect (#27865)
2023-12-09|||[integration] Update Ray Tune integration for Ray 2.7 (#26499)
2023-12-08|||Allow `resume_from_checkpoint` to handle `auto_find_batch_size` (#27568)
2023-12-08|||fix: non-atomic checkpoint save (#27820)
2023-12-07|||update `create_model_card` to properly save peft details when using Trainer with PEFT (#27754)
2023-12-04|||[Hot-Fix][XLA] Re-enable broken _tpu_save for XLATensors (#27799)
2023-12-04|||Add `persistent_workers` parameter to `TrainingArguments` (#27189)
2023-11-28|||Docs: Fix broken cross-references, i.e. `~transformer.` -> `~transformers.` (#27740)
2023-11-28|||Fixed passing scheduler-specific kwargs via TrainingArguments lr_scheduler_kwargs (#27595)
2023-11-28|||docs: replace torch.distributed.run by torchrun (#27528)
2023-11-24|||Refactoring Trainer, adds `save_only_model` arg and simplifying FSDP integration (#27652)
2023-11-22|||remove the deprecated method `init_git_repo` (#27617)
2023-11-14|||Track the number of tokens seen to metrics (#27274)
2023-11-14|||Have seq2seq just use gather (#27025)
2023-11-08|||Allow scheduler parameters (#26480)
2023-11-02|||Fixed base model class name extraction from PeftModels (#27162)
2023-11-02|||Reproducible checkpoint for npu (#27208)
2023-11-01|||Enable split_batches through TrainingArguments (#26798)
2023-10-31|||[FEAT] Add Neftune into transformers Trainer (#27141)
2023-10-30|||remove the obsolete code related to fairscale FSDP (#26651)
2023-10-30|||[`Trainer` / `GC`] Add `gradient_checkpointing_kwargs` in trainer and training arguments (#27068)
2023-10-26|||Save TB logs as part of push_to_hub (#27022)
2023-10-26|||Correct docstrings and a typo in comments (#27047)
2023-10-26|||Bring back `set_epoch` for Accelerate-based dataloaders (#26850)
2023-10-16|||fix resume_from_checkpoint bug (#26739)
2023-10-12|||Add many missing spaces in adjacent strings (#26751)
2023-10-06|||remove SharedDDP as it is deprecated (#25702)
2023-10-04|||Docstring check (#26052)
2023-10-04|||Extend Trainer to enable Ascend NPU to use the fused Adamw optimizer when training (#26194)
2023-09-26|||Add torch `RMSProp` optimizer (#26425)
2023-09-22|||[QUICK FIX LINK] Update trainer.py (#26293)
2023-09-20|||[`Trainer`] Refactor trainer + bnb logic (#26248)
2023-09-20|||FSDP tests and checkpointing fixes (#26180)
2023-09-18|||refactor decay_parameters production into its own function (#26152)
2023-09-14|||Fix eval accumulation when `accelerate` > 0.20.3 (#26060)
2023-09-13|||enable optuna multi-objectives feature (#25969)
2023-09-11|||only main process should call _save on deepspeed zero3 (#25959)
2023-09-08|||Try to fix training Loss inconsistent after resume from old checkpoint (#25872)
2023-09-08|||Add `tgs` speed metrics (#25858)
2023-09-07|||Fix err with FSDP (#25991)
2023-09-05|||deepspeed resume from ckpt fixes and adding support for deepspeed optimizer and HF scheduler (#25863)
2023-09-01|||Revert frozen training arguments (#25903)
2023-09-01|||fix FSDP model resume optimizer & scheduler (#25852)
2023-08-31|||fix ds z3 checkpointing when  `stage3_gather_16bit_weights_on_model_save=False` (#25817)
2023-08-29|||Error with checking args.eval_accumulation_steps to gather tensors (#25819)
2023-08-29|||Arde/fsdp activation checkpointing (#25771)
2023-08-25|||ðŸš¨ðŸš¨ðŸš¨ [`Refactor`] Move third-party related utility files into `integrations/` folder ðŸš¨ðŸš¨ðŸš¨ (#25599)
2023-08-18|||[`PEFT`] Peft integration alternative design  (#25077)
2023-08-17|||add warning for 8bit optimizers (#25575)
2023-08-17|||add util for ram efficient loading of model when using fsdp (#25107)
2023-08-17|||Revert "change version (#25387)" (#25573)
2023-08-16|||Update trainer.py (#25553)
2023-08-16|||More frozen args (#25540)
2023-08-10|||GPTQ integration (#25062)
2023-08-10|||Fix issue with ratio evaluation steps and auto find batch size (#25436)
2023-08-09|||rm useless condition since the previous condition contains it. (#25403)
2023-08-08|||change version (#25387)
2023-08-07|||Migrate Trainer from `Repository` to `upload_folder` (#25095)
2023-08-02|||Fix set of model parallel in the Trainer when no GPUs are available (#25239)
2023-07-28|||Fix `.push_to_hub` and cleanup `get_full_repo_name` usage (#25120)
2023-07-27|||fix delete all checkpoints when save_total_limit is set to 1 (#25136)
2023-07-27|||fix deepspeed load best model at end when the model gets sharded (#25057)
2023-07-24|||compute_loss in trainer failing to label shift for PEFT model when label smoothing enabled. (#25044)
2023-07-24|||Add dispatch_batches to training arguments (#25038)
2023-07-21|||Use main_input_name for include_inputs_for_metrics (#24993)
2023-07-21|||fsdp fixes and enhancements (#24980)
2023-07-21|||fix fsdp checkpointing issues (#24926)
2023-07-18|||Remove deprecated codes (#24837)
2023-07-12|||Rm duplicate pad_across_processes (#24780)
2023-07-12|||Fix pad across processes dim in trainer and not being able to set the timeout (#24775)
2023-07-12|||Fix eval_accumulation_steps leading to incorrect metrics (#24756)
2023-07-11|||Fix lr scheduler not being reset on reruns (#24758)
2023-07-11|||Docs: add `kwargs` type to fix formatting (#24733)
2023-07-06|||Fix integration with Accelerate and failing test (#24691)
2023-07-06|||DeepSpeed/FSDP ckpt saving utils fixes and FSDP training args fixes (#24591)
2023-06-30|||fix peft ckpts not being pushed to hub  (#24578)
2023-06-27|||Fix LR scheduler based on bs from auto bs finder (#24521)
2023-06-27|||set model to training mode before accelerate.prepare (#24520)
2023-06-27|||use accelerate autocast in jit eval path, since mix precision logic isâ€¦ (#24460)
2023-06-27|||Fix 'local_rank' AttiributeError in Trainer class (#24297)
2023-06-26|||deepspeed z1/z2 state dict fix (#24489)
2023-06-26|||when resume from peft checkpoint, the model should be trainable (#24463)
2023-06-23|||fixes issue when saving fsdp via accelerate's FSDP plugin (#24446)
2023-06-23|||fix the grad_acc issue at epoch boundaries (#24415)
2023-06-23|||[`Trainer`] Fix `.to` call on 4bit models (#24444)
2023-06-22|||Clarify batch size displayed when using DataParallel (#24430)
2023-06-22|||Refactor hyperparameter search backends (#24384)
2023-06-21|||[Trainer] Fix optimizer step on PyTorch TPU (#24389)
2023-06-20|||Fix resuming PeftModel checkpoints in Trainer  (#24274)
2023-06-16|||Adding ddp_broadcast_buffers argument to Trainer (#24326)
2023-06-16|||Byebye pytorch 1.9 (#24080)
2023-06-15|||deepspeed init during eval fix (#24298)
2023-06-14|||Clean up old Accelerate checks (#24279)
2023-06-14|||update FSDP save and load logic (#24249)
2023-06-12|||Finish dataloader integration (#24201)
2023-06-12|||ðŸš¨ðŸš¨ðŸš¨ Replace DataLoader logic for Accelerate in Trainer, remove unneeded tests ðŸš¨ðŸš¨ðŸš¨ (#24028)
2023-06-09|||fix bugs with trainer (#24134)
2023-06-08|||[`Trainer`] Correct behavior of `_load_best_model` for PEFT models (#24103)
2023-06-08|||fix accelerator prepare during eval only mode (#24014)
2023-06-07|||Do not prepare lr scheduler as it as the right number of steps (#24088)
2023-06-07|||fix executable batch size issue (#24067)
2023-06-07|||Support PEFT models when saving the model using trainer (#24073)
2023-06-05|||fix trainer slow tests related to hyperparam search (#24011)
2023-06-02|||Trainer: fixed evaluate raising `KeyError` for ReduceLROnPlateau (#23952)
2023-05-31|||remove the extra `accelerator.prepare`  (#23914)
2023-05-31|||Fix Trainer when model is loaded on a different GPU (#23792)
2023-05-31|||accelerate deepspeed and gradient accumulation integrate (#23236)
2023-05-31|||Fix last instances of kbit -> quantized (#23797)
2023-05-31|||shift torch dynamo handling to accelerate (#23168)
2023-05-31|||move fsdp handling to accelerate (#23158)
2023-05-31|||Smangrul/accelerate ddp integrate (#23151)
2023-05-31|||Smangrul/accelerate mp integrate (#23148)
2023-05-26|||Log the right train_batch_size if using auto_find_batch_size and also log the adjusted value seperately. (#23800)
2023-05-25|||Fix psuh_to_hub in Trainer when nothing needs pushing (#23751)
2023-05-24|||Fix sagemaker DP/MP (#23681)
2023-05-24|||Paged Optimizer + Lion Optimizer for Trainer (#23217)
2023-05-24|||4-bit QLoRA via bitsandbytes (4-bit base model + LoRA) (#23479)
2023-05-24|||fix: load_best_model_at_end error when load_in_8bit is True (#23443)
2023-05-17|||Remove hardcoded prints in Trainer (#23432)
2023-05-16|||Why crash the whole run when HFHub gives a 50x error? (#23320)
2023-05-09|||Support ratios for `logging_steps`, `eval_steps`, and `save_steps` (#23235)
2023-05-04|||fix resume fsdp (#23111)
2023-05-02|||Fix check for backword_pos (#23075)
2023-04-28|||Cuda rng_state_all is used when saving in distributed mode so same should also be used when loading (#23045)
2023-04-28|||Add Trainer support for ReduceLROnPlateau (#23010)
2023-04-21|||ddp fixes for training (#22874)
2023-04-19|||move preprocess_logits_for_metrics before _nested_gather in trainer.eâ€¦ (#22603)
2023-04-17|||Introduce `PartialState` as the device handler in the `Trainer` (#22752)
2023-04-07|||Fix typo (#22650)
2023-04-07|||fix FSDP version related issues (#22489)
2023-04-06|||[`bnb`] 8bit models should not be converted to `DDP` (#22628)
2023-04-05|||Add thousands separator in training summary (#22583)
2023-04-04|||Implemented safetensors checkpoints save/load for Trainer (#22498)
2023-04-04|||[setup] drop deprecated `distutils` usage (#22531)
2023-04-03|||Fix missing metrics with multiple eval datasets (#22536)
2023-04-03|||[`Trainer`] Force `is_model_parallel` when model is loaded in multiple GPUs using `accelerate` (#22532)
2023-03-29|||Revert "Fix --bf16 option support for Neuron after PR #22300" (#22451)
2023-03-23|||Fix --bf16 option support for Neuron after PR #22300 (#22307)
2023-03-23|||Mention why one needs to specify max_steps in Trainer (#22333)
2023-03-22|||docs: Resolve incorrect type typo in trainer methods (#22316)
2023-03-21|||Restore fp16 support on xla gpu device (#22300)
2023-03-20|||Move torch.compile() wrapping after DDP/FSDP wrapping to ensure correct graph breaks during training (#22279)
2023-03-20|||Proper map location for optimizer load (#22273)
2023-03-17|||[trainer] param count for deepspeed zero3 (#22193)
2023-03-14|||Load optimizer state on CPU to avoid CUDA OOM (#22159)
2023-03-14|||Revert "Enforce same behavior as PyTorch 2.0 for older versions" (#22163)
2023-03-14|||[trainer] add `--optim adamw_torch_fused` for pt-2.0+ (#22144)
2023-03-13|||Remove backend check for torch.compile (#22140)
2023-03-13|||[trainer] fix bug in grad accum with multiple epochs (#22098)
2023-03-13|||Enforce same behavior as PyTorch 2.0 for older versions (#22136)
2023-03-09|||Return analysis for hyperparameter_search with Ray backend (#22040)
2023-03-08|||Fix test for torchneuroncore in Trainer (#22028)
2023-03-06|||Disable DDP for neuron (#21953)
2023-03-02|||fsdp bf16 enable autocast (#21847)
2023-03-01|||[deepspeed] check whether model is NLP one instead of counting on input type (#21800)
2023-02-25|||Fix resume_from_checkpoint for deepspeed (#21735)
2023-02-22|||Apply ruff flake8-comprehensions (#21694)
2023-02-20|||Enable PyTorch/XLA Fully Sharded Data Parallel (FSDP) (#21406)
2023-02-14|||fix: Race Condition when using Sagemaker Checkpointing and Model Repository (#21614)
2023-02-07|||Add limit_all_gathers option to fsdp_config and fix forward_prefetch bug (#21489)
2023-02-06|||Fix epoch number when resuming training (#21478)
2023-02-06|||Update quality tooling for formatting (#21480)
2023-02-03|||For IterableDataset, return DataLoader using self._train_batch_size. â€¦ (#21447)
2023-02-03|||do not scale gradient in bf16 mode (#21428)
2023-02-02|||[`bnb`] Fine-tuning HF 8-bit models (#21290)
2023-02-01|||Skip batches fast with accelerate (#21390)
2023-01-31|||Add support of backward_prefetch and forward_prefetch (#21237)
2023-01-23|||Optimize by not computing gradients for parameters set to requires_grad=False (#21236)
2023-01-18|||Adapt repository creation to latest hf_hub (#21158)
2023-01-15|||Fixed typo in docstring (#21115)
2023-01-12|||[bnb optim] fixing test (#21030)
2023-01-12|||Optimize inference only mode memory if ipex is used (#21083)
2023-01-03|||Fix race condition on cleaning checkpoints when save_total_limit set to 1 (#20989)
2023-01-03|||Ignore errors when deleting old checkpoints in trainer (#20984)