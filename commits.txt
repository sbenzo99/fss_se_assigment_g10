Release: v4.57.0
v4.57.0 Branch (#41310)
Don't list dropout in eager_paged_attention_forward (#40924)
Add FlexOlmo model (#40921)
Standardize audio embedding function name for audio multimodal models (#40919)
Update expected values for some `test_speculative_generation` (#40949)
Fix `Glm4vModelTest::test_eager_matches_fa2_generate` (#40947)
Remove nested import logic for torchvision (#40940)
Consistent naming for images kwargs (#40834)
Raise error instead of warning when using meta device in from_pretrained (#40942)
Fix `Glm4vMoeIntegrationTest` (#40930)
Fix trainer tests (#40823)
docs(i18n): Correct the descriptive text in the README_zh-hans.md (#40941)
Intel CPU dockerfile (#40806)
[models] remove unused `import torch.utils.checkpoint`  (#40934)
[DOC] Add missing dates in model cards (#40922)
Add LongCat-Flash (#40730)
Add support for Florence-2 training (#40914)
Minor fix for #40727 (#40929)
Adding activation kernels (#40890)
[torchao safetensors] renaming get_state_dict function (#40774)
Fix #40067: Add dedicated UMT5 support to GGUF loader (config, tokenizer, test) (#40218)
[Llama4] Remove `image_sizes` arg and deprecate `vision_feature_layer` (#40832)
Processor load with multi-processing (#40786)
[Docs] Adding documentation of MXFP4 Quantization (#40885)
Fix dtype in Paligemma (#40912)
üî¥Make `center_crop` fast equivalent to slow (#40856)
[generate] misc fixes (#40906)
[gemma3] `Gemma3ForConditionalGeneration` compatible with assisted generation (#40791)
disable `test_fast_is_faster_than_slow` (#40909)
Remove `runner_map` (#40880)
Improve module name handling for local custom code (#40809)
remove dummy EncodingFast (#40864)
Add Olmo3 model (#40778)
Set seed for `Glm4vIntegrationTest` (#40905)
[cache] Only use scalars in `get_mask_sizes` (#40907)
Harmonize CacheLayer names (#40892)
[cache] Merge static sliding and static chunked layer (#40893)
Fix flaky `Gemma3nAudioFeatureExtractionTest::test_dither` (#40902)
Fix getter  regression (#40824)
Fixing the call to kernelize (#40628)
Make debugging failing tests (check and update expect output values) easier üî•  (#40727)
[generate] remove docs of a feature that no longer exists (#40895)
üåê [i18n-KO] Translated `imageprocessor.md` to Korean (#39557)
üåê [i18n-KO] Translated smolvlm.md to Korean (#40414)
Remove dict branch of attention_mask in sdpa_attention_paged_forward (#40882)
Fix deta loading & dataclass (#40878)
Add Fast PromptDepthAnything Processor (#40602)
Use torch.expm1 and torch.log1p for better numerical results (#40860)
Clarify passing is_causal in sdpa_attention_paged_forward (#40838)
üî¥ Move variable output controls to `_prepare_generation_config ` (#40715)
Fix modular consistency (#40883)
[`VaultGemma`] Update expectations in integration tests (#40855)
Adding Support for Qwen3-VL Series (#40795)
[Qwen3 Next] Use numerically stable `rsqrt` (#40848)
Update model tags and integration references in bug report (#40881)
fix: XIELU act parameters not being casted to correct dtype (#40812)
fix florence kwargs  (#40826)
[docstrings / type hints] Update outdated annotations for `past_key_values`  (#40803)
[Bug fix #40813] Fix base_model_tp_plan of Starcoder2 model. (#40814)
Redirect MI355 CI results to dummy dataset (#40862)
Fix TrainingArguments.parallelism_config NameError with accelerate<1.10.1 (#40818)
Use checkpoint in auto_class_docstring (#40844)
[generate] Always use decoder config to init cache (#40772)
[tests] move generative tests away from `test_modeling_common.py` (#40854)
[test] Fix test_eager_matches_sdpa incorrectly skipped (#40852)
add: differential privacy research model (#40851)
[Sam2Video] Fix video inference with batched boxes and add test (#40797)
[SAM2] Fix inconsistent results with original implementation with input boxes (#40800)
[tests] re-enable aria fast tests (#40846)
Fixes for continuous batching (#40828)
Fix the misalignment between the l2norm in GDN of Qwen3-Next and the implementation in the FLA library. (#40842)
Replace image classification loss functions to `self.loss_function` (#40764)
Update no split modules in T5Gemma model (#40810)
Adds Causal Conv 1D kernel for mamba models (#40765)
Add VideoProcessors to auto-backend requirements (#40843)
Improve torch_dtype checks (#40808)
üåê [i18n-KO] Translated clipseg.md to Korean (#39903)
[`Jetmoe`] Fix RoPE (#40819)
Push generation config along with checkpoints (#40804)
add general hub test for Fast Image Processors in test_image_processing_utils (#40086)
Fix typos in src (#40782)
Align torch implementation of Gated DeltaNet in Qwen3-Next with fla library. (#40807)
‚ö†Ô∏è üî¥ Add ministral model (#40247)
Fix config dtype parsing for Emu3 edge case (#40766)
Fix edge case for tokenize (#36277) (#36555)
feature: Add robust token counting with padding exclusion  (#40416)
Fix DeepSpeed mixed precision precedence over Accelerate defaults (#39856)
[Docs] Add missing class documentation for optimizer_schedules (#31870,  #23010) (#40761)
fix_image_processing_fast_for_glm4v (#40483)
Remove use_ipex option from Trainer (#40784)
Move num_items_in_batch to correct device before accelerator.gather (#40773)
Fix the issue that csm model cannot work with pipeline mode. (#39349)
Fix dotted model names (#40745)
Read config pattern for Qwen3Next (#40792)
Use functools.cached_property (#40607)
Fix invalid PipelineParallel member (#40789)
Fix typos in tests and util (#40780)
Fix doc for PerceptionLMForConditionalGeneration forward. (#40733)
üö® Fix Inconsistant `input_feature` length and `attention_mask` length in `WhisperFeatureExtractor` (#39221)
Enable ruff on benchmark and scripts (#40634)
[processors] Unbloating simple processors (#40377)
Remove reference of video_load_backend and video_fps for processor (#40719)
Fix gpt-oss router_indices in EP (#40545)
Adding Support for Qwen3-Next (#40771)
[docs] CPU install (#40631)
[pipeline] ASR pipeline kwargs are forwared to `generate` (#40375)
Fix crash when executing MambaCache sample code (#40557)
[RoPE] run RoPE tests when the model uses RoPE (#40630)
[tests] update `test_past_key_values_format` and delete overwrites (#40701)
rm src/transformers/convert_pytorch_checkpoint_to_tf2.py (#40718)
[deprecations] Remove generate-related deprecations up to v4.56 (#40729)
Support sliding window in CB (#40688)
[generate] `PromptLookupCandidateGenerator` won't generate forbidden tokens (#40726)
Fix: swanlab `public.cloud.experiment_url` api error (#40763)
Add EfficientLoFTRImageProcessorFast for GPU-accelerated image processing (#40215)
Fix Bark failing tests (#39478)
üåê [i18n-KO] Translated 'xclip.md' to Korean (#39594)
Fix `continue_final_message` in `apply_chat_template` to prevent substring matching issues (#40732)
Fix inconsistency in SeamlessM4T and SeamlessM4Tv2 docs (#39364)
Fix more typos (#40627)
Remove unnecessary tildes from documentation (#40748)
docs: add continuous batching to serving (#40758)
feat: err when unsupported attn impl is set w/ `--continuous_batching` (#40618)
remove FSDP prefix when using save_pretrained with FSDP2 (#40207)
remove gemmas eager training warning (#40744)
Add BF16 support check for MUSA backend (#40576)
Set accepts_loss_kwargs to False for ConvNext(|V2)ForImageClassification (#40746)
Fix np array typing (#40741)
Fix order of mask functions when using `and/or_mask_function` (#40753)
[Continous Batching] fix do_Sample=True in continuous batching (#40692)
refactor(serve): move `request_id` to headers (#40722)
Skip `VitMatteImageProcessingTest::test_fast_is_faster_than_slow` (#40713)
Keypoint matching docs (#40541)
[`Gemma Embedding`] Fix SWA (#40700)
Add Optional typing (#40686)
[tests] remove overwrites of removed test (#40720)
[serve] re-enable tests (#40717)
Fix arguments (#40605)
üî¥ Update Glm4V to use config values (#40712)
Fix parent classes of AllKwargsForChatTemplate (#40685)
[onnx] use logical `or` for grounding dino mask (#40625)
[moduar] Add missing `self` in post-process methods (#40711)
[tests] fix blip2 edge case (#40699)
üö® Allow `check_model_inputs` in core VLMs (#40342)
Fix parent classes of ProcessingKwargs (#40676)
feat(serve): add healthcheck test (#40697)
Fetch more test data with `hf_hub_download` (#40710)
Add Fast Image Processor for ImageGPT (#39592)
Fetch one missing test data (#40703)
Align assisted generate for unified signature in decoding methods (#40657)
Avoid `T5GemmaModelTest::test_eager_matches_sdpa_inference` being flaky (#40702)
Fix broken Llama4 accuracy in MoE part (#40609)
[Glm4.5V] fix vLLM support (#40696)
Fix self.dropout_p is not defined for SamAttention/Sam2Attention (#40667)
Fix backward compatibility with accelerate in Trainer (#40668)
Change docker image to preview for the MI355 CI (#40693)
Fixing bug in Voxtral when merging text and audio embeddings (#40671)
feat: support request cancellation (#40599)
add: embedding model (#40694)
Final test data cache - inside CI docker images (#40689)
Load a tiny video to make CI faster (#40684)
fix broken offline mode when loading tokenizer from hub (#40669)
Add codebook_dim attribute to DacVectorQuantize for DacResidualVectorQuantize.from_latents() (#40665)
Add sequence classification support for small Gemma 3 text models (#40562)
CircleCI docker images cleanup / update / fix (#40681)
Mark `Aimv2ModelTest::test_eager_matches_sdpa_inference_04_fp16_pad_right_sdpa_kernels` as flaky (#40683)
Avoid night torch CI not run because of irrelevant docker image failing to build  (#40677)
Skip more fast v.s slow image processor tests (#40675)
Even more test data cached (#40636)
Benchmarking V2: framework impl (#40486)
refactor: use `tolist` instead of list comprehension calling `.item()` (#40646)
Remove overwritten `GitModelTest::test_beam_search_generate` (#40666)
Skip `test_prompt_lookup_decoding_matches_greedy_search` for `qwen2_audio` (#40664)
Fix warning for output_attentions=True (#40597)
Skip `test_fast_is_faster_than_slow` for `Owlv2ImageProcessingTest` (#40663)
Update `check_determinism` inside `test_determinism` (#40661)
Allow custom args in `custom_generate` Callables and unify generation args structure (#40586)
Avoid attention_mask copy in qwen2.5 (#40658)
Fix Metaclip modular conversion (#40660)
feat(serving): add healthcheck (#40653)
fix pipeline dtype (#40638)
Mark `LongformerModelTest::test_attention_outputs` as flaky (#40655)
Remove TF/Flax examples (#40654)
fix MetaCLIP 2 wrong link & wrong model names in the docstrings (#40565)
add DeepseekV3ForTokenClassification (#40641)
Skip `test_prompt_lookup_decoding_matches_greedy_search` for `voxtral` (#40643)
Fix: PIL image load in Processing utils apply_chat_template (#40622)
[CP] Add attention_mask to the buffer when the mask is causal  (#40619)
[auto-model] propagate kwargs (#40491)
fix: gas for gemma fixed (#40591)
Fix `too many requests` in `TestMistralCommonTokenizer` (#40623)
üåê [i18n-KO] Translated `deepseek_v3.md` to Korean  (#39649)
Remove random flag (#40629)
Support TF32 flag for MUSA backend (#33187)
Enable more ruff UP rules (#40579)
Fix invalid typing (#40612)
Remove unnecessary pillow version check (#40604)
Add collated reports job to Nvidia CI (#40470)
Fix flaky `JambaModelTest.test_load_balancing_loss` (#40617)
Avoid `too many request` caused by `AutoModelTest::test_dynamic_saving_from_local_repo` (#40614)
Fix processor chat template (#40613)
fix: continuous batching in `transformers serve` (#40479)
Disable cache for `TokenizerTesterMixin` temporarily (#40611)
Multiple fixes to FA tests in AMD (#40498)
Pin torchcodec to 0.5 in AMD docker (#40598)
Reduce more test data fetch (#40595)
[`Tests`] Fixup duplicated mrope logic (#40592)
Fix quite a lot of FA tests (#40548)
Fix for missing default values in encoder decoder  (#40517)
Fix `siglip` flaky `test_eager_matches_sdpa_inference` (#40584)
Add Copilot instructions (#40432)
Fix inexistent imports (#40580)
Skip `TvpImageProcessingTest::test_slow_fast_equivalence` (#40593)
Fix typos (#40585)
üö® Remove Constrained Beam Search decoding strategy (#40518)
Support batch size > 1 image-text inference (#36682)
üö® Remove Group Beam Search decoding strategy (#40495)
Fix custom generate relative imports (#40480)
Update `get_*_features` methods + update doc snippets (#40555)
Fix llava image processor (#40588)
Allow `remi-or` to `run-slow` (#40590)
Fix CircleCI step passes in the case of pytest worker crash at test collection time (#40552)
Fix `test_eager_matches_sdpa_inference` not run for `CLIP` (#40581)
[qwen-vl] fix position ids (#40490)
processor tests - use dummy videos (#40537)
Set `test_all_params_have_gradient=False` for `DeepseekV2ModelTest` (#40566)
remove the redundant non maintained jieba and use rjieba instead (#40383)
pin `pytest-rerunfailures<16.0` (#40561)
Fix collated reports upload filename (#40556)
Dev version
Fix `GptOssModelTest::test_assisted_decoding_matches_greedy_search_1_same` (#40551)
fix gpt-oss out shape (#40535)
Flaky CI is annoying (#40543)
Fix gpt-oss rope warning  (#40550)
Add bfloat16 support detection for MPS in is_torch_bf16_gpu_available() (#40458)
Allow compression on meta device (#39039)
Clean-up kernel loading and dispatch (#40542)
Redundant code removal (#40534)
Fix typos (#40511)
Oupsy  (#40544)
`tokenizers` bump tokenizers version (#40540)
Fix `SeamlessM4Tv2ModelWithTextInputTest::test_retain_grad_hidden_states_attentions` (#40532)
Set `test_all_params_have_gradient=False` for `HunYuanMoEV1ModelTest` (#40530)
[`Qwen Omni/VL`] Fix fa tests (#40528)
Improve Gemma3n model and tests (#39764)
Lazy import torchcodec (#40526)
Fix typo: 'casual' to 'causal' (#40374)
skip some `padding_matches_padding_free_with_position_ids` for FA2 (#40521)
Fix mistral3 tests after "[Kosmos 2.5] Rename checkpoints" (#40523)
[`kernels`] If flash attention2 is not installed / fails to import (cc on our cluster) default to kernels (#40178)
Skip some flex attn tests (#40519)
[`FA`] Remaining Cleanup (#40424)
[omni modality] support composite processor config (#38142)
Use the config for DynamicCache initialization in all modelings (#40420)
[serve] fix ` request_id` unexpected (#40501)
sped up gguf tokenizer for nemotron test (#40509)
correct kes to keys. (#40489)
[vision] Improve keypoint-matching models docs (#40497)
[Kosmos 2.5] Rename checkpoints (#40338)
Add more missing arguments (#40354)
Add Apertus (#39381)
Update quantization overview for XPU (#40331)
fix typo (#40484)
Various AMD expectations (#40510)
Include machine type in collated reports filename (#40514)
[modular] Classes can now be defined and referenced in arbitrary order (without bringing unwanted dependencies) (#40507)
docs(pixtral): Update Pixtral model card to new format (#40442)
Fix the CI workflow of `merge to main` (#40503)
Collated reports: no need to upload artifact (#40502)
[Whisper] Add rocm expected results to certain tests (#40482)
Fix `qwen2_moe` tests (#40494)
[EfficientLoFTR] dynamic image size support (#40329)
[ESM] support attention API (#40370)
[modular] Remove ambiguity in all calls to parent class methods + fix dependency graph (#40456)
[modular] Use multi-processing + fix model import issue (#40481)
Validate GptOssConfig rope config after it's fully initialized (#40474)
CI when PR merged to `main` (#40451)
Fix nightly torch CI (#40469)
Not to shock AMD team by the cancelled workflow run notification ‚ù§Ô∏è üíñ (#40467)
Update SegFormer model card (#40417)
[pipeline] Add Keypoint Matching pipeline (#39970)
[RoPE] explicit factor > implicit factor in YaRN (#40320)
[fast_image_processor] fix image normalization for resize (#40436)
deci gguf support (#38669)
Fix extra template loading (#40455)
flash_paged: s_aux may not exist (#40434)
Continuous batching refactor (#40426)
üö® Remove Contrastive Search decoding strategy (#40428)
Make cache_config not mandatory (#40316)
rename get_cuda_warm_up_factor to get_accelerator_warm_up_factor (#40363)
[video processors] decode only sampled videos -> less RAM and faster processing (#39600)
fix qwen25-vl grad acc (#40333)
[Trainer] accelerate contextparallel support in trainer (#40205)
Refactor ViT-like models (#39816)
Fix non FA2 tests after FA2 installed in CI docker image (#40430)
Fix collated reports model name entry (#40441)
InternVL MI325 test expectations (#40387)
Fix collated reports uploading (#40440)
Fix https://github.com/huggingface/transformers/issues/40292 (#40439)
Fix collated reports model directory traversal (#40437)
Gemma3 text fixes: Add expectations for MI325 (#40384)
üåê [i18n-KO] Translated `models.md` to Korean (#39518)
Remove working-dir from collated reports job (#40435)
[docs] remove last references to `transformers` TF classes/methods (#40429)
Fix typo and improve GPU kernel check error message in MXFP4 quantization (#40349) (#40408)
Add `tokenizer_kwargs`  argument to the text generation pipeline (#40364)
Update collated reports working directory and --path (#40433)
Fix modular for modernbert-decoder (#40431)
üö® Remove DoLa decoding strategy (#40082)
[`Mxfp4`] Add a way to save with a quantization method (#40176)
Fix label smoothing incompatibility with multi-label classification (#40296)
Fix processing tests (#40379)
Gpt oss optim (#40304)
Fix UnboundLocalError in WER metric computation (#40402)
Fix typo: 'seperator' to 'separator' in variable names (#40389)
Fix CI (hunyuan moe does not support fullgraph) (#40423)
Fix typo: 'casual' -> 'causal' in code and documentation (#40371) (#40407)
[docs] flax/jax purge (#40372)
fix to accept cumulative_seqlens from TransformersKwargs in FA (#40194)
:broom: :broom: :broom: Get set decoder cleanup (#39509)
Reactivate a lot of tests skipped for no reason anymore (#40378)
Run FA2 tests in CI (#40397)
HF papers in doc (#40381)
Update README_zh-hans.md (#40380)
Rework the Cache documentation (#40373)
Chat Template Doc Fixes (#40173)
Bug Fix: Dynamically set return_lse flag in FlexAttention (#40352)
Add GptOssForTokenClassification for GPT-OSS models (#40190)
Addiing ByteDance Seed Seed-OSS (#40272)
fix(example): align parameter names with the latest function definition for gdino (#40369)
[configuration] allow to overwrite kwargs from subconfigs (#40241)
[processor] move commonalities to mixin (#40339)
‚ö†Ô∏è‚ö†Ô∏è Use `dtype` instead of `torch_dtype` everywhere! (#39782)
[pipelines] add support to `skip_special_tokens` in the main text generation pipelines (#40356)
Change multimodal data links to HF hub (#40309)
wav2vec2 fixes (#40341)
Fix idefics3 vision embeddings indices dtype (#40360)
HunYuan opensource (#39606)
DOCS: Clarification on the use of `label_names` as an argument to TrainingArguments (#40353)
[4/N]more docs to device agnostic (#40355)
[generate] handle support for cache classes when num enc layers != num dec layers (#40277)
Qwen2.5-VL test fixes for ROCm (#40308)
[`FA`] Fix some model tests (#40350)
Remove more PyTorch 2.2 compatible code (#40337)
[detection] use consistent dtype for Conditional and DAB DETR positional embeddings (#40300)
[serve] add cors warnings (#40112)
Clean up XCodec and other codecs (#40348)
[ModernBert] Prevent the attention mask from being None in ModernBertForSequenceClassification (#35991)
Fix attention vizualizer (#40285)
(small) fix conditional for input_ids and input_embeds in marian (#40045)
Update `test_spm_converter_bytefallback_warning` (#40284)
T5 test and target device fixes (#40313)
Fix links in Glm4vMoe configuration classes to point to the correct H‚Ä¶ (#40310)
Fix an infinite loop bug in recursive search of relative imports (#40326)
add type hints (#40319)
Fix: Only call Trainer.align_special_tokens if model has "config" attribute (#40322)
[docs] remove TF references from `/en/model_doc` (#40344)
Add missing arguments to class constructors (#40068)
Fix deprecation warning version (#40343)
Add DeepseekV3ForSequenceClassification for Deepseek V3 models (#40200)
Change Qwen2RMSNorm to RMSNorm from PyTorch (#40066)
Fix qwen-omni processor text only mode (#40336)
[docs] remove flax references from `/en/model_doc` (#40311)
Fix chunked attention mask with left-padding (#40324)
One cache class to rule them all (#40276)
Update notification service amd_daily_ci_workflows definition (#40314)
Fix: Apply `get_placeholder_mask` in Ovis2 (#40280)
Update CI with nightly torch workflow file (#40306)
[`GPT OSS`] Refactor the tests as it was not properly checking the outputs (#40288)
No more `natten` (#40287)
byebye torch 2.1 (#40317)
Add back `_tp_plan` attribute (#39944)
Qwen2.5-Omni test fixes (#40307)
Add support for Florence-2 (#38188)
Remove unnecessary contiguous calls for modern torch (#40315)
:rotating_light: [`Flash Attention`] Fix sliding window size (#40163)
chore: fix typo in `find_executable_batch_size` to match new 0.9 ratio (#40206)
[`fix`] Pass adamw optimizer parameters to StableAdamW (#40184)
Fix GOT-OCR2 and Cohere2Vision image processor patches caculation (#40312)
Remove OTel SDK dependencies (#40305)
Clean up X-Codec. (#40271)
[docs] delete more TF/Flax docs (#40289)
[`FA`] Fix dtype in varlen with position ids (#40295)
Allow to be able to run `torch.compile` tests with `fullgraph=True` (#40164)
Add MetaCLIP 2 (#39826)
[3/3] make docs device agnostic, all en docs for existing models done  (#40298)
make model docs device agnostic (2) (#40256)
SmolVLM test fixes (#40275)
Adjust ROCm test output expectations (#40279)
Standardize BertGeneration model card (#40250)
SmolVLM and InternVL: Ensure pixel values are converted to the correct dtype for fp16/bf16 (#40121)
Update model card for gpt neox japanese (#39862)
docs: Update TrOCR model card to new format (#40240)
Standardize RAG model card (#40222)
docs(layoutlm): add missing `id=usage` to `<hfoptions>` tag in LayoutLM model card (#40273)
Fix chat CLI GPU loading and request_id validation issues (#40230) (#40232)
fix which routing method (#40283)
Update image_processing_perception_lm_fast.py to allow for proper override of vision_input_type (#40252)
Skipping pytree registration in case fsdp is enabled (#40075)
Add Kosmos-2.5 (#31711)
[detection] fix correct `k_proj` weight and bias slicing in D-FINE (#40257)
Fix setting attention for multimodal models (#39984)
üö®üö® Switch default compilation to fullgraph=False (#40137)
Fix slow static cache export tests (#40261)
[detection] fix attention mask for RT-DETR-based models (#40269)
set inputs_embeds to None while generate to avoid audio encoder forward in generation process (#40248)
Remove MI300 CI (#40270)
Skip broken tests (#40157)
docs: Update OLMo model card (#40233)
Fix benchmark workflow (#40254)
Correct typo and update notes in docs Readme (#40234)
Model card for NLLB (#40074)
fix: Catch correct ConnectionError for additional_chat_templates (#39874)
Fixes for EncoderDecoderCache (#40008)
[`CI`] Fix repo consistency (#40249)
[serve] guard imports (#39825)
[typing] fix type annotation error in DepthPro model image processor (#40238)
Add `chat_template` (`jinja2`) as an extra dependency (#40128)
remove transpose_for_scores call in ESM-2 (#40210)
üö® Always return Cache objects in modelings (to align with generate) (#39765)
Fix more pylint warnings (#40204)
Add Ovis2 model and processor implementation (#37088)
AMD scheduled CI ref env file (#40243)
Fix ESM token_dropout crash when using inputs_embeds instead of input_ids (#40181)
Fix more typos (#40212)
[SAM 2] Change checkpoints in docs and tests (#40213)
fix error vocab_size at Qwen2_5_VLForConditionalGeneration loss_function (#40130)
Use correct `model_input_names` for PixtralImageProcessor (#40226)
Revert "Pin torch to 2.7.1 on CircleCI for now" + Final fix for `too long with no output` (#40201)
docs: Update LayoutLM model card according to new standardized format (#40129)
Fix GPT-OSS `swiglu_limit` not passed in for MXFP4 (#40197)
Add X-Codec model (#38248)
Benchmarking improvements (#39768)
Update: add type hints to check_tokenizers.py (#40094)
Fix various Pylint warnings (#40107)
Avoid CUDA stream sync (#40060)
Remove _prepare_flash_attention_from_position_ids (#40069)
Fix typos (#40175)
Add repr to EncoderDecoderCache (#40195)
Fix fsdp for generic-task models (#40191)
fix to avoid modifying a view in place (#40162)
make model doc device agnostic (#40143)
[MINOR:TYPO] Update base.py (#40169)
Update dynamic attnt setter for multimodals (#39908)
Pin torch to 2.7.1 on CircleCI for now (#40174)
Add dates to the model docs (#39320)
Standardize BARTpho model card: badges, new examples, fixed broken im‚Ä¶ (#40051)
Add GptOssForSequenceClassification for GPT-OSS models (#40043)
build: Add fast image processor tvp (#39529)
Fix docs typo (#40167)
[bugfix] fix flash-attention2 unavailable error for Ascend NPU (#40151)
[FA2] Fix it finally - revert fa kwargs preparation (#40161)
Replace `self.tokenizer` by `self.processing_class` (#40119)
[Continous Batching] set head_dim when config.head_dim is None (#40159)
Fix CI: Use correct import in SAM for torchvision InterpolationMode (#40160)
[efficientloftr] fix bugs and follow original cross attn implementation strictly (#40141)
[Cohere2Vision] remove unused arg (#40103)
Create self-scheduled-amd-mi355-caller.yml (#40134)
Update Dockerfiles to install packages inside a virtual environment (#39098)
Add pytest marker: `torch_compile_test` and `torch_export_test` (#39950)
Fix quantized cache with only cache_implementation in generate (#40144)
üåê [i18n-KO] Translated `gemma3.md` to Korean (#39865)
updated visualBERT modelcard (#40057)
Remove an old badly designed test (#40142)
[docs] Fix ko toctree (#40138)
Add Segment Anything 2 (SAM2) (#32317)
Fix Janus (#40140)
gpt oss is important (#40139)
üåê [i18n-KO] Translated `pipelines.md` to Korean (#39577)
üö® Use lru_cache for sine pos embeddings MaskFormer (#40007)
üåê [i18n-KO] Translated grounding-dino.md to Korean (#39861)
üåê [i18n-KO] Translated `optimizers.md` to Korean (#40011)
üåê [i18n-KO] Translated `gpt2.md` to Korean (#39808)
üö®üö®  [generate] ignore `cache_implementation="hybrid"` hub defaults (#40135)
üåê [i18n-KO] Translated `main_classes/optimizer_schedules.md` to Korean (#39713)
üåê [i18n-KO] Translated `jamba.md` to Korean (#39890)
üåê [i18n-KO] Translated `main_classes/processors.md` to Korean (#39519)
Fix hidden torchvision>=0.15 dependency issue (#39928)
[trainer] handle case where EOS token is None in `generation_config` (#40127)
DOCS: Add missing space in SECURITY.md (#40087)
Collated reports (#40080)
`decoding_method` argument in generate (#40085)
[serve] allow array `content` inputs for LLMs (#39829)
Fix QuantoQuantizedCache import issues (#40109)
changed xLSTMRMSNorm to RMSNorm (#40113)
[bugfix] Fix tensor device in Idefics2, Idefics3, and SmolVLM (#39975)
üåê [i18n-KO] Translated `tiny_agents.md` to Korean (#39913)
remove sequence parallel in llama4 (#40084)
Add model card for MobileViT (#40033)
[docs] Add reference to HF-maintained `custom_generate` collections (#39894)
Fix Causality Handling in Flash Attention to Support Bidirectional Attention (#39707)
[trainer] ensure special tokens in model configs are aligned with tokenizer at train time (#38441)
[`Flash Attention`] Fix flash attention integration (#40002)
Default to dequantize if cpu in device_map for mxfp4 (#39993)
Fix error on importing unavailable torch.distributed (#40038)
Fix Qwen3 MoE GGUF architecture mismatch (#39976)
Switch the order of args in StaticCache (for BC and future logic) (#40100)
Fix regression in mllama vision encoder (#40083)
Replace `logger.warning` with `logger.warning_once` in `GradientCheckpointingLayer` (#40091)
Re-apply make style (#40106)
feat: add `is_fast` to ImageProcessor (#39603)
Enable SIM rules (#39806)
New DynamicSlidingWindowLayer & associated Cache (#40039)
Audio encodings now match conv2d weight dtype in Gemma3nAudioSSCPConvBlock (#39743)
Causal loss for `ForConditionalGeneration` (#39973)
Add glm4.5&&glm4.5V doc (#40095)
Update Glm4V processor and add tests (#39988)
[docs] Zero Shot Object Detection Task (#40096)
[fix] batch inference for llava_onevision (#40021)
Revert FA2 kwargs construction (#40029)
Fix PerceptionLM image preprocessing for non-tiled image input. (#40006)
Update notification service MI325 (#40078)
feat: extract rev in attn_implementation kernels via @ (#40009)
[`GPT Big Code`] Fix attention scaling (#40041)
chore: standardize DeBERTa model card (#37409)
Fix `time_spent ` in `notification_service.py`. (#40081)
added Textnet fast image processor (#39884)
Fix repo consistency (#40077)
guard on model.eval when using torch.compile + FSDP2 (#37413)
Remove deprecated cache-related objects (#40035)
fix: move super().__init__ after vision_config init in Mistral3Config (#40063)
[gemma3] update conversion key mapping (#39778)
[qwen-vl] fix beam search with videos (#39726)
fix: resolve triton version check compatibility on windows (#39986)
unpin `torchcodec==0.5.0` and use `torch 2.8` on daily CI (#40072)
Update HuBERT model card according to template (#39742)
Revert "fix `notification_service.py` about `time_spent`" (#40044)
GLM-4.5V Model Support (#39805)
fix `notification_service.py` about `time_spent` (#40037)
Bnb failling tests (#40026)
Tie weights recursively on all submodels (#39996)
fix
[core] Refactor the Cache logic to make it simpler and more general (#39797)
Fix missing None default values for Gemma3n model in get_placeholder_mask (#39991) (#40024)
Harmonize `past_key_value` to `past_key_valueS` everywhere (#39956)
Fix an annoying flaky test (#40000)
Higgs modules_to_not_convert standardization (#39989)
Fix broken image inference for Fuyu model (#39915)
pin torchcodec==0.5.0 for now with torch 2.7.1 on daily CI (#40013)
Update expected output values after #39885 (part 2) (#40015)
Raising error when quantizing a quantized model (#39998)
docs: fix duplication in 'en/optimizers.md' (#40014)
unpin torch<2.8 on circleci (#40012)
FA2 can continue generation from cache (#39843)
Fix default values of getenv (#39867)
Fix HGNetV2 Model Card and Image Classification Pipeline Usage Tips (#39965)
fix: remove CHAT_TEMPLATE import in tests for deepseek-vl (#40003)
Fix missing video inputs for PerceptionLM. (#39971)
Fix int4 quantized model cannot work with cpu (#39724)
Update expected output values after #39885 (part 1) (#39990)
Fix consistency (#39995)
[typing] Fix return typehint for decoder and inv_freq annotation (#39610)
Bump transformers from 4.48.0 to 4.53.0 in /examples/tensorflow/language-modeling-tpu (#39967)
Fix gemma3n feature extractor's incorrect squeeze (#39919)
[Idefics] fix device mismatch (#39981)
Various test fixes for AMD (#39978)
Support input_embeds in torch exportable decoders (#39836)
[superglue] Fixed the way batch mask was applied to the scores before match assignment computation (#39968)
Gemma3 fixes (#39960)
Modular fix: remove the model name in `find_file_type` (#39897)
chore: update Deformable_Detr model card (#39902)
[bugfix] fix flash_attention_2 unavailable error on Ascend NPU (#39844)
Fix `fix_and_overwrite` mode of `utils/check_docstring.py` (#39369)
remove `triton_kernels` dep with `kernels` instead (#39926)
[image processor] fix glm4v (#39964)
fix typo (#39936)
Fix grammatical error in MoE variable name: expert_hitted ‚Üí expert_hit, hitted_experts ‚Üí hit_experts (#39959)
docs: fix typo in 'quantization-aware training' (#39904)
Enable gpt-oss mxfp4 on older hardware (sm75+) (#39940)
Fix MXFP4 quantizer validation to allow CPU inference with dequantize option (#39953)
[docs] ko toc fix (#39927)
circleci: pin torch 2.7.1 until `torchcodec` is updated (#39951)
Fix CI: Tests failing on CPU due to `torch.device('cpu').index` being None (#39933)
Avoid `utils/check_bad_commit.py` failing due to rate limit (requesting  `api.github.com`) (#39918)
[CI] post-`GptOss` fixes for green CI (#39929)
Dev version
gpt_oss last chat template changes (#39925)
Add GPT OSS model from OpenAI  (#39923)
üåê [i18n-KO] Translated `cache_explanation.md` to Korean (#39535)
Export SmolvLM (#39614)
[docs] update object detection guide (#39909)
run model debugging with forward arg (#39905)
Revert "remove dtensors, not explicit (#39840)" (#39912)
Fix aria tests (#39879)
Fix eval thread fork bomb (#39717)
Replace video_fps with fps in tests (#39898)
Fix misleading WandB error when WANDB_DISABLED is set (#39891)
Avoid aliasing in cond's branches for torch 2.8 (#39488)
[qwen] remove unnecessary CUDA sync in qwen2_5_vl (#39870)
fix test_working_of_tp failure of accelerate ut (#39828)
[`Exaone4`] Fixes the attn implementation!  (#39906)
Reorder serving docs (#39634)
chore: update DETR model card (#39822)
Add support for `ModernBertForMultipleChoice` (#39232)
send some feedback when manually building doc via comment (#39889)
Update cohere2 vision test (#39888)
[DOCS] : Improved mimi model card (#39824)
Fix link to models in README (#39880)
[typing] better return type hint for `AutoModelForCausalLM` and `AutoModelForImageTextToText` (#39881)
Set `torch.backends.cudnn.allow_tf32 = False` for CI (#39885)
Replace `Tokenizer` with `PreTrainedTokenizerFast` in `ContinuousBatchProcessor` (#39858)
Rework add-new-model-like with modular and make test filenames coherent (#39612)
Fix quant docker for fp-quant  (#39641)
[core] Fix attn_implementation setter with missing `sub_configs` (#39855)
Add support for including in-memory videos (not just files/urls) in apply_chat_template (#39494)
Use comment to build doc on PRs (#39846)
Refactor label name handling for PEFT models in Trainer class (#39265)
Improve `is_wandb_available` function to verify WandB installation (#39875)
remove dtensors, not explicit (#39840)
Allow `TrackioCallback` to work when pynvml is not installed (#39851)
[image-processing] deprecate `plot_keypoint_matching`, make `visualize_keypoint_matching` as a standard (#39830)
Add fast image processor Janus, Deepseek VL, Deepseek VL hybrid (#39739)
Fix responses add tests (#39848)
Update ux cb (#39845)
Add MM Grounding DINO (#37925)
[typecheck] proper export of private symbols (#39729)
[`attn_implementation`] remove recursive, allows custom kernels with wrappers (#39823)
[VLMs] split out "get placeholder mask" to helper (#39777)
Fix tp cb (#39838)
Fix bad markdown links (#39819)
Fix broken links (#39809)
[cohere2 vision] move doc to multimodal section (#39820)
Update documentation for Cohere2Vision models (#39817)
[Model] Cohere2 Vision (#39810)
[docs] fix korean docs yet again (#39813)
feat(tokenization): add encode_message to tokenize messages one by one (#39507)
fix: providing a tensor to cache_position in model.generate kwargs always crashes because of boolean test (#39300)
Add callback to monitor progress in whisper transcription (#37483)
Update mT5 model card (#39702)
Update model card for Cohere2 (Command R7B) (#39604)
standardized BARThez model card (#39701)
Fix re-compilations for cross attention cache (#39788)
Simplify conditional code (#39781)
Fix an invalid condition (#39762)
fix chameleonvision UT failure (#39646)
Super tiny update (#39727)
more info in `model_results.json` (#39783)
[ASR pipline] fix with datasets 4.0 (#39504)
enable static cache on vision encoder decoder (#39773)
Fix Evolla and xLSTM tests (#39769)
Don't set `run_name` when none (#39695)
Standardize CLAP model card format (#39738)
docs: Update EfficientLoFTR documentation (#39620)
Fix OmDet test after arg deprecation (#39766)
Remove python3.7 reference from doc link (#39706)
[docs] Ko doc fixes after toc update (#39660)
Fix Cache.max_cache_len max value for Hybrid models (#39737)
fix(trainer): Correct loss scaling for incomplete gradient accumulation steps (#39659)
üåê [i18n-KO] Translated `how_to_hack_models.md` to Korean (#39536)
üåê [i18n-KO] Translated `perf_train_gpu_one.md` to Korean (#39552)
üåê [i18n-KO] Translated `pipeline_gradio.md` to Korean (#39520)
üåê [i18n-KO] Translated `tokenizer.md` to Korean (#39532)
üåê [i18n-KO] Translated `tvp.md` to Korean (#39578)
üåê [i18n-KO] Translated albert.md to Korean (#39524)
üåê [i18n-KO] Translated `main_classes/peft.md` (#39515)
[modenbert] fix regression (#39750)
add `libcst` to `extras["testing"]` in `setup.py` (#39761)
Fix version issue in modeling_utils.py (#39759)
Enable xpu allocator on caching_allocator_warmup (#39654)
Support loading Qwen3 MoE GGUF (#39638)
Fix GPT2 with cross attention (#39754)
Avoid OOM when other tests are failing (#39758)
AMD disable torchcodec (#39757)
Use `--gpus all` in workflow files (#39752)
Apply several ruff SIM rules   (#37283)
Fix mamba regression (#39728)
Update IMPORTANT_MODELS list (#39734)
update `GemmaIntegrationTest::test_model_2b_bf16_dola` again (#39731)
Fix: add back base model plan (#39733)
[Fix] import two missing typos in `models/__init__.py` for typo checking (#39745)
fix cache inheritance (#39748)
extend more trainer test cases to XPU, all pass (#39652)
BLIPs clean-up  (#35560)
Add Fast Segformer Processor (#37024)
Superpoint fast image processor (#37804)
Fix AMD dockerfile for audio models (#39669)
Fix cache-related tests (#39676)
Fix Layer device placement in Caches (#39732)
Fix `Qwen2AudioForConditionalGeneration.forward()` and `test_flash_attn_kernels_inference_equivalence` (#39503)
skip `Glm4MoeModelTest::test_torch_compile_for_training` (#39670)
Update `QAPipelineTests::test_large_model_course` after #39193 (#39666)
mllama outputs refactor (#39643)
Remove all expired deprecation cycles (#39725)
[`CI`] Add Eric to comment slow ci (#39601)
PATCH: add back n-dim device-mesh + fix tp trainer saving (#39693)
Add self-hosted runner scale set workflow for mi325 CI (#39651)
[configuration] remove redundant `classmethod` (#38812)
update ernie model card (#39657)
[processors] add tests for helper fn (#39629)
xpu optimization for generation case (#39573)
fix(tokenization): check token.content for trie (#39587)
Fix missing initialization of `FastSpeech2Conformer` (#39689)
fix missing model._tp_size from ep refactor (#39688)
More robust tied weight test (#39681)
dev version 4.55
Add padding-free to Granite hybrid moe models  (#39677)
Fix tied weight test (#39680)
fix break for ckpt without _tp_plan (#39658)
Add EXAONE 4.0 model (#39129)
Support `typing.Literal` as type of tool parameters or return value (#39633)
Add ep (#39501)
bad_words_ids no longer slow on mps (#39556)
Add xlstm model (#39665)
Use auto_docstring for perception_lm fast image processor (#39679)
fix: HWIO to OIHW (#39200)
Fix auto_docstring crashing when dependencies are missing (#39564)
Add support for DeepseekAI's DeepseekVL (#36248)
Add missing flag for CacheLayer (#39678)
Add evolla rebase main (#36232)
update expected outputs for whisper after #38778 (#39304)
fix `kyutai` tests (#39416)
Fixes the BC (#39636)
Delete bad rebasing functions (#39672)
[`Ernie 4.5`] Post merge adaptations (#39664)
[CI] revert device in `test_export_static_cache` (#39662)
Fix ModernBERT Decoder model (#39671)
üö®[Fast Image Processor] Force Fast Image Processor for Qwen2_VL/2_5_VL + Refactor (#39591)
Rename huggingface_cli to hf (#39630)
fix(voxtral): correct typo in apply_transcription_request (#39572)
make fixup (#39661)
[docs] fix ko cache docs (#39644)
Make pytorch examples UV-compatible (#39635)
revert change to cu_seqlen_k and max_k when preparing from position_ids (#39653)
Fix: explicit not none check for tensors in flash attention (#39639)
[attention] fix test for packed padfree masking (#39582)
Add owlv2 fast processor (#39041)
revert behavior of _prepare_from_posids (#39622)
[Voxtral] values for A10 runners (#39605)
[timm] new timm pin (#39640)
[efficientloftr] fix model_id in tests (#39621)
Update recent processors for vLLM backend (#39583)
[Docs] Translate audio_classification.md from English to Spanish (#39513)
standardized YOLOS model card according to template in #36979 (#39528)
Feature/standardize opt model card (#39568)
üî¥ Fix EnCodec internals and integration tests (#39431)
Fix DAC integration tests and checkpoint conversion. (#39313)
Move openai import (#39613)
Transformers serve VLM (#39454)
Fix important models CI (#39576)
Fix typos and grammar issues in documentation and code (#39598)
Allow `device_mesh` have multiple dim  (#38949)
enable triton backend on awq xpu (#39443)
[idefics3] fix for vLLM (#39470)
fix moe routing_weights (#39581)
FP-Quant support (#38696)
Rename `supports_static_cache` to `can_compile_fullgraph` (#39505)
[Trackio] Allow single-gpu training and monitor power (#39595)
Generic task-specific base classes (#39584)
Fix DynamicCache and simplify Cache classes a bit (#39590)
Mask2former & Maskformer Fast Image Processor (#35685)
üéØ Trackio integration (#38814)
[WIP] Add OneformerFastImageProcessor (#38343)
Fix link in "Inference server backends" doc (#39589)
Torchdec RuntimeError catch  (#39580)
[Paged-Attention] Handle continuous batching for repetition penalty (#39457)
updated mistral3 model card (#39531)
Update `docs/source/ko/_toctree.yml` (#39516)
[cache refactor] Move all the caching logic to a per-layer approach (#39106)
General weight initialization scheme  (#39579)
Add AMD GPU expectations for LLaVA tests (#39486)
Kernels flash attn (#39474)
Add AMD expectations to Mistral3 tests (#39481)
[docs] Create page on inference servers with transformers backend (#39550)
[docs] update attention implementation and cache docs (#39547)
Add AMD test expectations to DETR model (#39539)
[timm_wrapper] add support for gradient checkpointing (#39287)
Fixes needed for n-d parallelism and TP (#39562)
Bump AMD container for 2.7.1 PyTorch (#39458)
Add EfficientLoFTR model (#36355)
[gemma3] fix bidirectional image mask (#39396)
Update OLMoE model card (#39344)
Update modernbertdecoder docs (#39453)
[`CI`] Fix post merge ernie 4.5 (#39561)
[Fast image processors] Improve handling of image-like inputs other than images (segmentation_maps) (#39489)
[`Ernie 4.5`] Add ernie text models (#39228)
Refactor embedding input/output getter/setter (#39339)
üåê [i18n-KO] Translated `perf_infer_gpu_multi.md` to Korean (#39441)
[Fast image processor] refactor fast image processor glm4v (#39490)
fix ndim check of device_mesh for TP (#39538)
Refactor `MambaCache` to `modeling_mamba.py` (#38086)
Fix Docstring of BarkProcessor (#39546)
use the enable_gqa param in torch.nn.functional.scaled_dot_product_at‚Ä¶ (#39412)
Fix missing initializations for models created in 2023 (#39239)
Raise `TypeError` instead of ValueError for invalid types (#38660)
Fix pylint warnings (#39477)
Fix Qwen Omni integration test (#39553)
üö®üö®üö® [Trainer] Enable `average_tokens_across_devices` by default in `TrainingArguments` (#39395)
Rename `_supports_flash_attn_2` in examples and tests (#39471)
Fix the check in flex test (#39548)
Fix bad tensor shape in failing Hubert test. (#39502)
GLM-4 Update (#39393)
[qwen2 vl] fix packing with all attentions (#39447)
[gemma3] support sequence classification task (#39465)
Fix placeholders replacement logic in auto_docstring (#39433)
Update SAM/SAM HQ attention implementation + fix Cuda sync issues (#39386)
Improve @auto_docstring doc and rename `args_doc.py` to `auto_docstring.py` (#39439)
Add fast image processor SAM (#39385)
Fix BatchEncoding.to() for nested elements (#38985)
[gemma3] Fix do_convert_rgb in image processors. (#39438)
[chat template] return assistant mask in processors (#38545)
[dependencies] Update `datasets` pin (#39500)
Slack CI bot: set default result for non-existing artifacts (#39499)
üö®üö® Fix and simplify attention implementation dispatch and subconfigs handling (#39423)
[dependencies] temporary pyarrow pin (#39496)
Add voxtral (#39429)
Fix typing order (#39467)
Add unified logits_to_keep support to LLMClass (#39472)
[serve] Add speech to text (`/v1/audio/transcriptions`) (#39434)
Update integration_utils.py (#39469)
fix: ImageTextToTextPipeline handles user-defined generation_config (#39374)
Enable some ruff checks for performance and readability (#39383)
Fix convert_and_export_with_cache failures for GPU models (#38976)
Update `GemmaIntegrationTest::test_model_2b_bf16_dola` (#39362)
fix a comment typo in utils.py (#39459)
Use newer typing notation (#38934)
Fix tests due to breaking change in accelerate (#39451)
fix max_length calculating using cu_seq_lens (#39341)
fix(pipelines): QA pipeline returns fewer than top_k results in batch mode (#39193)
Corrections to PR #38642 and enhancements to Wav2Vec2Processor __call__ and pad docstrings (#38822)
create ijepa modelcard (ref : PR  #36979 ). (#39354)
Improve grammar and clarity in perf_hardware.md (#39428)
fix cached file error when repo type is dataset (#36909)
Fix indentation bug in SmolVLM image processor causing KeyError (#39452)
Updated Megatron conversion script for gpt2 checkpoints  (#38969)
[`CI`] Fix partially red CI (#39448)
Fixes #39204: add fallback if get_base_model missing (#39226)
make the loss context manager easier to extend (#39321)
Remove something that should have never been there (#38254)
Fix processor tests (#39450)
[Bugfix] [Quantization] Remove unused init arg (#39324)
Better typing for model.config (#39132)
Fix typo in generation configuration for Janus model weight conversion (#39432)
Responses API in `transformers serve` (#39155)
[cache] make all classes cache compatible finally (#38635)
docs: add missing numpy import to minimal example (#39444)
Remove runtime conditions for type checking (#37340)
Add StableAdamW Optimizer  (#39446)
add test scanner (#39419)
Fix missing definition of diff_file_url in notification service (#39445)
Add¬†cosine_with_min_lr_schedule_with_warmup_lr_rate¬†scheduler in Trainer (#31870)
Change log level from warning to info for scheduled request logging in `ContinuousBatchProcessor` (#39372)
Defaults to adamw_torch_fused for  Pytorch>=2.8 (#37358)
Fix L270 - hasattr("moe_args") returning False error (#38715)
[chat template] add a testcase for kwargs (#39415)
Fixed a bug calculating cross entropy loss in `JetMoeForCausalLM` (#37830)
Remove double soft-max in load-balancing loss. Fixes #39055 . (#39056)
[Core] [Offloading] Fix saving offloaded submodules (#39280)
[autodocstring] add video and audio inputs (#39420)
CI workflow for performed test regressions (#39198)
docs: update LightGlue docs (#39407)
docs: update SuperGlue docs (#39406)
[vlm] fix loading of retrieval VLMs (#39242)
handle training summary when creating modelcard but offline mode is set (#37095)
Remove residual quantization attribute from dequantized models (#39373)
Remove deprecated audio utils functions (#39330)
Fix bugs in pytorch example run_clm when streaming is enabled (#39286)
Fix bugs from pipeline preprocessor overhaul (#39425)
refactor: remove `set_tracer_provider` and `set_meter_provider` calls (#39422)
Fix invalid property (#39384)
set document_question_answering pipeline _load_tokenizer to True (#39411)
Ignore extra position embeddings weights for ESM (#39063)
support loading qwen3 gguf (#38645)
Add ModernBERT Decoder Models - ModernBERT, but trained with CLM! (#38967)
Fix typo in `/v1/models` output payload (#39414)
[refactor] set attention implementation (#38974)
[siglip] fix pooling comment (#39378)
Update phi4_multimodal.md (#38830)
[Docs] Fix typo in CustomTrainer compute_loss method and adjust loss reduction logic (#39391)
Use np.pad instead of np.lib.pad. (#39346)
Totally rewrite how pipelines load preprocessors (#38947)
[examples] fix do_reduce_labels argument for run_semantic_segmentation_no_trainer (#39322)
Fix Lfm2 and common tests (#39398)
Deprecate AutoModelForVision2Seq (#38900)
[Qwen2.5-VL] Fix torch.finfo() TypeError for integer attention_mask_tensor (#39333)
[BLIP] remove cache from Qformer (#39335)
[shieldgemma] fix checkpoint loading (#39348)
Fix overriding Fast Image/Video Processors instance attributes affect other instances (#39363)
update docker file to use latest `timm` (for `perception_lm`) (#39380)
Update Model Card for Encoder Decoder Model (#39272)
fix gpt2 usage doc (#39351)
Updated CamemBERT model card to new standardized format (#39227)
Update Readme to Run Multiple Choice Script from Example Directory (#39323)
Add mistral common support (#38906)
Remove device check in HQQ quantizer (#39299)
Verbose error in fix mode for utils/check_docstrings.py (#38915)
fix failing `test_sdpa_can_dispatch_on_flash` (#39259)
update cb TP (#39361)
Fix link for testpypi (#39360)
PerceptionLM (#37878)
Updated Switch Transformers model card with standardized format (Issue #36979) (#39305)
[modular] speedup check_modular_conversion with multiprocessing (#37456)
Add a default value for `position_ids` in masking_utils (#39310)
[Core] [Offloading] Enable saving offloaded models with multiple shared tensor groups (#39263)
[tests] tag serve tests as slow  (#39343)
[modeling][lfm2] LFM2: Remove deprecated seen_tokens (#39342)
LFM2 (#39340)
[server] add tests and fix passing a custom `generation_config` (#39230)
Handle DAC conversion when using weight_norm with newer PyTorch versions (#36393)
fix `phi3` tests (#39312)
fix Glm4v batch videos forward (#39172)
Delete deprecated stuff (#38838)
Fix broken SAM after #39120 (#39289)
enable static cache on TP model (#39164)
Fix `max_length_q` and `max_length_k` types to `flash_attn_varlen_func` (#37206)
Granite speech speedups (#39197)
Fix typo: langauge -> language (#39317)
docs: update LLaVA-NeXT model card (#38894)
skip files in `src/` for doctest (for now) (#39316)
Updated the Model docs - for the MARIAN model (#39138)
add `stevhliu` to the list in `self-comment-ci.yml` (#39315)
Fix consistency and a few docstrings warnings (#39314)
üåê [i18n-KO] Translated quark.md to Korean (#39268)
Add DeepSeek V2 Model into Transformers (#36400)
[sliding window] revert and deprecate (#39301)
[modular] Allow method with the same name in case of @property decorator (#39308)
skip `test_torchscript_*` for now until the majority of the community ask for it (#39307)
fix `aria` tests (#39277)
[flash attn 3] bring back flags (#39294)
Fix SDPA attention precision issue in Qwen2.5-VL (#37363)
[Tests] Update model_id in AIMv2 Tests (#39281)
Update T5gemma (#39210)
Add torchcodec in docstrings/tests for `datasets` 4.0 (#39156)
[lightglue] add support for remote code DISK keypoint detector (#39253)
fix flaky `test_generate_compile_model_forward` (#39276)
Refactor `PretrainedConfig.__init__` method to make it more explicit (#39158)
[smollm3] add tokenizer mapping for `smollm3` (#39271)
[pagged-attention] fix off-by-1 error in pagged attention generation (#39258)
[CI] fix docs (#39273)
Add Aimv2 model (#36625)
Add Doge model (#35891)
Fix errors when use verl to train GLM4.1v model (#39199)
fix recompiles due to instance key, and deepcopy issues (#39270)
fix(generation): stop beam search per-instance when heuristic satisfied (#38778)
remove broken block (#39255)
Skip `test_eager_matches sdpa generate` and update an integration test for blip-like models (#39248)
Fix license text, duplicate assignment, and typo in constant names (#39250)
fix xpu failures on PT 2.7 and 2.8 w/o IPEX and enable hqq cases on XPU (#39187)
Glm 4 doc (#39247)
Update LED model card (#39233)
fix some flaky tests in `tests/generation/test_utils.py` (#39254)
Simplify Mixtral and its modular children (#39252)
Add `segmentation_maps` support to MobileNetV2ImageProcessor (#37312)
Clarify per_device_train_batch_size scaling in TrainingArguments (#38‚Ä¶ (#38857)
Add Korean translation for glossary.md (#38804)
Update tiny-agents example (#39245)
adjust input and output texts for test_modeling_recurrent_gemma.py (#39190)
enable xpu on kv-cache and hqq doc (#39246)
Fix patch helper (#39216)
RotaryEmbeddings change `is not None` -> `isinstance(..., dict)` (#39145)
fix `fastspeech2_conformer` tests (#39229)
[bugfix] fix flash attention 2 unavailable error on Ascend NPU (#39166)
[modular] Simplify logic and docstring handling (#39185)
Make _compute_dynamic_ntk_parameters exportable (#39171)
fix bug using FSDP V1 will lead to model device not properly set (#39177)
Don't send new comment if the previous one is less than 30 minutes (unless the content is changed) (#39170)
fix typo in Gemma3n notes (#39196)
[modular] Follow global indexing and attribute setting, and their dependencies (#39180)
Fix missing fast tokenizer/image_processor in whisper/qwen2.5-omni processor (#39244)
[vjepa2] replace einsum with unsqueeze (#39234)
Expectations re-order and corrected FA3 skip (#39195)
[video processors] Support float fps for precise frame sampling (#39134)
Refactor the way we handle outputs for new llamas and new models (#39120)
Update expected values (after switching to A10) - part 8 - Final (#39220)
Update expected values (after switching to A10) - part 7 (#39218)
Add packed tensor format support for flex/sdpa/eager through the mask! (#39194)
Update expected values (after switching to A10) - part 6 (#39207)
Update expected values (after switching to A10) - part 5 (#39205)
Fix continuous batching in `transformers serve` (#39149)
[serve] Cursor support, move docs into separate page, add more examples (#39133)
[typing] better return typehints for `from_pretrained` (#39184)
Update expected values (after switching to A10) - part 4 (#39189)
[`Dia`] Change ckpt path in docs (#39181)
Fix many HPU failures in the CI (#39066)
Decouple device_map='auto' and tp_plan='auto'  (#38942)
when delaying optimizer creation only prepare the model (#39152)
[glm4v] fix video inference (#39174)
Test fixes for Aria (and some Expectation for llava_next_video) (#39131)
Update expected values (after switching to A10) - part 3 (#39179)
Update expected values (after switching to A10) - part 2 (#39165)
Random serve fixes (#39176)
[serve] Model name or path should be required (#39178)
[generate] document non-canonical beam search default behavior (#39000)
[docs] ViTPose (#38630)
Reduce Glm4v model test size significantly (#39173)
Fix missing initializations for models created in 2024 (#38987)
Blip2 fixes (#39080)
Fix multimodal processor get duplicate arguments when receive kwargs for initialization (#39125)
üö®üö®üö® [eomt] make EoMT compatible with pipeline (#39122)
[smolvlm] fix video inference (#39147)
fix default value of config to match checkpionts in LLaVa-OV models (#39163)
Add activation sparsity reference in gemma3n doc (#39160)
fix `llama` tests (#39161)
Update expected values (after switching to A10) (#39157)
Suggest jobs to use in `run-slow` (#39100)
update bnb ground truth (#39117)
fix: remove undefined variable (#39146)
Change `@lru_cache()` to `@lru_cache` to match styles from #38883. (#39093)
Fix: Ensure wandb logs config in offline mode (#38992)
Fix missing fsdp & trainer jobs in daily CI (#39153)
[superglue] fix wrong concatenation which made batching results wrong (#38850)
[VLMs] support passing embeds along with pixels (#38467)
[typing] LlamaAttention return typehint  (#38998)
[qwen2-vl] fix FA2 inference (#39121)
feat: support indivisible shards for TP model loading and TPlizing. (#37220)
fix caching_allocator_warmup with tie weights (#39070)
üö® Don't use cache in non-generative models (#38751)
Several fixes for Gemma3n (#39135)
Fix key mapping for VLMs (#39029)
[Whisper] update token timestamps tests (#39126)
Update BigBirdPegasus model card (#39104)
switch default xpu tp backend to pytorch built-in XCCL from pytorch 2.8 (#39024)
docs: correct two typos in awesome-transformers.md (#39102)
Enable XPU doc (#38929)
Fix chat (#39128)
Licenses (#39127)
Split `transformers chat` and `transformers serve`  (#38443)
All CI jobs with A10 (#39119)
docs: Gemma 3n audio encoder (#39087)
Fix some bug for finetune and batch infer For GLM-4.1V (#39090)
fix UT failures on XPU w/ stock PyTorch 2.7 & 2.8 (#39116)
skip some `test_sdpa_can_dispatch_on_flash` (#39092)
Fixes the failing test `test_is_split_into_words` in `test_pipelines_token_classification.py` (#39079)
Sandeepyadav1478/2025 06 19 deberta v2 model card update (#38895)
[fix] Add FastSpeech2ConformerWithHifiGan (#38207)
TST Fix PEFT integration test bitsandbytes config (#39082)
Fix: unprotected import of tp plugin (#39083)
Add Fast Image Processor for Chameleon (#37140)
fix `dots1` tests (#39088)
guard torch distributed check (#39057)
Add Fast Image Processor for mobileViT (#37143)
add fast image processor nougat (#37661)
TST PEFT integration tests with pipeline generate (#39086)
fixed typo for docstring in prepare_inputs method (#39071)
fix `mistral3` tests (#38989)
[Whisper] üö® Fix pipeline word timestamp: timestamp token is end of token time !!! (#36632)
Pipeline: fix unnecessary warnings (#35753)
‚ú® Add EoMT Model ||  üö® Fix Mask2Former loss calculation (#37610)
fix a bunch of XPU UT failures on stock PyTorch 2.7 and 2.8 (#39069)
Uninstallling Flash attention from quantization docker (#39078)
Fix initialization of OneFormer (#38901)
fix `Gemma3nProcessorTest` (#39068)
Cleanup Attention class for Siglip and dependent models (#39040)
[Whisper] fix shape mismatch in tests (#39074)
[docs] Tensor parallelism (#38241)
[docs] @auto_docstring (#39011)
Update PEGASUS-X model card (#38971)
[docs] Model contribution (#38995)
fix `layoutlmv3` tests (#39050)
Update SuperPoint model card (#38896)
fix `t5gemma` tests (#39052)
fix `test_compare_unprocessed_logit_scores` (#39053)
[`Flex Attn`] Fix torch 2.5.1 incompatibilities (#37406)
Dev version
[Modeling] Fix encoder CPU offloading for whisper (#38994)
Gemma 3n (#39059)
[tests] remove tests from libraries with deprecated support (flax, tensorflow_text, ...) (#39051)
[Whisper] Pipeline: handle long form generation (#35750)
add _keep_in_fp32_modules_strict (#39058)
fix condition where torch_dtype auto collides with model_kwargs. (#39054)
[qwen2-vl] fix vision attention scaling (#39043)
polishing docs: error fixes for clarity (#39042)
Create test for #38916 (custom generate from local dir with imports) (#39015)
Internvl fix (#38946)
[`Generate`] Fix no grad on some models (#39008)
Add Dia model (#38405)
Fix Bad Outputs in Fast Path for GraniteMoeHybrid (#39033)
Granite speech speedup + model saving bugfix (#39028)
[tests] remove TF tests (uses of `require_tf`) (#38944)
Two ReDOS fixes (#39013)
[Kyutai-STT] correct model type + model id (#39035)
Add SmolLM3 (#38755)
refactor: remove custom BarkLayerNorm (#39003)
Fix grammatical error in models documentation (#39019)
Remove script datasets in tests (#38940)
fix gemma3 grad acc (#37208)
fix: astronomical loss with ModernBERT when using gradient checkpointing (#38982) (#38983)
Support for Flash Attention 3 (#38972)
Fix the seamless_m4t cannot work on Gaudi (#38363)
[Model] add dots1 (#38143)
Encoder-Decoder Gemma (#38332)
GLM-4.1V Model support (#38431)
Drop unnecessary tokens in GPT2Model generation (#39016)
[video processor] support torchcodec and decrease cuda memory usage (#38880)
[AutoModelForMaskGeneration] Remove duplicate code (#38622)
Fix graph break in torch.compile when using FA2 with attention_mask=None and batch size > 1 (#37332)
Add zero dim tensor check when using flash_attention (#38280)
[LightGlue] Fixed attribute usage from descriptor_dim to keypoint_detector_descriptor_dim (#39021)
Add Hugging Face authentication procedure for IDEs (PyCharm, VS Code,‚Ä¶ (#38954)
[HPU][Critical Issue Fix] ThreadPool instead of Pool for parallel pre-processing (#39002)
Skip sdpa dispatch on flash test due to unsupported head dims (#39010)
Update self-comment-ci.yml user list (#39014)
Fix bugs in DynamicCache (#37880)
Add kyutai stt (#38909)
Add kernelize to transformers (#38205)
Granite speech - minor fixes to support training with the HF trainer (#38833)
Fix undeterministic order in modular dependencies (#39005)
Skip non-selected experts for qwen3_moe (#38133)
Update attention_visualizer.py (#37860)
Added scikit-learn to the example image-classification requirements.txt (#37506)
Fixes for Arcee model (#39001)
Add Arcee model support (#38621)
[`Attention`] Small fix on output attentions (#38948)
Removing extra space in large command for speech-pretraining example (#38705)
[qwen] refactor attentions for vision/audio (#38930)
üî¥ Update default `dtype` for pipelines to `auto` (#38882)
[docs] Typos - Single GPU efficient training features (#38964)
Fix `rag` (#38585)
[Feature] Support `is_split_into_words` in the `TokenClassificationPipeline`. (#38818)
fix `mistral` and `mistral3` tests (#38978)
Add support for auto_docstring with model outputs (#38242)
fix: add __bool__ operator to tokenizer to avoid bloated asserts (#38899)
Add Idefics2/3 and SmolVLM Fast image processors + improvements for fast image processors (#38157)
Break tie in Expectations and gemma3 fixes (#38943)
Apply GradientCheckpointingLayer to the whole repo (#38913)
Remove dead protected imports (#38980)
[modular] CLI allows positional arguments, and more defaults names for the optional arg (#38979)
Fix(informer): Correct tensor shape for input_size=1 (#38856)
Fix DTensor import compatibility for PyTorch < 2.5 (#38836)
Gaudi3 CI (#38790)
Update blip model card (#38513)
Fix custom generate from local directory (#38916)
Switch to use A10 progressively (#38936)
Fix more flaky `test_initialization` (#38932)
Correctly raise error for awq quantization (#38945)
Pin PyTorch extras for AMD containers (#38941)
Add kwargs for timm.create_model in TimmWrapper (#38860)
[static cache] fix device map per layer in VLMs (#38488)
Remove `ALL_LAYERNORM_LAYERS` (#38922)
add pytorch-xpu Dockerfile (#38875)
Modernbert fixes (#38912)
Skip some tests for now (#38931)
Remove deprecated classes in modeling_utils.py (#38919)
feat: add flexible Liger Kernel configuration to TrainingArguments (#38911)
Allow make-fixup on main branch, albeit slowly (#38892)
feat: Add granite architectures to auto tokenizer name mappings (#38802)
Fix ReDOS in tokenizer digit substitution (#38844)
Skip sdpa tests if submodule does not support sdpa (#38907)
Fix `FalconMambaIntegrationTests` (#38566)
align xpu's autocast behavior w/ cuda by using device agnostic torch APIs (#38284)
Fix unnecessary super calls (#38897)
Fix `fsmt` tests (#38904)
[phi-4] use mel filters from audio utils (#36966)
Use `raise from e` in `hub.py` utility (#37241)
Add support for specifying revisions when pushing to Hub via internal Trainer call (#36852)
Update bamba model card (#38853)
[video processor] fix slow tests (#38881)
36978 | Fast image processor for DPT model (#37481)
Docs: Add custom fine-tuning tutorial to TrOCR model page (#38847)
log: Add logging when using split_batches and per_device_train_batch_size (#38633)
[bugfix] fix ATTN_MASK_NPU device mismatch error on multi-device NPU ‚Ä¶ (#38876)
Fix loop var naming (#38885)
More PYUP fixes (#38883)
null deepspeed_plugin in args for wandb callback fake trainer (#38867)
Fixed markdown for BertTokenizer's '[CLS]' token. (#38506)
Fix HQQ model param device transfer issue (#38466)
Fix `qwen3_moe` tests (#38865)
üö®üö® Fix initialization of Mask2Former (#38864)
Fix `phi4_multimodal` tests (#38816)
enable misc test cases on XPU (#38852)
Post-PR fixes! (#38868)
No more Tuple, List, Dict (#38797)
Update roc bert docs (#38835)
Update CvT documentation with improved usage examples and additional ‚Ä¶ (#38731)
Add LightGlue model (#31718)
Fix `qwen3` tests (#38862)
Improve `auxiliary_in_channels` default behavior in UperNet (#37540)
Fix `qwen2_5_vl` tests (#38845)
Allow customization of sdpa in executorch.py (#38827)
Fix incorrect width ratio calculation in Llama4 image processor (#38842)
[video processor] fix BC when no video config if found (#38840)
Remove merge conflict artifacts in Albert model doc (#38849)
Updated aya_vision.md (#38749)
GraniteMoeHybrid: Allow for only shared expert case. (#38801)
[BugFix] QA pipeline edge case: `align_to_words=True` in `QuestionAnsweringPipeline` can lead to duplicate answers (#38761)
Fix broken tag in Longformer model card (#38828)
Fix broken notebooks link in Italian training docs (#38834)
Fix peft integration (#38841)
add default mapping to peft integration
bugfix: propage weight key_mapping to peft to fix 3.52 VLM renaming  (#38627)
Fix redundant code in Janus (#38826)
[internvl] fix video inference (#38811)
Updated Albert model Card (#37753)
[docs] updated roberta model card (#38777)
[docs] Update docs moved to the course (#38800)
fixed docstring in modular_qwen2_5_vl.py (#38798)
Add V-JEPA for video classification model (#38788)
Fix trainer.py not showing signature columns (#38465)
Fix a minor security issue (#38815)
change fsdp_strategy to fsdp in TrainingArguments in accelerate doc (#38807)
Refactor DBRX tests to use CausalLMModelTest base classes (#38475)
Use `wandb.run.url` instead of `wandb.run.get_url()` (deprecated) (#38817)
Expectation fixes and added AMD expectations (#38729)
Fix `llava_next` tests (#38813)
Better pipeline type hints ‚ú® (#38049)
Simplify and update trl examples (#38772)
Use HF papers (#38184)
Disable custom MRA kernels for ROCm (#38738)
Unbreak optimum-executorch (#38646)
Fix configs and doc for the Qwens (#38808)
Fix erroneous docstring for the ordering of SWA layers (#38794)
[docs] update cache docs with new info (#38775)
refactor create_token_type_ids_from_sequences (#37681)
Updated moonshine modelcard (#38711)
Add missing div in Pegasus model card (#38773)
[Docs] New DiT model card (#38721)
Remove all traces of `low_cpu_mem_usage` (#38792)
build: :pushpin: Remove upper bound on PyTorch (#38789)
Fix `mllama` (#38704)
Initialize flash attn flag (#38768)
Fix Typos in Comments: "quantitation" ‚Üí "quantization", "averege" ‚Üí "average" (#38766)
Reword README in light of model definitions (#38762)
Fix `llava_onevision` tests (#38791)
Fix `qwen_2_5 omni` (#38658)
[docs] Add int4wo + 2:4 sparsity example to TorchAO README (#38592)
Update PULL_REQUEST_TEMPLATE.md (#38770)
Reduce verbosity for `average_tokens_across_devices=True` and `world size = 1` (#38785)
Skip some export tests on torch 2.7 (#38677)
[video processors] support frame sampling within processors (#38105)
Fix masking utils (#38783)
[Hotfix] Fix style bot  (#38779)
[masking utils] check `None` instead of try/except (#38561)
Add Qwen2 MoE model card (#38649)
Update altCLIP model card (#38306)
chore(pixtral): emit block attention mask when using flash attention (#38741)
Make style bot trigger CI after push (#38754)
Update pegasus model card (#38675)
fix(qwen3_moe): pass kwargs to self_attn (#38691)
Deprecate TF + JAX (#38758)
Update repo consistency check (#38763)
Remove IPEX requirement for bitsandbytes on CPU (#38594)
Prepare for TF+Jax deprecation (#38760)
Better typing for num_items_in_batch (#38728)
Add V-JEPA 2 (#38746)
Add z-loss to Bamba for v2 (#37842)
Revert "Trigger doc-builder job after style bot" (#38735)
[DeepSeek-V3] implement when q_lora_rank is None (#38743)
fix: bf16 with TPU is allowed in configuration (#38670)
from 1.11.0, torchao.prototype.low_bit_optim is promoted to torchao.optim (#38689)
fix: Add method to get image features in PaliGemmaForConditionalGeneration (#38730)
[llava] fix integration tests with Siglip (#38732)
Fixed a multiple-devices issue in SmolVLM model (#38736)
New canine model card (#38631)
Add AGENTS.md (#38734)
Fix typo in Language Modeling example scripts and update TPU type (#38652)
[add-new-model-like] Robust search & proper outer '),' in tokenizer mapping (#38703)
Use OSError (#38712)
Fix `llava` tests (#38722)
Logging message for ``` is_bitsandbytes_available() ```  (#38528)
Update some tests for torch 2.7.1 (#38701)
Fix smart resize (#38706)
Standardize ByT5 model card format (#38699)
Fix `aya_vision` test (#38674)
Created model card for xlm-roberta-xl (#38597)
Update XLM-RoBERTa model documentation with enhanced usage examples and improved layout (#38596)
Created model card for XLM model (#38595)
Drop as_target_processor from the _call_ and pad methods (#38642)
Docs: update bitsandbytes torch.compile compatibility (#38651)
Fix TypeError: 'NoneType' object is not iterable for esm (#38667) (#38668)
Fix retrieve function signature and remove faiss requirement (#38624)
Fix some models import (#38694)
Fix attention mask expansion when converting to executorch (#38637)
fix: "check out" as verb (#38678)
Fixed modeling_auto.py MODEL_FOR_MASK_GENERATION_MAPPING_NAMES variable (#38664)
Fix qwen2-audio chat template audio placeholder insertion (#38640)
Use torch 2.7.1 on daily CI (#38620)
Fix `InternVL` integration test (#38612)
Skip torchscript tests for 2 models (#38643)
remove ipex_optimize_model usage (#38632)
Better CI (#38552)
fix torch_dtype on awq (#38463)
fix total batch size calculation in trainer (#38286)
Don't run `AriaForConditionalGenerationModelTest` on CircleCI (#38615)
fix: support grad clipping for TP through replicating non-sharded modules (#36132)
Improve `test_initialization` for `SwiftFormer` (#38636)
update `ColQwen2ModelIntegrationTest` (#38583)
[generation] bring back tests on vision models (#38603)
Use torch 2.7.1 on CircleCI jobs (#37856)
Improve `test_initialization` (#38607)
enable more test cases on xpu (#38572)
Fix `MiniMax` (docs and integration tests checkpoint) (#38575)
Updated Aria model card (#38472)
[Nit] Add Note on SigOpt being in Public Archive Mode (#38610)
Fix typo in LLaVa documentation (#38618)
docs: fix dark mode logo display. (#38586)
Fix `return_dict=False` giving errors in a few VLM models (#38519)
Remove `isort` from dependencies (#38616)
fix spelling errors  (#38608)
Avoid overwrite existing local implementation when loading remote custom model (#38474)
Allow `mlm_probability` to be set to `None` when `mlm=False` in DataCollatorForLanguageModeling (#38522) (#38537)
Bump torch from 2.6.0 to 2.7.1 in /examples/flax/vision (#38606)
pin pandas (#38605)
Remove custom pytest and pluggy (#38589)
[qwen-omni] fix sliding window (#38525)
added fast image processor for ZoeDepth and expanded tests accordingly (#38515)
Updated deprecated typing imports with equivalents for Python 3.9+ (#38546)
New gpt neo model card (#38505)
tests/roformer: fix couple roformer tests on gpus (#38570)
[Dinov2] Enable device_map="auto" support (#38487)
feat: add `repository` field to benchmarks table (#38582)
Docs: fix code formatting in torchao docs (#38504)
allow custom head_dim for qwen2_moe (#37188)
fix(attention_visualizer): add default value for image_seq_length (#38577)
[`FlexAttn`] Fix models with unique characteristics (#38433)
Fix `deepseekv3` (#38562)
update `utils/notification_service.py` for AMD vs Nvidia (#38563)
Fix `chameleon` tests (#38565)
Add support for MiniMax's MiniMax-Text-01 (#35831)
[janus] Fix failing tests on mi3XX (#38426)
[docs] Format fix (#38414)
Fix hqq issue (#38551)
Name change AOPermod -> ModuleFqn (#38456)
Fix `utils/notification_service.py` (#38556)
Explicitly setting encoding in tokenization_utils_base.py (#38553)
[TP] Change command in tests to `python3` (#38555)
[bugfix] [WIP] fix apply_rotary_emb error on Ascend NPU (#38491)
Update docker image to use `av` (#38548)
update emu3 test (#38543)
Don't use default attn if pre-set in sub-config (#38526)
[tests] expand flex-attn test for vision models (#38434)
Fix blip2 tests (#38510)
Fix `Gemma2IntegrationTest` (#38492)
Remove type annotation in Siglip Attention Module (#38503)
Num parameters in model.safetensors.index.json (#38531)
[flax/mistral] support sliding_window: null in config (#37402)
Fix amp deprecation issue (#38100)
remove unhandled parameter (#38145)
Add ColQwen2 to ü§ó transformers (#35778)
[generate] move `SinkCache` to a `custom_generate` repo (#38399)
[generate] add soft deprecations on custom generation methods (#38406)
Update Loss Functions to Accept Tensor num_items_in_batch (#38029)
[seamless_m4t] Skip some tests when speech is not available (#38430)
Fix setting FLASH_ATTENTION_DETERMINISTIC after importing (#37185)
Remove deprecated use_flash_attention_2 parameter (#37131)
[docs] add xpu environment variable for gpu selection (#38194)
protect dtensor import  (#38496)
Align TP check (#38328)
[Tests] Reduced model size for albert-test model (#38480)
Bump torch from 2.2.0 to 2.6.0 in /examples/flax/vision (#37618)
Fix incorrect bbox_embed initialization when decoder_bbox_embed_share=False in GroundingDINO (#38238)
Fix convert_internvl_weights_to_hf.py to support local paths (#38264)
make it go brrrr (#38409)
fix: handle no scheduler passed by user (#38407)
[Qwen2.5-Omni] Fix dtype of cos,sin when used with flash attention (#38453)
Fix `Gemma3IntegrationTest` (#38471)
Cleanup `BatchFeature` and `BatchEncoding` (#38459)
Fix TypeError in save_pretrained error handling (fixes #38422) (#38449)
üî¥ [VLM] modeling updates (#38317)
[Tests] Clean up test cases for few models (#38315)
feat: add cache retention for requests (#38446)
Fix GLM4 checkpoints (#38412)
Merge type hints from `microsoft/python-type-stubs` (post dropping support for Python 3.8) (#38335)
Model card for mobilenet v1 and v2 (#37948)
Updated the model card for ViTMAE (#38302)
Updated the Model docs - for the ALIGN model (#38072)
Fix handling of slow/fast image processors in image_processing_auto.py (#38161)
Fix `from_args_and_dict` ProcessorMixin (#38296)
Fix MoE gradient test (#38438)
Remove redundant test_sdpa_equivalence test (#38436)
Trigger doc-builder job after style bot (#38398)
Fix convert weights for InternVL (#38233)
Fix typo in tokenization_utils_base.py docstring (#38418)
[core] support tensor-valued _extra_state values in `from_pretrained` (#38155)
üî¥[`Attention`] Attention refactor for Whisper-based models (#38235)
make Llama4TextMoe forward more readable (#37529)
Fix CircleCI not triggered when PR is opened from a branch of `huggingface/transformers` (#38413)
Update error when using additional and/or masks (#38429)
Disable mi210 scheduled CI (#38411)
enable large_gpu and torchao cases on XPU (#38355)
Update `CsmForConditionalGenerationIntegrationTest` (#38424)
[qwen-vl] Look for vocab size in text config (#38372)
Fix an error in verify_tp_plan for keys without '.' (#38420)
Change slack channel for mi250 CI (#38410)
Add mi300 to amd daily ci workflows definition (#38415)
Updated model card for OLMo2 (#38394)
Falcon-H1 - Fix auto_docstring and add can_return_tuple decorator (#38260)
Update granite.md (#37791)
New bart model card (#37858)
Updated BERTweet model card. (#37981)
Updated BigBird Model card as per #36979. (#37959)
Updated Zoedepth model card (#37898)
Update Model Card for Mamba-2 (#37951)
[mllama] Allow `pixel_values` with `inputs_embeds` (#38334)
[tests] remove overload for deleted test (`test_offloaded_cache_implementation`) (#37896)
[cleanup] delete deprecated kwargs in qwen2_audio üßπ  (#38404)
[CSM] update model id (#38211)
Add report_repo_id to mi300 workflow (#38401)
[CSM] infer codec model with no_grad + audio eos label (#38215)
Fix Qwen2.5-VL Video Processor (#38366)
[chat] use the checkpoint's `generation_config.json` as base parameterization (#38330)
Fix convert to original state dict for VLMs (#38385)
[chat] improvements for thinking models and reduce default verbosity (#38322)
guard size mismatch check to only quantized models (#38397)
[aya vision] fix processor for vLLM (#38371)
[video utils] group and reorder by number of frames (#38374)
[paligemma] fix processor with suffix (#38365)
[transformers x vLLM] standardize processors (#37915)
Fix image token mask in Gemma3 (#38295)
Add AMD MI300 CI caller leveraging self-hosted runner scale set workflow in hf-workflows (#38132)
Stop autoconverting custom code checkpoints (#37751)
update gemma tests (#38384)
[cli] cli usable without torch (#38386)
:rotating_light: :rotating_light: Fix custom code saving (#37716)
Stop TF weight rename reDOS (#38325)
fix typo: `tokenizer` -> `tokenize` (#38357)
fix typos (#38336)
Better check in `initialize_weights` (#38382)
Use one `utils/notification_service.py` (#38379)
for now disable compile (#38383)
Improved cache docs (#38060)
[Falcon H1] Fix slow path forward pass (#38320)
Protect `get_default_device` for torch<2.3 (#38376)
Fix incorrect batching audio index calculation for Phi-4-Multimodal  (#38103)
Fix all import errors based on older torch versions (#38370)
[`OPT`] Fix attention scaling (#38290)
switch to device agnostic device calling for test cases (#38247)
[VLMs] add helpers for get/set embedding (#38144)
Uninstall `kernels` for AMD docker images (#38354)
Hot fix for AMD CI workflow (#38349)
new failure CI reports for all jobs  (#38298)
[docs]: update roformer.md model card (#37946)
docs(swinv2): Update SwinV2 model card to new standard format (#37942)
Update BioGPT model card (#38214)
Remove duplicate docstring: resample (#38305)
Never fallback to eager implicitly (#38327)
Use Gradient Checkpointing Layer in Jamba & Blip Related Models (#38310)
:rotating_light: :rotating_light: Inherited CausalLM Tests (#37590)
Enhance Model Loading By Providing Parallelism, Uses Optional Env Flag (#36835)
[`FlexAttention`] Reenable flex for encoder-decoder and make the test more robust (#38321)
refactor can_save_slow_tokenizer (#37722)
[performance_optim] reduce frequency of declaring attention_mask in Ascend NPU flash attention (#38278)
üö®Early-errorüö® config will error out if `output_attentions=True` and the attn implementation is wrong (#38288)
Fix some tests (especially compile with fullgraph=True on Python<3.11) (#38319)
add `vasqu` to `self-comment-ci.yml` (#38324)
[custom_generate] don't forward `custom_generate` and `trust_remote_code` (#38304)
Expose AutoModelForTimeSeriesPrediction for import (#38307)
[Whisper + beam search] fix usage of `beam_indices` (#38259)
[tf/flax] handle `forced_decoder_ids` deletion (#38316)
Adds use_repr to model_addition_debugger_context (#37984)
Fix typo: change 'env' to 'environment' in .circleci/config.yml (#38273)
Fix run_slow (#38314)
[emu3] fix conversion script (#38297)
[Tests] Cleanup Janus Testcase (#38311)
Oups typo for HybridChunkedCache (#38303)
Add CB (#38085)
Fix HybridChunedCache & Llama4 (#38299)
üî¥üî¥üî¥ [`Attention`] Refactor Attention Interface for Bart-based Models (#38108)
Update CI Docker base image for AMD tests (#38261)
refine `transformers env` output (#38274)
More typing in src/transformers/training_args.py (#38106)
Fix tp error when torch distributed is already initialized (#38294)
add `liger-kernel` to docker file (#38292)
üö®üö®[core] Completely rewrite the masking logic for all attentions (#37866)
[Whisper] handle deprecation of `forced_decoder_ids` (#38232)
[whisper] move processor test into processor test file üßπ  (#38266)
add XPU info print in print_env (#38282)
docs(swin): Update Swin model card to standard format (#37628)
Update Model Card for Mamba (#37863)
Protect ParallelInterface (#38262)
Remove Japanese sequence_classification doc and update references (#38246)
assign the correct torchao data layout for xpu (#37781)
Fix: missing else branch to handle "--load_best_model_at_end" in training_args.py (#38217)
Improve typing in TrainingArgument (#36944)
Simplify DTensor Check for modeling_utils.py (#38245)
[whisper] small changes for faster tests (#38236)
Clearer error on import failure (#38257)
Add tearDown method to Quark to solve OOM issues (#38234)
fix multi-image case for llava-onevision (#38084)
[`compile`] re-enable for Qwen-VL models (#38127)
[Falcon H1] Fix Typo in Integration Test (#38256)
[MODEL] Add Falcon H1 (#38249)
tp plan should not be NONE (#38255)
Revert parallelism temporarily (#38240)
CI reporting improvements (#38230)
Protect ParallelInterface
v4.53.0.dev0
[gemma3] fix bidirectional attention mask (#38080)
[mllama] fix loading and inference (#38223)
Add padding-free to bamba (#35861)
Fixing Bitnet after use_rms_norm introduction (#38229)
Enable Quantize KV Cache for Mistral Model (#35042)
parallelism goes brrr (#37877)
Fix Llama4 (#38222)
Mamba2 remove unecessary test parameterization (#38227)
Minor llama4 fixes (#38123)
fix dead flax links modeling_flax_pytorch_utils.py (#38212)
Make `train_dataset` attribute in `_get_train_sampler` optional  (#38226)
In Llama4 fix wrongly inverted causal attention mask when using SDPA implementation (#38094)
Disable torchscript tests for AriaForConditionalGenerationModelTest (#38225)
Add support to Marimo Notebooks and Enverge.ai (#38210)
New cache tests and refactored Hybrid Cache (#37972)
Add `Llama4TextModel` to `AutoModel` mapping (#38162)
Remove trust_remote_code=True tests from bnb quantization tests (MPT now integrated) (#38206)
[fix] sliding window attention mask (#38045)
Fix broken example generation script for Llama3 (#38062)
Fix: make docs work better with doc builder (#38213)
enable misc cases on XPU & use device agnostic APIs for cases in tests (#38192)
Qwen2.5-Omni: Update modeling_qwen2_5_omni.py to fix error when loading quantized weights with AutoAWQ.  (#38013)
Feat: save_pretrained for tensor parallel (and other parallelisms) models (#37919)
[doc] fix bugs in `how_to_hack_models.md` (#38198)
Translating model_doc/bert.md to Chinese (#37806)
Tensor parallel docs (#38178)
üö®üö®üö®  [pipelines] update defaults in pipelines that can `generate` (#38129)
[image-text-to-text pipeline] Accept a chat as a positional arg (#38204)
[SAM-HQ] Update names in the docs (#38058)
Remove Deprecated `verbose` arg in LayerWiseDummyScheduler (#38197)
Make HF implementation match original OLMo 2 models for lower precisions (#38131)
[docs] add Audio import (#38195)
[docs] minor fixes in `models.md` (#38193)
Pass `eps` to `Mistral3RMSNorm` (#38026)
Resolve Python logger warnings (#38183)
Support for transformers explicit filename (#38152)
[generation] Less verbose warnings by default (#38179)
Add adam_kwargs for Apollo Optimizer (#38168)
Refactor `get_XXX_dataloader` from Trainer (#38090)
[tests] remove `test_sdpa_equivalence` (redundant) (#37911)
fix bug in distributed loss test (#38166)
Fix import torchao.prototype.low_bit_optim since torchao  v0.11 (#38174)
Add args support for fast image processors (#37018)
[ESM] Add flash-attention-2 backend for ESM-2 (#38023)
Feat: add warnings for unused keys and rules in tensor parallel (#37893)
remove some commands from `fetch_tests` CircleCI job (#38176)
Disable `convert to draft` workflow (#38177)
Disable `Trigger CircleCI by ready for review` (#38171)
clean autoawq cases on xpu (#38163)
Bart: new cache format (#35314)
[VLMs] add helpers to get multimodal encodings (#37743)
Add optional RMSNorm support to BitNet quantization (config + layers) (#38087)
Fix Qwen2.5 Omni `SinusoidsPositionEmbedding` precision (#38151)
Include output embedding as well with `include_embedding` flag (#37935)
enable autoround cases on XPU (#38167)
[FIX] Save speed metrics to logs (#38136)
Omit creation of positional IDs within ESM if applicable (#38089)
disable deepspeed when setting up fake trainer (#38101)
enable trainer test cases on xpu (#38138)
Hotfix: Flash Attention 2 support in Pixtral (#38146)
[generate] Run custom generation code from the Hub (#36405)
Remove head mask in generative models (#35786)
enable csm integration cases on xpu, all passed (#38140)
[Qwen3] Qwen3 MoE add tp plan for expert mlps (#38135)
Fix incorrect attention mask truncate in WhisperFlashAttention2 (#36477)
enable d_fine finetuning properly (#37962)
Add `manueldeprada` to `run_slow` whitelist (#38126)
[docs] add uv installation instructions for source builds (#37968)
Update trainer.md (#38113)
Add config validation and style tweaks (#37589)
Fix auto batch size finder test (#38125)
Fix temporal padding in Qwen2VLImageProcessor when the number of frames is not divisible by temporal_patch_size (#38076)
[video processor] fix tests (#38104)
enable finegrained_fp8 and granite_speech cases on XPU (#38036)
Fix description and formatting errors in code docs (#38074)
Add style bot (#38102)
[CSM] update test for t4 runners (#38110)
Add Fast Image Processor for vilt (#37304)
Fix InternVL interpolate_pos_encoding and add to video_processing_auto (#38092)
fix `check_bad commit.py` gives wrong results (#38107)
[bug] fix llava processor to calculate unpadding size correctly (#37988)
Fix `past_key_values` type hint in model output types (#37953)
Fix bug in prefill_chunk_size that ignores disable_compile flag (#38067)
[smolvlm] skip the test (#38099)
Disable report callbacks for certain training tests (#38088)
fix: Propagate `lr_scheduler_kwargs` options to create LR Scheduler when LayerWiseDummyOptimizer is used (#34559)
add timeout for downloading the `librispeech_asr` dataset (#38073)
update `require_read_token` (#38093)
Refactor image processor phi4 (#36976)
uninstall `kernels` from docker images (#38083)
update seed_worker to set seed based on worker_id and rank (#37980)
Fix tot update in trainer (#37923)
fix the inconsist docstring in apply_chat_template (#38069)
chore(qwen2): display warning log only when sliding window attention ‚Ä¶ (#36316)
Fix mt5 test on AMD devices (#38081)
docs: fix md style (#38057)
Add AMD expectation to test_gpt2_sample (#38079)
Fix OneFormer integration test (#38016)
[`chat`] generate parameterization powered by `GenerationConfig` and UX-related changes (#38047)
[VLM] fix loading issues (#38051)
üî¥ Video processors as a separate class (#35206)
fix(conversion): Fix size mismatch error during TF->PT model loading (#38014)
enable generation fsdp/utils cases on XPU (#38009)
Fix linalg.norm for CovnNextV2 (#38015)
Fix cache update! (#38046)
Fix reduce-labels in BEIT Fast Image Processor (#38042)
Re-Enable `Trigger CircleCI via GitHub Actions when "ready for review" (#37885)` (#38041)
Support for version spec in requires & arbitrary mismatching depths across folders (#37854)
Do not erase a cache_position passed explicitly to generate(), if there is one (#37986)
Disable `Trigger CircleCI via GitHub Actions when `ready for review` (#38038)
Trigger CircleCI via GitHub Actions when `ready for review` (#37885)
[Temporary] Log some information in some pytest/pluggy internal places (#37996)
enable utils test cases on XPU (#38005)
make mistral3 pass on xpu (#37882)
fix document masking for chunked attention (#37429)
[`AutoDocstring`] Based on inspect parsing of the signature (#33771)
update bnb tests (#38011)
enable mamba2 integration cases on xpu (#38006)
make `test_speculative_decoding_non_distil` device-agnostic (#38010)
[VLMs]  support attention backends (#37576)
Fix wording in `torchscript.md` (#38004)
Fix incorrect installation instructions (for issue #37476) (#37640)
Skip `test_push_to_hub_with_saves_each_epoch` for now (#38022)
[caches] Raise exception on offloaded static caches + multi device (#37974)
[CI] remove duplicated message on GH comment to run slow tests (#37970)
Print commit SHA on slack message for new model notification. (#38019)
Fix `Optional` typing (#38018)
Enable RUF013 to enforce optional typing (#37266)
Add ALL_ATTENTION_FUNCTIONS compatibility for Pixtral model (#37960)
Fix `pad` image transform for batched inputs (#37544)
Add Swin2SR ImageProcessorFast (#37169)
üî¥ [VLM] Add base model without head  (#37033)
[CSM] tiny fix on generation (#38001)
Add CSM model (#36719)
Add a check to import_utils.py to allow for use of faiss_gpu installation (#37997)
remove duplicate code (#37991)
[chat template] separate jinja logic from tokenizers  (#37602)
make aya vision 5 integration tests pass on xpu (#37990)
[offload] respect `max_memory` argument when factoring in unused reserved memory (#37982)
Fix Qwen models export with torch 2.7 (#37985)
[Fast Processor] BEiT (#37005)
Fix donut backtracking (#37788)
Enable granite speech 3.3 tests (#37560)
fix FSDP + torch.compile bug when saving pretrained model  (#37725)
enable xpu in test_trainer (#37774)
Fix typo (#37964)
[speech2text] fix init of sinusoidal embeddings  (#37931)
Fix typos (#37978)
Small typo lines 47 and 199 perf_infer_gpu_one.md (#37938)
fix docs serving typos. (#37936)
add job links to new model failure report (#37973)
[llava] one pixel is missing from padding when length is odd (#37819)
[tests] Smaller model in slow cache tests (#37922)
add xpu memory check  (#37969)
üö®üö®üö® Fix forward of Dinov2ForImageClassification for models with registers (#37836)
Add GraniteMoeHybrid support for 4.0 (#37658)
[Ready to Merge][HFQuantizer] Squelch pydantic warnings (#37726)
Fix incorrect type annotation in get_auxiliary_logits (#37955)
[generate] Fix `vocab_size` access for multimodal models (#37937)
Use T4 single GPU runner with more CPU RAM (#37961)
[core] reuse unused reserved cuda memory when loading models (#37920)
More fault tolerant notification service (#37924)
[D-FINE] Update names (#37957)
[docs] logits docstring (#37929)
Break weight tying when quantizing input embedding (#37905)
Aligning modling code for GPT2 to work with vLLM (fallback) (#36934)
Add usage example for DINOv2 (#37398)
üåê [i18n-KO] Translated `gpu_selection.md` to Korean (#36757)
Improve performance of `load_state_dict` (#37902)
[chat] clean code and add base help (#37892)
Fix typos in strings and comments (#37910)
üö® rm already deprecated pad_to_max_length arg (#37617)
fixed gemma3 collection path pointing to llama 2 collection. (#37899)
Support `AOPerModuleConfig` and `include_embedding` (#37802)
Enhance documentation to explain chat-based few-shot prompting (#37828)
Fix Qwen3 tp plan with FP8 (#37871)
[tests] reset logs in `torch.compile` test (#37894)
[tests] Test all cache implementations (#37873)
Support FlaxPreTrainedModel to load model checkpoint from local subfolder safetensors (#37732)
update comment in image_processing_base.py to reference image_process‚Ä¶ (#37864)
Fix: reassign in qwen3 moe model (#37848)
uniformize kwargs for VisionTextDualEncoder (#34563)
Fix qwen2-vl-docs. (#37879)
make sure lr is not a tensor (#37881)
fix error for _register_pytree_node in torch2.1.0 and  fix bf16 assertion in xpu and npu (#37839)
update Clean_up_tokenization_spaces typos. (#37865)
Transformers cli clean command (#37657)
Llama Guard updates (#37872)
enable internvl UTs on XPU (#37779)
Allow override inputs to export recipe (#37508)
Skip is_flaky tests in the CI (#37723)
Update modeling_llama4.py (#37841)
üåê [i18n-KO] Translated `electra.md` to Korean (#36763)
Add Intel Gaudi doc (#37855)
Processor chat template: pass custom kwargs (#37852)
docs: Details for ambigious channel dimension assignment (#37600)
Fix Bitnet tokenizer in pipeline (#37861)
Fix cache get item return type hints (#37847)
Fix check of unecessary packages (issue #37626) (#37825)
Revert change that breaks on Torch 2.1 (#37531)
[tests] reorganize cache tests and clean memory between tests (#37684)
[tests] fix flaky pattern in `test_generate_continue_from_past_key_values` (#37724)
Add D-FINE Model into Transformers (#36261)
[modular] Fix the prefix-based renaming if the old and new model share a common name suffix (#37829)
Fast image processor for VitMatte added and bug in slow version fixed (#37616)
Samhq model addition  (#35147)
[config] revert #37603 (#37821)
change XLA deprecated api (#37741)
Fix error of HPU TP (#37782)
Add Optional to remaining types (#37808)
FIX: Faulty PEFT tests (#37757)
Add Bitnet model (#37742)
[RT-DETR] Improve docs (#37814)
Fix: Correct tensor shape comment in Mamba modeling (#37801)
[doc] fix the code examples in qwen doc (#37803)
Fix typos in strings and comments (#37799)
Define warmup allocator for torchao quantization (#37764)
Fix the fsdp config cannot work issue. (#37549)
Gemma3 is Torch Exportable (#37728)
Fix error message in `hub.py` (#37796)
fix performance issue in convert_ids_to_tokens (#37773)
chore: update SigLIP2 model card (#37624)
[i18n-KO] Translated `keypoint_detection.md` to Korean (#36649)
fix mpt test of different outputs from cuda (#37691)
Force torch>=2.6 with torch.load to avoid vulnerability issue (#37785)
Fix tensor parallel with non-floating dtypes (#37790)
Fix typos in strings and comments (#37784)
Align gpt2 mask preparation to #37612 (#37787)
unpin pytest<8 (#37768)
[causal mask] fix preparation with multi-gpu (#37612)
üåê [i18n-KO] Translated `roberta.md` to Korean (#37069)
Update model card for Gemma (#37674)
Fix auto-round hfoption  (#37759)
Guard DeepSpeed imports (#37755)
[deps] pin max `torch` version  (#37760)
Fix typos in comments (#37694)
Fix load of rng state for resuming training from checkpoint (#37162)
Fix tied weight loading with TP and loading sub state_dicts (#37758)
Refine parameter type annotations (#37666)
Fix wrong input shapes in doc-string of models (#37729)
[generate] fix default autocompile case on gpu (#37756)
Fix qwen2_5 get_rope_index tensor device locations (#37597)
updated hidden_features for FlaxDinov2SwiGLUFFN in Dinov2  (#37747)
[generate] skip compilation on cpu offload (#37709)
`GPT2Model` StaticCache support (#35761)
[cache] fix `HybridCache` init when `device` is passed (#37718)
Expand quantized data type support for tensor parallelism  (#37719)
Update `MllamaForConditionalGenerationIntegrationTest` (#37750)
Skip all `AriaForConditionalGenerationIntegrationTest` on `T4` (#37746)
[performance_optim] define flash attention mask on NPU device directly (#37698)
Correctly raise errors when downloading tokenizer files (#37740)
Fix `embeds_to_talker` device in Qwen2.5-Omni (#37739)
fix: learning_rate logged as tensor causing save issue with deepspeed (#37704)
[VLMs] fix flash-attention tests (#37603)
Make sure torch_is_available before using torch.distributed (#37693)
[tests] fix `test_nemotron_8b_generation_sdpa` (#37665)
Fix torchao doc examples (#37697)
Fix inference bugs in Qwen2.5 Omni (#37701)
Fix Aria tests (#37444)
Add Fast Image Processor for MobileNetV1  (#37111)
Add Fast Image Processor for PoolFormer (#37182)
Add Fast PVT Processor (#37204)
enable 4 test_trainer cases on XPU (#37645)
Process inputs directly in apply_chat_template in image-text-to-text pipeline (#35616)
[tests, `qwen2_5_omni`] fix flaky tests (#37721)
Qwen 2.5 Omni: apply video defaults (#37660)
[internvl] fix chat template (#37656)
TransfoXL is deprecated, don't keep it in tested examples! (#37707)
[CI] add back `sacrebleu` (and document why) (#37700)
Add maintainers for ROCm/Intel XPU/Ascend NPU (#37678)
[cleanup] remove `/model_cards` üßπ üßπ  (#37685)
Pin torch == 2.6 on PR CI docker images for now (#37695)
enable cpu offloading for Bark on xpu (#37599)
fix: remove classmethod from `Qwen2_5OmniConfig.get_text_config` (#37690)
Updated model card for mbart and mbart50 (#37619)
üåê [i18n-KO] Translated `siglip.md` to Korean (#37145)
enable blip2 and emu3 cases on XPU (#37662)
Add counters for dataset classes (#37636)
[Docs] Move models to appropriate section (#37338)
typo update in the parameter name (#37655)
[docs] only build `en` docs in push CI (#37677)
[cleanup] remove old scripts in `/scripts` üßπ üßπ  (#37676)
enable 6 granite cases on xpu (#37569)
enable mllama cases on xpu (#37644)
Refactor bitsandbytes doc (#37668)
Fix no_split_modules for Llama4 pretrained models (#37673)
Fix autoround docs  (#37675)
Fixing quantization tests (#37650)
Add AutoRound quantization support (#37393)
Correct warm-up with fp8 (#37670)
Fix duplicated weights in fp8 quantization (#37667)
[qwen-omni] fix training (#37517)
Introduce GradientCheckpointingLayer (#37223)
Fixes #37219 : RecurrentGemma crashes for inputs longer than sliding window length (#37613)
Fix ValueError when eval_do_concat_batches=False with examples (#37621)
[tests] Stricter generate + compilation test -- no recompilations allowed (#37629)
[test] update `test_past_key_values_format` (#37614)
Add test to ensure unknown exceptions reraising in utils/hub.py::cached_files() (#37651)
Support loading Gemma3 QAT GGUF models (#37649)
Restructure torchao quantization examples (#37592)
[fix gemma] Set default value for output_attentions parameter in Gemma2 and Gemma‚Ä¶ (#37633)
[fix] make legacy bnb code work (#37331)
Fix Qwen2.5-Omni get_chunked_index chunking functionality (#37631)
Refactor phi doc (#37583)
Update longformer.md (#37622)
fix link in kv_cache.md (#37652)
Allow Exclusion of Input IDs from RepetitionPenaltyLogitsProcessor (#37625)
Remove torchvision requirement from AutoImageProcessor (#37457)
[kernels] use original forward at compile time (#37604)
Fix InternVL attention when using qk_norm (38B and 78B) (#37620)
chore: update model card for SigLIP (#37585)
Fixing the example in generation strategy doc (#37598)
Deprecate modeling_utils.py classes (#37298)
Add InternVL (2.5 MPO) (#35968)
fix issue that some example with no trainer use accelerator.end_train‚Ä¶ (#37435)
fix 2 encoder_decoder issues on XPU (#37572)
[VLMs] use only `xxx_token_id` for multimodal tokens (#37573)
Model debugger upgrades (#37391)
[Gemma3] compile ‚ú®  (#37447)
enable 6 modeling cases on XPU (#37571)
enable 6 gemma2 cases on XPU (#37564)
Flag SpeechT5 flaky test (#37587)
[Bugfix] Fix flash-attention func param mismatch and softmax_scale default value mistake on Ascend NPU (#37575)
remove _run_third_party_device_tests (#37445)
Fix some GPU OOM after #37553 (#37591)
Gaudi: Add the bf16 support for hpu (#37568)
Fix Quark quantization config (#37578)
Update Phi4 converter (#37594)
Ensure positive warm-up size (#37581)
docs: fix typo (#37567)
[phi4] update conversion (#37579)
Small fix on context manager detection (#37562)
Fix qwen2audio wanr -> warn (#37559)
[TimesFM] use the main revison instead of revision for integration test (#37558)
[qwen-vl] Standardize config (#37268)
[chat template] fix security vulnerability (#37523)
Add Janus model (#36053)
All models can be initialized on meta device (#37563)
Bridgetower fast image processor (#37373)
Fix Mamba2 Grouped SSD Support in the torch_forward Path (#37533)
Add EfficientNet Image PreProcessor (#37055)
[vlm] adjust max length for special tokens (#37342)
Fix pixel attention mask padding in smolvlm (#37497)
Run `test_can_load_with_global_device_set` using a subprocess (#37553)
:red_circle: Update CLIP vision attention to new attention interface (#37498)
Fix TimesFm doc issue (#37552)
Make Ignored Columns ValueError More Informative (#33299)
Fix device issue for tapas (with `as_tensor`) (#37551)
docs(typo): Update ISSUES.md, fix a small typo (#37542)
add FlashAttentionKwargs and seq_idx to flat collator (#36456)
Update quantization docs (#37439)
Add TimesFM Time Series Forecasting Model (#34082)
Refactor torchao docs  (#37490)
Keep Quark loading through meta device (#37538)
convert scale and zero to cuda when using HQQ backend (#37425)
Fixes hqq by following a new path for bias parameter in pre_quantized models (#37530)
More appropriate cuda warmup in resource-constrained hardware (#37550)
Add Fast Grounding-Dino Processor (#37108)
enable 6 rt_detr_v2 cases on xpu (#37548)
enable 3 mpt test cases on XPU (#37546)
Fix BitsAndBytesConfig JSON serialization in TrainingArguments (#37520)
enable `test_offloaded_cache_implementation` on XPU (#37514)
enable several cases on XPU (#37516)
enable 5 cases on XPU (#37507)
Refactor ColPali model documentation (#37309)
Update VITS model card (#37335)
Fix broken add-fast-image-processor CLI (#37499)
Add Fast Conditional-DETR Processor (#37071)
Add Fast Chinese-CLIP Processor (#37012)
VDR task guide (#37485)
fix and enhance pipeline_webserver.md (#36992)
Fix missing return type for MLCD docs (#37527)
fix: Restore explicit error surfacing for unexpected hub exceptions (#37525)
Add Fast Yolos Processor (#37292)
Llama4: remove redundant transpose of router_logits (#37468)
Add MLCD model (#36182)
Change default value of `attn_temperature_tuning` (#37501)
Detect and use device context manager or global device in `from_pretrained` (#37216)
Don't auto-assign reviewers when the author is in HF (#37500)
Remove deprecation warning for `num_logits_to_keep` (#37149)
Add Fast owlvit Processor (#37164)
[qwen-omni] fix processor (#37493)
Fixing gated repo issues (#37463)
Fix wrong argparse type in modular checker script (#37472)
Add Fast Mobilenet-V2 Processor (#37113)
Add ImageProcessorFast to BiT processor (#37180)
Add Fast LeViT Processor (#37154)
Fix mask handling for flex attention in llama/gemma2/mistral/qwen2 (#37381)
[bug] deprecated deta load_cuda_kernel, MultiScaleDeformableAttention (#37443)
Add Fast Image Processor for Donut (#37081)
Detect and fix most `_init_weights()` issues - make it work for composite models (#37070)
Add Fast Image Processor for LayoutLMv3 (#37201)
Fixed broken links (#37466)
Add Fast Image Processor for LayoutLMv2 (#37203)
Add Fast Image Processor for Flava (#37135)
[ci] fix doc builder (#37489)
Add Fast Image Processor for Perceiver (#37176)
Add Qwen2.5-Omni (#36752)
Fix tests failed with gated repos. (#37484)
Remove `fsspec` dependency which isn't directly used by transformers (#37318)
make test_snowman_image_captioning pass on XPU, by sharing same atol w/ ROCM (#37480)
fix: (llama4) fix no_split_modules to be picked up for fsdpv1 and v2 sharding (#37462)
Fix typing issues with SigLip2 (#37356)
[agents] remove agents üßπ  (#37368)
Delete hubconf.py (#37455)
Add Granite Speech Support (#36801)
nit: typing use Llama4TextConfig instead of Llama4Config (#37430)
Add XPU case to is_torch_bf16_gpu_available (#37132)
Add weights_only=True to torch.load (#37062)
:rotating_light: :rotating_light: Allow saving and loading multiple "raw" chat template files (#36588)
Disable kernels for quantization (#37446)
prevent creating a view/leaf param for low rank optimizers w FSDP (#37379)
[Regression] Fix Quark quantized model loading after refactorization (#37407)
[processor] clean up mulitmodal tests (#37362)
Remove triton mlp kernel, not compiling for some models (#37449)
Fix the test fetcher (#37452)
Add moe kernels (#37376)
Update-kernel-pin (#37448)
Simplify soft dependencies and update the dummy-creation process (#36827)
Fixes: Corrects file path for CUDA kernels (#37438)
enhance require_deterministic_for_xpu (#37437)
Remove old code for  PyTorch,  Accelerator and tokenizers (#37234)
[Feat] Support npu in modeling models (#37369)
Adding to self_comment_ci.yml (#37426)
(Part 2) feat: allow for tp_size attr for tplizing the model (#37054)
fix: use mtime by default in Trainer._rotate_checkpoints with automatic fallback (#37260)
Add GGUF support to Gemma3 Text backbone (#37424)
Llama Kernel integration (#37092)
Fix require_read_token (#37422)
Correctly drop tokens in SwitchTransformer (#37123)
Add image classifier donut & update loss calculation for all swins  (#37224)
Quark Quantization gated repo (#37412)
Fix new failure reports not including anything other than `tests/models/` (#37415)
[chat-template] Unify tests and clean up üßº  (#37275)
use `rms_norm_eps` for the L2Norm for Llama4 (#37418)
Allow rocm systems to run these tests (#37278)
from_pretrained should handle xpu case (#37382)
Send trainer/fsdp/deepspeed CI job reports to a single channel (#37411)
update `kernels` to 0.4.3 (#37419)
mark llama4 as not supported with fa2 (#37416)
Offloaded hybrid cache for Llama4 (#37401)
Fix Llama4 offset (#37414)
Restrict & Explain tp_plan for FBgemm (#37404)
Handle torch ver in flexattn (#37400)
Add warning when failed to acquire other user's lock at model download (#37395)
handle torch version edge cases (#37399)
the fix that did not get in (#37370)
Attention Quantization with FBGemm & TP (#37384)
Fix some failing AWQ tests (#37383)
Apply torchfix to replace deprecated functions: `_pytree._register_pytree_node` and `torch.cpu.amp.autocast` (#37372)
Fix warning message for PEFT models in text-generation pipeline #36783 (#36887)
Add "selecting a quantization method" doc (#37159)
update deepspeed docker (#37371)
Add glm4 (#37388)
fix: llama4 conversion script no_rope_layers (#37359)
Update composition flag usage (#36263)
Preserve requires_grad in pre quantized model (#37354)
:rotating_light: :rotating_light: Setup -> setupclass conversion (#37282)
fix(qwen): fix shape error when using tp (#36947)
prune LM Head for USD (#36695)
[core] remove `GenerationMixin` inheritance by default in `PreTrainedModel` (#37173)
Skip non-selected experts for mixtral and qwen2_moe (#32429)
[llama 4] dynamic rope decorator (#37365)
Set vision config to None for Gemma 1B conversion (#37366)
fix deepspeed job (#37284)
A bit of cleaning üßπüßπ (#37215)
Use Python 3.9 syntax in tests (#37343)
convert float for yarn related arguments in rope_scaling (#37139)
Expose blip2qformer (#37254)
Multiple llama4 fixe (#37353)
Fixing flex attention for torch=2.6.0 (#37285)
more fixes for post-training llama4 (#37329)
Remove unnecessary attr assignment (#36837)
Updated Model-card for donut (#37290)
Add bnb to the list of supported quantization methods for LLama4 (#37348)
Update Model Card for Jamba (#37152)
Improvements in Gemma2 model card (#37076)
Clean up the compressed-tensors integration (#37349)
Update Model card for GPT2 (#37101)
Update falcon mamba card (#37253)
Update model-card for DINOv2 (#37104)
updated model card for Mistral (#37156)
Remove HQQ from caching allocator warmup (#37347)
Update translation template (#37294)
fix derived berts `_init_weights` (#37341)
Avoid build crashes when torch.version.xpu doesn't exist and fix Llama4 processor tests (#37346)
enable 2 llama UT cases on xpu (#37126)
byebye torch 2.0 (#37277)
Fix torchao usage (#37034)
Use Python 3.9 syntax in examples (#37279)
Fix `init empty weights` without accelerate (#37337)
Fix deepspeed with quantization (#37324)
fix llama4 training (#37319)
fix flex attn when optional args aren't passed (#37327)
v4.52.0.dev0
Add llama4 (#37307)
Hf Xet extra (#37305)
Fix deepspeed loading (part 2) (#37306)
Fix deepspeed loading (#37281)
Update OpenAI GPT model card (#37255)
Updated T5 model card with standardized format (#37261)
Updated model card for distilbert (#37157)
mobilebert model card update (#37256)
Fix: Unexpected Keys, Improve `run_compressed`, Rename Test Folder (#37077)
Update model card for Depth Anything (#37065)
Disable delay_optimizer_creation in `Trainer` to support fsdp2 (#37147)
fix test device spec relative path importing issue (#37190)
Fix llava_onevision tests (#37280)
[RoPE] abstract dynamic RoPE update under a decorator ‚ú®  (#37249)
Hugging Face Hub pin to v0.30.0 for Xet (#37166)
[Tests] flaky `test_constrained_beam_search_generate_dict_output`  (#37276)
Clarify error message to ensure min 28x28 image supplied for Qwen 2.5 VL (#37264)
pin specific `natten` version in docker file  (#37274)
Fix deprecated PT functions (#37237)
Fix `utils/check_bad_commit.py` (#37272)
Introduce modular files for speech models (#35902)
update error msg (#37207)
[qwen-vl] fix image processor (#37258)
Update model card for electra (#37063)
Update Model Card for ModernBERT (#37052)
chore: Update model doc for code_llama (#37115)
Update model card for Cohere (#37056)
Purge unused ModelTester code (#37085)
feat: updated model card for qwen_2.5_vl (#37099)
Add Optional to types (#37163)
Adding links to ShieldGemma 2 technical report (#37247)
[CI] green llama tests (#37244)
Allow flexible generation params arg when checking pipeline specs (#37211)
Add support for fast image processing in image-pretraining example (#37021)
Fix AST parsing when looking for remote code imports (#37245)
enable 2 types of case on XPU (#37198)
[CI] lazy loading external datasets (#37218)
[tests] fix mamba integration simple inference precision issue (#37193)
Fix test (#37213)
Add new dim to `num_items_in_batch` if necessary (#36967)
[Phi4] add multimodal chat template (#36996)
Fix static cache export (#37229)
Updated model card for Qwen2  (#37192)
Update falcon model card (#37184)
Updated the model card for CLIP (#37040)
More ReDOS fixes! (#36964)
Stop DOSing the Hub in the CI (#37209)
[Tests] add `min_new_tokens` to prevent flaky length checks (#37175)
No more dtype_byte_size() (#37144)
Add py.typed (#37022)
[3/N] Use pyupgrade --py39-plus to improve code (#36936)
Merge tensor operations with device transfer operations (#37097)
Fix some code annotation typos. (#37102)
fix: Add 'image-text-to-text' to `TASK_MAPPING` (#37107)
Try to avoid/reduce some remaining CI job failures (#37202)
Fixes DynamicCache export issues due to control flow and inplace modifications (#36652)
Add device workaround for int4 weight only quantization after API update (#36980)
Skip code `307` in `RequestCounter` (#36953)
[chat-template] fix video loading (#37146)
[doc] Fix link for Quark quantization page (#37179)
Revert #37031 (#37178)
Fix meta state dict loading with quantizers (#37136)
Avoid pipeline test failing related to Hub call (#37170)
Fixes the inconsistency of the optionality of attention_mask (#37153)
Refactor attention for SigLIP based models (#36981)
fix XPU UT error case brough by RNG difference btw XPU and CUDA (#37121)
[`ModernBERT`] Never save 'reference_compile' config; should be set based on end user (#36305)
Make canine model exportable by removing unncessary complicated logic (#37124)
Only count num items in batch when needed (#36867)
Convert `_VALID_DICT_FIELDS` to class attribute for shared dict parsing in subclasses (#36736)
Use public export API on torch 2.5 and future (#36781)
enable `test_assisted_decoding_in_different_gpu` test on XPU (#37120)
Fix llava xpu tests. (#37130)
add gpt2 test on XPU (#37028)
Fix std initialization in Idefics variants (#37100)
Fix more inefficient PT operations (#37060)
Refactor `return_dict` logic to remove complicated if/else paths (#36794)
Remove low_cpu_mem_usage and _fast_init (#36963)
[qwen3] fix generation tests (#37142)
[Feature] Support using FlashAttention2 on Ascend NPU (#36696)
skip (#37141)
Export T5 (encoder-decoder) to ExecuTorch (#36486)
[tests] remove cuda-only test marker in `AwqConfigTest`  (#37032)
Create and Expose SamVisionModel as public for better accessibility (#36493)
Remove deprecated code (#37059)
RWKV: fix mask warning typo (#37114)
Fix Gemma3 embedding scaling (#37109)
[MLU] Fix FA2 check error, remove deepspeed-mlu deps. (#36159)
fix whisper re-compile (#36712)
enable tp on CPU (#36299)
Fix 4090/ada not detected as having FP8 support (#37067)
Support passing flash_attn_kwargs when gradient_checkpointing is enabled (#37037)
Gaudi: Fix the pipeline failed issue with hpu device (#36990)
Adding Qwen3 and Qwen3MoE (#36878)
üåê [i18n-KO] Translated `qwen2_vl.md` to Korean (#36750)
Kenlm (#37091)
[Cache] rename dtype attribute üö® üö®  (#37044)
[generate] beam search -- fix output cropping (#37080)
fixed typo. (#37057)
Fix AttentionInterface following feedback (#37010)
Fix state_dict map location when quantized (#37086)
Update w/ new account (#37084)
fix tied weigths issue  (#37031)
[WIP] add deepseek-v3 (#35926)
[blip-2] Fix dtype mismatch when keep in fp32  (#37068)
Change deprecated PT functions (#37041)
Fix some typos about benchmark scripts. (#37027)
Use `lru_cache` for tokenization tests (#36818)
fix: AttributeError: 'LlavaProcessor' object has no attribute 'image_token_id' (#37026)
Fix SDPA implementation in Qwen2-VL (issues with torch==2.6.0) (#36891)
fix: Fully remove legacy cache from Llama (#36958)
fixed typo (#37036)
Remove deprecated batch_size parameter (#37007)
Replace default split function with jnp.split() in flax models (#37001)
Set weights_only in torch.load (#36991)
Fix typing for None valued variables (#37004)
Avoid unnecessary device operations in loss computing (#36950)
clean pipeline question_answering. (#36986)
[generate, cache] handle more complex device maps (#37014)
[audio utils] fix fft_bin_width computation (#36603)
[chat templates} support loading audio from video (#36955)
Fixup for distill_any_depth conversion script (#37043)
Optimize `to_py_obj` for python-native numeric lists and scalars (#36885)
fix pegasus init weights and other copied models (#36844)
Add Distill Any Depth (#36614)
Skip FP8 linear tests For device capability < 9.0(#37008)
remove redundant code in trainer (#36994)
Mark 2 tests as flaky for now (#37038)
[Modeling] Load FP8 safetensors such as DeepSeek (#36828)
Fix PixtralProcessor patch_size when spatial_merge_size is used (#37019)
Support QuestionAnswering Module for ModernBert based models. (#35566)
fix transformers_cli import relative path issue (#36989)
[docs] Attention mask image (#36970)
Remove deprecated training arguments (#36946)
fix typos in the code comments and error messages (#36993)
Log the correct learning rate (#36973)
Fix device_map check for ggml files (#37003)
Fix removing "cpu" from frozenset in bitsandbytes.py to allow better ROCm support. (#36975)
Allow easy registration of custom attention functions (#36889)
Fix get_device_properties (#36997)
Fix Optional type annotation (#36841)
Install `networkx==3.2.1` manually in some CircleCI jobs after #36957 (#37000)
Use torch.expm1 (#36995)
byebye CircleCI TF jobs (#36998)
Fix tensor dtype mismatch (#36985)
üö®Deprecate legacy argument for image-text-to-text models and adopt new behavior by default (#36307)
update bot comment again (#36974)
Add ruff target-version (#36971)
[docs] Fix image link (#36869)
Remove extra tensor clone in PyTorch code (#36748)
update examples after ruff being updated (#36972)
Updated docker files to use `uv` for installing packages (#36957)
typo fixed in README_fr.md (#36951)
Change GPUS to GPUs (#36945)
Update after #36962 (#36965)
Update ruff to `0.11.2` (#36962)
[Utils] torch version checks optionally accept dev versions (#36847)
Fix cuda index issue in cache allocator (#36937)
Support `return_tensors` in audio chat templates (#34601)
fix typos in the tests directory (#36932)
Export for Phi4-mini (#36780)
Fixing _pre_quantization_dtype when torch_dtype is None (#36930)
Add Phi4 multimodal (#36939)
Deprecate #36741 and map Causal to Conditional (#36917)
Disallow Offload to disk for gguf files (#36933)
Fix processor kwargs qwen2 vl (#36890)
Added support for seed in `DataCollatorForWholeWordMask` (#36903)
More precise comment (#36935)
Fix pytorch defomr attn path (#36923)
[2/N] Use pyupgrade --py39-plus to improve code (#36857)
Update `trainer_pt_utils.py` docstrings for consistency (#36912)
Fix typos (#36910)
Use another repo. for Mistral3 processor testing (#36925)
Fix Compressed tensors to_dict_diff (#36922)
[chameleon] fix num image token check (#36918)
tests: fix asyncio.wait() usage for python>=3.11 (#36898)
[Fix] Add `original_max_position_embeddings` to YARN rope_scaling optional keys (#36877)
Fix torch version guard at import (#36907)
fix Gemma3 Config (#36893)
Update installation.md (#36826)
[docs] Model docs (#36469)
Fix Pan and Scan on batched images Gemma3 (#36864)
Simplify keep_in_fp32_modules logic (#36722)
fix: loss computation after embeddings resize - mllama (#36840)
push v4.51.0.dev0
Fix: dtype cannot be str (#36262)
Minor Gemma 3 fixes  (#36884)
Use `deformable_detr` kernel from the Hub (#36853)
Gemma 3 tests expect greedy decoding (#36882)
:red_circle: :red_circle: :red_circle: supersede paligemma forward to shift pos id indexing (#36859)
add eustlb as an actor
[generate] model defaults being inherited only happens for newer models (#36881)
Revert "Update deprecated Jax calls (#35919)" (#36880)
Make ViTPooler configurable (#36517)
chore: fix typos in the tests directory (#36813)
Remove call to `.item` in `get_batch_samples` (#36861)
FIX FSDP plugin update for QLoRA (#36720)
[CI] doc builder without custom image (#36862)
Mllama: raise better error (#35934)
Refactor Aya Vision with modular (#36688)
Add support for seed in `DataCollatorForLanguageModeling` (#36497)
[CI] fix update metadata job (#36850)
Gemma3: fix test (#36820)
[torchao] revert to get_apply_tensor_subclass (#36849)
Add model visual debugger (#36798)
Add Prompt Depth Anything Model (#35401)
Refactor Attention implementation for ViT-based models (#36545)
DeepSpeed tensor parallel+ZeRO (#36825)
Support loading Quark quantized models in Transformers (#36372)
Use pyupgrade --py39-plus to improve code (#36843)
Fix hqq skipped modules and dynamic quant (#36821)
Fix ONNX export for sequence classification head  (#36332)
Shieldgemma2 (#36678)
Fix: remove the redundant snippet of _whole_word_mask (#36759)
Gemma 3: Adding explicit GenerationConfig and refactoring conversion ‚Ä¶ (#36833)
Fix import for torch 2.0, 2.1 - guard typehint for "device_mesh"  (#36768)
Update min safetensors bis (#36823)
[generate] clarify docstrings: when to inherit `GenerationMixin` (#36605)
[modular] Sort modular skips (#36304)
Pass state dict (#35234)
[qwen2 audio] remove redundant code and update docs (#36282)
Update deprecated Jax calls (#35919)
Fix fp16 ONNX export for RT-DETR and RT-DETRv2 (#36460)
Pass num_items_in_batch directly to loss computation (#36753)
Saving `Trainer.collator.tokenizer` in when `Trainer.processing_class` is `None` (#36552)
fix tiktoken convert to pass AddedToken to Tokenizer (#36566)
[ForCausalLMLoss] allow users to pass shifted labels (#36607)
Disable inductor config setter by default (#36608)
Fix swanlab global step (#36728)
Move the warning to the documentation for DataCollatorWithFlattening (#36707)
Just import torch AdamW instead (#36177)
Update configuration_qwen2.py (#36735)
quick fix fast_image_processor register error (#36716)
Add Space to Bitsandbytes doc (#36834)
Support tracable dynamicKVcache (#36311)
One more fix for reviewer assignment (#36829)
[gemma 3] multimodal checkpoints + AutoModelForCausalLM (#36741)
enable OffloadedCache on XPU from PyTorch 2.7 (#36654)
Add option for ao base configs (#36526)
Add attention visualization tool  (#36630)
[Generation] remove leftover code from end-to-end compilation (#36685)
Fix Device map for bitsandbytes tests (#36800)
Remove `dist": "loadfile"` for `pytest` in CircleCI jobs (#36811)
fix "Cannot copy out of meta tensor; no data!" issue for BartForConditionalGeneration model (#36572)
Expectations test utils (#36569)
[generate] ‚ú® vectorized beam search ‚ú® (#35802)
Support custom dosctrings in modular (#36726)
Fix chameleon's TypeError because inputs_embeds may None (#36673)
Fix casting dtype for qunatization (#36799)
Fix Mistral3 tests (#36797)
Loading optimizations (#36742)
Update SHA for `tj-actions/changed-files` (#36795)
fix hqq due to recent modeling changes (#36771)
Add Mistral3 (#36790)
Fix gemma3_text tokenizer in mapping (#36793)
Fixing typo in gemma3 image_processor_fast and adding a small test (#36776)
chore: fix typos in tests directory (#36785)
fix typos in the tests directory (#36717)
doc: Clarify `is_decoder` usage in PretrainedConfig documentation (#36724)
[docs] Update README (#36265)
[CI] remove redundant checks in `test_eager_matches_sdpa_inference` (#36740)
[MINOR:TYPO] Update hubert.md (#36733)
Fix `TrainingArguments.torch_empty_cache_steps` post_init check (#36734)
Fix test isolation for clear_import_cache utility (#36345)
fix xpu tests (#36656)
Allow ray datasets to be used with trainer (#36699)
fix can_generate (#36570)
enable/disable compile for quants methods (#36519)
üö®üö®üö® Fix sdpa in SAM and refactor relative position embeddings (#36422)
[Generation, Gemma 3] When passing a custom `generation_config`, overwrite default values with the model's base `generation_config` (#36684)
Update self-push-caller.yml
Fix grad accum arbitrary value (#36691)
Fix post_init() code duplication (#36727)
üåê [i18n-KO] Translated codegen.md to Korean (#36698)
[tests] Parameterized `test_eager_matches_sdpa_inference` (#36650)
Try working around the processor registration bugs (#36184)
Fix/best model checkpoint fix (#35885)
[model loading] don't `gc.collect()` if only 1 shard is used (#36721)
Cleanup the regex used for doc preprocessing (#36648)
Make the flaky list a little more general (#36704)
Gemma3 processor typo (#36710)
Add support for fast image processors in add-new-model-like CLI (#36313)
Final CI cleanup (#36703)
Add GGUF support to T5-Encoder (#36700)
Handling an exception related to HQQ quantization in modeling (#36702)
fix: fsdp sharded state dict wont work for save_only_model knob (#36627)
Add loading speed test (#36671)
[CI] Automatic rerun of certain test failures (#36694)
chore: fix typos in utils module (#36668)
Fix dtype for params without tp_plan (#36681)
fix type annotation for ALL_ATTENTION_FUNCTIONS (#36690)
Change Qwen2_VL image processors to have init and call accept the same kwargs (#36207)
Upgrading torch version and cuda version in quantization docker (#36264)
fix wandb hp search unable to resume from sweep_id (#35883)
Changing the test model in Quanto kv cache (#36670)
Fix slicing for 0-dim param (#36580)
Update config.torch_dtype correctly (#36679)
[Cache] Don't initialize the cache on `meta` device (#36543)
Fix rescale normalize inconsistencies in fast image processors (#36388)
Refactor siglip2 fast image processor (#36406)
Remove differences between init and preprocess kwargs for fast image processors (#36186)
[quants] refactor logic for modules_to_not_convert (#36672)
Remove hardcoded slow image processor class in processors supporting fast ones (#36266)
Fix Failing GPTQ tests (#36666)
Don't accidentally mutate the base_model_tp_plan (#36677)
[core] Large/full refactor of `from_pretrained` (#36033)
Fix bnb regression due to empty state dict (#36663)
[CI] gemma 3 `make fix-copies` (#36664)
fix block mask typing (#36661)
HPU support (#36424)
Gemma3 (#36658)
fix typos in the docs directory (#36639)
Fix gguf docs (#36601)
Remove research projects (#36645)
[docs] Update docs dependency (#36635)
Stop warnings from unnecessary torch.tensor() overuse (#36538)
Remove remote code warning (#36285)
Fix AriaForConditionalGeneration flex attn test (#36604)
Proper_flex (#36643)
Fix bugs in mllama image processing (#36156)
Refactor some core stuff (#36539)
[docs] Serving LLMs (#36522)
chore: fix typos in language models (#36586)
Fix auto-assign reviewers (#36631)
[`HybridCache`] disable automatic compilation (#36620)
Fix check for XPU. PyTorch >= 2.6 no longer needs ipex. (#36593)
Fixed datatype related issues in `DataCollatorForLanguageModeling` (#36457)
Bump jinja2 from 3.1.5 to 3.1.6 in /examples/research_projects/decision_transformer (#36582)
Update "who to tag" / "who can review" (#36394)
Update chat_extras.md  with content correction (#36599)
Github action for auto-assigning reviewers (#35846)
Export base streamer. (#36500)
avoid errors when the size of `input_ids` passed to `PrefixConstrainedLogitsProcessor` is zero (#36489)
Mention UltraScale Playbook üåå in docs (#36589)
fix: argument (#36558)
[XGLM] tag tests as slow (#36592)
[bark] fix loading of generation config (#36587)
Integrate SwanLab for offline/online experiment tracking and local visualization (#36433)
Modular Conversion --fix_and_overwrite on Windows (#36583)
Delete redundancy if case in model_utils (#36559)
Bump transformers from 4.38.0 to 4.48.0 in /examples/research_projects/pplm (#36540)
chore: enhance message descriptions in parameters,comments,logs and docstrings (#36554)
Fix typos . (#36551)
Fix typos in tests (#36547)
guard torch version for uint16 (#36520)
chore: enhance messages in docstrings (#36525)
Fix links in quantization doc (#36528)
Fix bamba tests amd (#36535)
chore: Fix typos in docs and examples (#36524)
Add aya (#36521)
[docs] Redesign (#31757)
Remove unused code (#36459)
[Style] fix E721 warnings (#36474)
Fix edge case for continue_final_message (#36404)
Fix pipeline+peft interaction (#36480)
chore: fix message descriptions in arguments and comments (#36504)
Fix some typos in docs (#36502)
fix torch_dtype, contiguous, and load_state_dict regression (#36512)
Fix kwargs UserWarning in SamImageProcessor (#36479)
Check `TRUST_REMOTE_CODE` for `RealmRetriever` for security (#36511)
Fix loading zero3 weights (#36455)
Fix _load_state_dict_into_meta_model with device_map=None (#36488)
Fix couples of issues from #36335 (#36453)
Add Got-OCR 2 Fast image processor and refactor slow one (#36185)
[docs] fix bug in deepspeed config (#36081)
Fix loading models with mismatched sizes (#36463)
[GroundingDino] Fix grounding dino loss üö® (#31828)
Fix `hub_retry` (#36449)
Lazy import libraries in `src/transformers/image_utils.py` (#36435)
[generate] `torch.distributed`-compatible `DynamicCache` (#36373)
[save_pretrained ] Skip collecting duplicated weight (#36409)
Add `contents: write` (#36445)
Fix another permission (#36444)
Fix permission (#36443)
Change PR to draft when it is (re)opened (#36417)
restrict cache allocator to non quantized model (#36428)
Fix Expected output for compressed-tensors tests (#36425)
Update form pretrained to make TP a first class citizen (#36335)
Fix compressed tensors config (#36421)
Universal Speculative Decoding `CandidateGenerator` (#35029)
fix: prevent model access error during Optuna hyperparameter tuning (#36395)
add recommendations for NPU using flash_attn (#36383)
Fixing the docs corresponding to the breaking change in torch 2.6. (#36420)
Deprecate transformers.agents (#36415)
Add retry hf hub decorator (#35213)
Fixed VitDet for non-squre Images (#35969)
Security fix for `benchmark.yml` (#36402)
Fix convert_to_rgb for SAM ImageProcessor (#36369)
[CLI] add import guards (#36376)
Fix pytorch integration tests for SAM (#36397)
chore: fix function argument descriptions (#36392)
fix audio classification pipeline fp16 test on cuda (#36359)
[tests] enable autoawq tests on XPU  (#36327)
tests: revert change of torch_require_multi_gpu to be device agnostic (#35721)
addressing the issue #34611 to make FlaxDinov2 compatible with any batch size (#35138)
Added handling for length <2 of suppress_tokens for whisper (#36336)
Fix doc formatting in forward passes & modular (#36243)
Update _get_eval_sampler to reflect Trainer.tokenizer is deprecation  self.tokenizer -> self.processing_class (#36315)
enable torchao quantization on CPU (#36146)
Fix `is_causal` fail with compile (#36374)
[modular] Do not track imports in functions (#36279)
Load models much faster on accelerator devices!! (#36380)
Update modeling_llava_onevision.py (#36391)
notify new model merged to `main` (#36375)
[Modeling] Reduce runtime when loading missing keys (#36312)
fix(type): padding_side type should be Optional[str] (#36326)
Update amd pytorch index to match base image (#36347)
Add autoquant support for torchao quantizer (#35503)
Change slack channel for mi250 CI to amd-hf-ci (#36346)
Improve model loading for compressed tensor models (#36152)
[tests] enable bnb tests on xpu (#36233)
Fix exploitable regexes in Nougat and GPTSan/GPTJNeoXJapanese (#36121)
Uses Collection in transformers.image_transforms.normalize (#36301)
[tests] make quanto tests device-agnostic (#36328)
[CI] Check test if the `GenerationTesterMixin` inheritance is correct üêõ üî´  (#36180)
Add SigLIP 2 (#36323)
VLMs: even more clean-up (#36249)
Fix default attention mask of generate in MoshiForConditionalGeneration (#36171)
[smolvlm] make CI green (#36306)
fix: prevent second save in the end of training if last step was saved already (#36219)
Fix typo in Pixtral example (#36302)
SmolVLM2 (#36126)
Ignore conversion files in test fetcher (#36251)
Fix broken CI on release branch due to missing conversion files  (#36275)
Make cache traceable (#35873)
Fix callback handler reference (#36250)
docs: Update README_zh-hans.md (#36269)
Add Example for Custom quantization (#36286)
[tests] make `test_from_pretrained_low_cpu_mem_usage_equal` less flaky (#36255)
[tests] remove flax-pt equivalence and cross tests (#36283)
[tests] deflake dither test (#36284)
TP initialization module-by-module (#35996)
[tests] remove `pt_tf` equivalence tests (#36253)
Add dithering to the `Speech2TextFeatureExtractor` API. (#34638)
Add support for post-processing kwargs in image-text-to-text pipeline (#35374)
Uniformize LlavaNextVideoProcessor kwargs (#35613)
Qwen2VL fix cos,sin dtypes to float when used with deepspeed (#36188)
Added Support for Custom Quantization (#35915)
GitModelIntegrationTest - flatten the expected slice tensor (#36260)
Fix XGLM loss computation (PyTorch and TensorFlow) (#35878)
feat: add support for tensor parallel training workflow with accelerate (#34194)
Remove flakiness in VLMs  (#36242)
Fix TorchAoConfig not JSON serializable (#36206)
Au revoir flaky `test_fast_is_faster_than_slow` (#36240)
[tests] remove `test_export_to_onnx` (#36241)
Add compressed tensor in quant dockerfile (#36239)
Bump transformers from 4.38.0 to 4.48.0 in /examples/research_projects/codeparrot/examples (#36237)
[generate] Fix encoder decoder models attention mask (#36018)
[tests] remove tf/flax tests in `/generation` (#36235)
v4.45.0-dev0
Add missing atol to torch.testing.assert_close where rtol is specified (#36234)
[generate] remove cache v4.47 deprecations (#36212)
AMD DeepSpeed image additional HIP dependencies (#36195)
Fix `LlavaForConditionalGenerationModelTest::test_config` after #36077 (#36230)
[tests] fix `EsmModelIntegrationTest::test_inference_bitsandbytes`  (#36225)
set `test_torchscript = False` for Blip2 testing  (#35972)
Use `args.num_workers` in `check_modular_conversion.py` (#36200)
add shared experts for upcoming Granite 4.0 language models (#35894)
Add @require_bitsandbytes to Aria test_batched_generation (#36192)
[Bugfix] Fix reloading of pixtral/llava configs (#36077)
üî¥ VLM: compile compatibility (#35724)
Guard against unset resolved_archive_file (#35628)
Revert qwen2 breaking changes related to attention refactor (#36162)
Add require_read_token to fp8 tests (#36189)
New HIGGS quantization interfaces, JIT kernel compilation support. (#36148)
Prepare processors for VideoLLMs (#36149)
Add ImageProcessorFast to Qwen2.5-VL processor (#36164)
Chat template docs (#36163)
CI: fix `test-save-trainer` (#36191)
Add support for partial rotary embeddings in Phi3 model (#35947)
Uniformize OwlViT and Owlv2 processors (#35700)
Fix make_batched_videos and add tests (#36143)
Fix a mistake in #36175 (#36179)
Follow up to SpQR integration (#36176)
Fix the key name for _load_rng_state under torch.cuda (#36138)
Make `check_repository_consistency` run faster by MP (#36175)
Optimize Qwen2VL vision model by precomputing cos/sin embeds before ViT blocks (#35837)
Use tqdm auto (#35726)
CI: avoid human error, automatically infer generative models (#33212)
add disable compile option (#36161)
fix training issues (#36158)
Efficient Inference Kernel for SpQR  (#34976)
Bump transformers from 4.38.0 to 4.48.0 in /examples/research_projects/adversarial (#36168)
Bump transformers from 4.38.0 to 4.48.0 in /examples/tensorflow/language-modeling-tpu (#36167)
[generate] revert change in Aria: the maximum cache length must match `max_length` (#36120)
Fix : fix doc fp8 (#36173)
Fix red CI (#36174)
[Modular] skip modular checks based on diff (#36130)
Remove loading custom kernel for RT-DETRv2 (#36098)
Adding FP8 Quantization to transformers (#36026)
Helium documentation fixes (#36170)
Move `DataCollatorForMultipleChoice` from the docs to the package (#34763)
Fix PretrainedTokenizerFast check => Fix PretrainedTokenizerFast Save (#35835)
docs: fix return type annotation of `get_default_model_revision` (#35982)
qwen2.5vl: fix bugs when using flash2+bf16 or num_return_sequences>1 (#36083)
Fix tests for vision models (#35654)
Replace deprecated update_repo_visibility (#35970)
Fix Gemma2 dtype issue when storing weights in float16 precision (#35398)
Add reminder config to issue template and print DS version in env (#35156)
Fix PaliGemma Pad Token Masking During Training #35855 (#35859)
Mllama fsdp (#36000)
Add git LFS to AMD docker image (#36016)
skip `test_initialization` for `VitPoseBackboneModelTest` for now (#36154)
Fix test fetcher (#36129)
Add more rigerous non-slow grad accum tests (#35668)
Update doc re list of models supporting TP (#35864)
adding option to save/reload scaler (#34932)
Fix multi gpu loss sync condition, add doc and test (#35743)
 Optim: APOLLO optimizer integration (#36062)
multi-gpu: fix tensor device placements for various models (#35763)
üö® Remove cache migration script (#35810)
Bump cryptography from 43.0.1 to 44.0.1 in /examples/research_projects/decision_transformer (#36142)
Bump transformers from 4.38.0 to 4.48.0 in /examples/research_projects/vqgan-clip (#36136)
Fix Gradient Checkpointing for Deberta & Deberta-V2 using PEFT / Adapters (#35898)
[commands] remove deprecated/inoperational commands (#35718)
VLM: enable skipped tests (#35746)
Add utility for Reload Transformers imports cache for development workflow #35508 (#35858)
Whisper: remove redundant assisted generation tests (#34814)
added warning to Trainer when label_names is not specified for PeftModel (#32085)
add RAdamScheduleFree optimizer (#35313)
Add pipeline parallel plan to `PretrainedConfig` and `PreTrainedModel` (#36091)
[docs] update awq doc (#36079)
[docs] minor doc fix (#36127)
Make `output_dir` Optional in `TrainingArguments` #27866 (#35735)
update tiktoken integ to use converted (#36135)
Fix CI issues  (#35662)
Fix max size deprecated warning (#34998)
update awesome-transformers.md. (#36115)
fix: typos in documentation files (#36122)
Add common test for `torch.export` and fix some vision models (#35124)
Fix nighlty CIs: missing atols (#35903)
AutoformerForPrediction test add atol (#36017)
[generate] shape checks in tests compatible with fixed-length caches (+ some minor fixes) (#35993)
fix bnb warning (#36116)
[Bugfix] fix file name of docstring in utils/check_table.py (#36108)
Revert checkpoint tmp dir (#36112)
Refactor OPT model (#36101)
Remove Multi-threaded image conversion for fast image processors (#36105)
Enable pytest live log and show warning logs on GitHub Actions CI runs (#35912)
Support constant lr with cooldown (#35453)
Add Apple's Depth-Pro for depth estimation (#34583)
Paligemma: revert #36084 (#36113)
Chat template: update for processor (#35953)
Processors: allow tuples of images when checking (#36084)
fix MllamaVisionAttention typehint (#35975)
[docs] fix not-working example code in `perf_infer_gpu_one.md` (#36087)
[docs] fix typo (#36080)
[docs] fix model checkpoint name (#36075)
Fix OS err (#36094)
Move audio top_k tests to the right file and add slow decorator (#36072)
Fix bug in apply_rotary_pos_emb_flashatt: in Qwen2-5-VL (#36065)
Adding RT-DETRv2 for object detection (#34773)
[docs] fix outdated example code in `trainer.md` (#36066)
Fix StopStringCriteria to handle tokens above len(tokenizer) (#35797)
Fix model kwargs (#35875)
Fix words typos in ggml test. (#36060)
Nail in edge case of torch dtype being overriden permantly in the case of an error (#35845)
Save checkpoint to temporary directory to handle partial saves during failures (#35580)
Paligemma: fix generation with Gemma2 (#36044)
Update `test_flash_attn_2_can_dispatch_composite_models` (#36050)
Fix repo consistency (#36063)
Fix usage of unpad_input function (#35925)
Iterative generation using Input embeds and `past_key_values` (#35890)
Add `Qwen2VLImageProcessorFast` into `Qwen2VLProcessor` (#35987)
Fix Audio Classification Pipeline top_k Documentation Mismatch and Bug #35736 (#35771)
Fix how we compute the final non-padding token for ForSequenceClassification models (#35911)
[docs] no hard-coding cuda (#36043)
[docs] fix bugs in the bitsandbytes documentation (#35868)
[docs] no hard coding cuda as bnb has multi-backend support (#35867)
DeepSpeed github repo move sync (#36021)
add support for empty list as input to create_model_card (#36042)
Add XPU type for work-around -inf mask causing sdpa NaN issue in modeling files (#35647)
Fix synced multi-GPU generation with LLMs and VLMs (#35893)
Fix Gemma2 synced multi-GPU generation (#35232)
Refactoring of ImageProcessorFast (#35069)
Add DAB-DETR for object detection (#30803)
Update tests regarding attention types after  #35235 (#36024)
CircleCI with python 3.9 (#36027)
feat(ci): ignore trufflehog unverified results (#36031)
Hotfix for `self-comment-ci.yml` (#36030)
Display warning for unknown quants config instead of an error (#35963)
Commont bot CI for other jobs (`generation` / `quantization`) (#35341)
Fix RMSNormGated in Zamba2 (#35943)
Fix device mismatch error in Whisper model during feature extraction (#35866)
Refactor (and fix) gpt_neox (#35610)
Update Mistral converter (#35967)
layernorm_decay_fix (#35927)
apply_chat_template: consistent behaviour for return_assistant_tokens_mask=True return_tensors=True (#35582)
Fix custom kernel for DeformableDetr, RT-Detr, GroindingDINO, OmDet-Turbo in Pytorch 2.6.0 (#35979)
Qwen2-VL: fix rope delta calculation (#36013)
Update Granite Vision Model Path / Tests (#35998)
Add mean_resizing for every VLMs' resizing_token_embeddings() (#35717)
Update-tp test (#35844)
use torch 2.6 for daily CI (#35985)
Add GOT-OCR 2.0 to Transformers (#34721)
[Moshi] disable automatic compilation if the model can't compile (#35992)
[Moonshine] compute head_dim_padding at init (#35984)
Add support for nested images to LLava and VipLLava (#35558)
Handle empty change indices in SAM's mask to rle conversion (#35665)
not to use A100 for `benchmark.yml` (#35974)
Support batching for UsefulSensors Moonshine (#35922)
Less flaky for `TimmBackboneModelTest::test_batching_equivalence` (#35971)
Revert p_mask to a list in DQA pipeline (#35964)
Whisper: fix static cache CI (#35852)
Pixtral: vectorize patch embeddings and enable tests (#35122)
[bart] minor test fixes (#35965)
Fix is_causal being a tensor (#35791)
fix iterator overflow when gradient accumulation is 1 (#35960)
[generate] move max time tests (#35962)
Update README.md (#35958)
[tests] further fix `Tester object has no attribute '_testMethodName'`  (#35781)
update docker file `transformers-pytorch-deepspeed-latest-gpu` (#35940)
Trainer Refactor: Part 1 (#35567)
Output dicts support in text generation pipeline (#35092)
Fix flaky `test_assisted_decoding_matches_greedy_search` (#35951)
Update `squad_convert_example_to_features` to work with numpy v2 (#35955)
Update `unwrap_and_save_reload_schedule` to use `weights_only=False` (#35952)
fix `test_generated_length_assisted_generation` (#34935)
use torch constraints to check if covariance is positive definite during mean resizing. (#35693)
Remove INC notebook reference in documentation (#35936)
fix(FA): QKV not being casted to target_dtype for FA with dpo lora (#35834)
Test: generate with `torch.compile(model.forward)` as a fast test (#34544)
Fix TP initialization (#35860)
Qwen-2-5-VL: fix CI (#35935)
Fix mask slicing for models with HybridCache (#35681)
Fix: loading DBRX back from saved path (#35728)
Add default TP plan for all models with backend support (#35870)
Use rocm6.2 for AMD images (#35930)
Remove `_supports_static_cache = True` for some model classes (#34975)
[docs] Fix Zamba2 (#35916)
Close Zamba2Config code block (#35914)
Fix the config class comparison for remote code models (#35592)
[docs] uv install (#35821)
Fix typing in audio_utils.chroma_filter_bank (#35888)
Split and clean up GGUF quantization tests (#35502)
üö®üö®üö® image-classification pipeline single-label and multi-label prob type squashing fns (sigmoid vs softmax) are backwards (#35848)
üî¥ üî¥ üî¥  Added `segmentation maps` support for DPT image processor (#34345)
Update deepspeed amd image (#35906)
Add Zamba2 (#34517)
Fix fast image processor warnings in object detection examples (#35892)
[doctest] Fixes (#35863)
Add `Rocketknight1` to `self-comment-ci.yml` (#35881)
add xpu device check in device_placement (#35865)
use torch.testing.assertclose instead to get more details about error in cis (#35659)
Fix Llava-NeXT / Llava-NeXT Video / Llava-OneVision's token unpadding mismatch (#35779)
Fix `test_pipelines_video_classification` that was always failing (#35842)
fix apply_chat_template() padding choice (#35828)
Fix typo (#35854)
[DOC] Fix contamination and missing paragraph in translation (#35851)
Granite Vision Support (#35579)
Fix more CI tests (#35661)
Fix uploading processors/tokenizers to WandB on train end (#35701)
Fix GA loss for Deepspeed (#35808)
add qwen2.5vl (#35569)
[Backend support] Allow `num_logits_to_keep` as Tensor + add flag (#35757)
[ `tests`] remove some flash attention class tests (#35817)
Fix NoneType type as it requires py>=3.10 (#35843)
Add PyTorch version check for FA backend on AMD GPUs (#35813)
Fix compatibility issues when using auto_gptq with these older versions (#35830)
[chat] docs fix (#35840)
Fix `head_dim` in config extracted from Gemma2 GGUF model (#35818)
[Chat] Add Chat from TRL üêà  (#35714)
Fix : Nemotron tokenizer for GGUF format (#35836)
[pipeline] missing import regarding assisted generation (#35752)
[gpt2] fix generation tests (#35822)
Hotfix: missing `working-directory` in `self-comment-ci.yml` (#35833)
Init cache on meta device (#35164)
Another security patch for `self-comment-ci.yml` (#35816)
Remove pyav pin to allow python 3.11 to be used (#35823)
Remove old `benchmark` code (#35730)
[Mimi] update test expected values for t4 runners (#35696)
Improve modular documentation (#35737)
add Qwen2-VL image processor fast (#35733)
move fastspeech to audio models (#35788)
[i18n-ar] Translated file: `docs/source/ar/tasks/masked_language_modeling.md` into Arabic (#35198)
Optimized set_initialized_submodules. (#35493)
Remove deprecated `get_cached_models` (#35809)
Fixed typo in autoawq version number in an error message for IPEX backend requirements. (#35815)
Fix : BLOOM tie_word_embeddings in GGUF (#35812)
Auto-add `timm` tag to timm-wrapper models. (#35794)
Support adamw_torch_8bit (#34993)
add a new flax example for Bert model inference (#34794)
[Doc] Adding blog post to model doc for `TimmWrapper` (#35744)
Byebye `test_batching_equivalence`'s flakiness (#35729)
Add LlavaImageProcessor (#33191)
Update AMD Docker image (#35804)
Fix  "test_chat_template_dict" in video LLMs (#35660)
Deterministic sorting in modular converter when adding new functions (#35795)
modular_model_converter bugfix on assignments (#35642)
Fixes, improvements to `timm` import behaviour (#35800)
Tool calling: support more types (#35776)
fix low-precision audio classification pipeline (#35435)
Fix vits low-precision dtype (#35418)
fix document qa bf16 pipeline (#35456)
Don't import torch.distributed when it's not available (#35777)
Patch moonshine (#35731)
transformers.image_transforms.normalize wrong types (#35773)
[fix] cannot import name 'Pop2PianoFeatureExtractor' from 'transformers' (#35604)
Skip Falcon 7B GGML Test  (#35783)
remove code owners as it was generating too much noise BUT (#35784)
Remove read_video and run
[generate] update docstring of `SequenceBiasLogitsProcessor` (#35699)
fix register_buffer in MimiEuclideanCodebook (#35759)
Add SuperGlue model (#29886)
[ViTPose] Convert more checkpoints (#35638)
Security fix for `self-comment-ci.yml` (#35548)
Fix CI for VLMs (#35690)
Use AMD CI workflow defined in hf-workflows (#35058)
ci: fix xpu skip condition for test_model_parallel_beam_search (#35742)
Stop mutating input dicts in audio classification pipeline (#35754)
Revert "Unable to use `MimiModel` with DeepSpeed ZeRO-3" (#35755)
Restore is_torch_greater_or_equal_than for backward compatibility (#35734)
Grounding DINO Processor standardization (#34853)
OmDet Turbo processor standardization (#34937)
OwlViT/Owlv2 post processing standardization (#34929)
Added liger_kernel compatibility with `PeftModel` (#35680)
check is added for the report_to variable in TrainingArguments (#35403)
Unable to use `MimiModel` with DeepSpeed ZeRO-3 (#34735)
Fix some tests (#35682)
üö®üö®üö® An attempt to fix #29554. Include 'LayerNorm.' in gamma/beta rename scope, optimize string search. (#35615)
Added resource class configuration option for `check_circleci_user` job (#32866)
[generate] return Cache object even if passed in a legacy format (#35673)
[generate] can instantiate `GenerationConfig(cache_implementation="static")` (#35679)
Remove `pt_to_tf` (#35672)
üßπ remove `generate`-related objects and methods scheduled for removal in v4.48 (#35677)
[cache] add a test to confirm we can use cache at train time (#35709)
Remove batch size argument warning when unjustified (#35519)
Modular: support for importing functions from any file (#35692)
Optimize ForCausalLMLoss by removing unnecessary contiguous() call to reduce memory overhead (#35646)
Add proper jinja2 error (#35533)
[generation] fix type hint (#35725)
Fix the bug that `Trainer` cannot correctly call `torch_jit_model_eval` (#35722)
Fix condition when GA loss bug fix is not performed (#35651)
Fix: Falcon tie_word_embeddings in GGUF (#35715)
Replace deprecated batch_size with max_batch_size when using HybridCache (#35498)
Fix typo in /docs/source/ja/model_doc/decision_transformer.md URL (#35705)
Fix : Nemotron Processor in GGUF conversion (#35708)
Enable gptqmodel (#35012)
Add future import for Py < 3.10 (#35666)
Clean-up composite configs (#34603)
Enhance DataCollatorForLanguageModeling with Configurable Token Replacement Probabilities (#35251)
Enhanced Installation Section in README.md (#35094)
Fix : add require_read_token for gemma2 gated model (#35687)
Fix expected output for ggml test (#35686)
Fix : HQQ config when hqq not available (#35655)
Update torchao.md: use auto-compilation (#35490)
Fix : adding einops lib in the CI docker for some bitsandbytes tests (#35652)
Fix `zero_shot_image_classification` documentation guide link in SigLIP (#35671)
Add-helium (#35669)
[i18n-ar] Translated file : docs/source/ar/tasks/token_classification.md into Arabic (#35193)
[tests] make cuda-only tests device-agnostic (#35607)
[`Compile`] Only test compiling model forward pass (#35658)
Enable different torch dtype in sub models (#34873)
[`Phi`] bias should be True (#35650)
Removed some duplicated code (#35637)
Fix whisper compile (#35413)
Fix device in rope module when using dynamic updates (#35608)
Update codeowners with individual model owners (#35595)
Skip `MobileNetV1ModelTest::test_batching_equivalence` for now (#35614)
Fix flaky `test_beam_search_low_memory` (#35611)
Let `EarlyStoppingCallback` not require `load_best_model_at_end` (#35101)
Added error when sequence length is bigger than max_position_embeddings (#32156)
Use inherit tempdir makers for tests + fix failing DS tests (#35600)
Fix flaky `test_custom_4d_attention_mask` (#35606)
v4.49.0-dev
[WIP] Emu3: add model (#33770)
Fix flex_attention in training mode (#35605)
Remove `benchmark.py` after #34275
Chat template: return vectorized output in processors (#34275)
Add Moonshine  (#34784)
Skip `torchscript` tests if a cache object is in model's outputs (#35596)
ModernBert: reuse GemmaRotaryEmbedding via modular + Integration tests (#35459)
Add flex_attn to diffllama (#35601)
ModernBERT bug fixes (#35404)
add `_supports_flex_attn = True` for models that do support it (#35598)
[doc] deepspeed universal checkpoint (#35015)
Refactor/fix Cohere2 (#35594)
[`tokenizers`] Ensure that add_prefix_space is propagated to backend_tokenizer.pre_tokenizer (#35593)
Fix modular edge case + modular sorting order (#35562)
PR for Issue #22694: Fixed Training Evaluation table display for VSCode (#35557)
Small fix rope kwargs (#35589)
Fix flaky `SwitchTransformersModelTest::test_training_gradient` (#35587)
`tokenizer` train from iterator without pre_tokenizers  (#35396)
feat: add TP plan for granite (#35573)
[Idefics3] Move image features to same device as input embeds (#35100)
Add inputs_embeds param to ModernBertModel (#35373)
Fix flaky `test_batching_equivalence` (#35564)
Setup loss_type in config at model init time (#34616)
Re-add missing __all__ for Cohere and Phi3 (#35578)
Minor fix in video text 2 text docs (#35546)
More model refactoring! (#35359)
Don't show warning for `inv_freq` buffers (#35255)
Fix multi-gpu loss (#35395)
update code owners (#35576)
[i18n-ar] Translated file: `docs/source/ar/tasks/multiple_choice.md` into Arabic (#35199)
Fix all output_dir in test_trainer.py to use tmp_dir (#35266)
Pipeline: simple API for assisted generation (#34504)
[`PixtralLarge`] Update Pixtral conversion script to support large format! (#34801)
[docs] Remove Hiera from AUDIO MODELS in docs (#35544)
ovewrite top_k when crate audio classification pipeline (#35541)
add code owners (#35528)
Add ViTPose (#30530)
fix: Qwen2-VL generate with inputs_embeds (#35466)
Update doc for `metric_for_best_model` when `save_strategy="best"`. (#35389)
Add: num_additional_image_tokens to models (#35052)
Enable auto task for timm models in pipeline (#35531)
Bump torch requirement to >= 2 (#35479)
Timm wrapper label names (#35553)
Update missing model error message (#35370)
Update doc and default value of TextNetImageProcessor (#35563)
Add support for modular with fast image processors (#35379)
[Docs] links to `logits-processor-zoo` (#35552)
Fix Qwen2VL processor to handle odd number of frames (#35431)
support chat generator as input of TextGenerationPipeline (#35551)
Pass correct `num_items_in_batch` value into the training_step function (#35438)
MODERNBERT_INPUTS_DOCSTRING: past_key_values are ignored (#35513)
VLMs: major clean up üßº  (#34502)
Add TextNet (#34979)
[docs] Remove sortish_sampler (#35539)
Correctly list the chat template file in the Tokenizer saved files list (#34974)
[Whisper] fix docstrings typo (#35338)
[Qwen2Audio] handle input ids expansion during processing (#35534)
Release GPU memory after Optuna trial (#35440)
Check whether rescale is requested before checking is_scaled_image (#35439)
Fix bug when requesting input normalization with EnCodec (#34756)
Add diffllama (#34083)
NPU support SDPA (#35165)
Replace tokenizer to processing_class in Seq2SeqTrainer (#35452)
ci: mark model_parallel tests as cuda specific (#35269)
Zamba new attention standard (#35375)
[Dinov2 with Registers] Some fixes (#35411)
added logic for deleting adapters once loaded (#34650)
Fixed typo in Llama configuration docstring (#35520)
üåê [i18n-KO] Remove duplicates in toctree (#35496)
[GGUF] Refactor and decouple gguf checkpoint loading logic (#34385)
Bump jinja2 from 3.1.4 to 3.1.5 in /examples/research_projects/decision_transformer (#35408)
Update llm_optims docs for `sdpa_kernel` (#35481)
üåê [i18n-KO] Translated `altclip.md` to Korean (#34594)
Add check for if num_items_in_batch is not None (#35102)
Add `position_ids` in `XLMRobertaXLForCausalLM.prepare_inputs_for_generation` (#35044)
Add French translation of task_summary and tasks_explained (#33407)
Idefics: fix docstring (#35079)
 Fix Llava conversion for models that use safetensors to store weights (#35406)
Applies the rest of the init refactor except to modular files (#35238)
Add Gemma2 GGUF support (#34002)
Reuse "if not" logic in image_processing. (#35405)
Use `sdpa_kernel` in tests (#35472)
Change `is_soundfile_availble` to `is_soundfile_available` (#35030)
Fix paligemma warning message (#35486)
Fix docs typos. (#35465)
Fix new BNB test failures (#35345)
Reintroduce Python 3.9 support for ModernBERT (#35458)
Update translated docs for `sdpa_kernel` (#35461)
[i18n-ar] Translated file: `docs/source/ar/tasks/summarization.md` into Arabic (#35195)
[i18n-ar] Translated file: `docs/source/ar/tasks/question_answering.md` into Arabic (#35196)
Update docs for `sdpa_kernel` (#35410)
Add compute_loss_func to Seq2SeqTrainer (#35136)
Update perf_infer_gpu_one.md: fix a typo (#35441)
Fix `model_accepts_loss_kwargs` for timm model (#35257)
Fix f-string to show `ACCELERATE_MIN_VERSION` on error (#35189)
CLIP conversion script - Change fairseq to OpenAI (#35384)
Fix: Rename keyword argument in_channels to num_channels (#35289)
Drop inplace operation for loss computation with gradient accumulation (#35416)
[`GPTQ`, `CompressedTensors`] Fix unsafe imports and metada check (#34815)
Add DINOv2 with registers (#35348)
enable non-cuda awq model support without modify version (#35334)
Disable  `.github/workflows/self-comment-ci.yml` for now (#35366)
Add compile test for fast image processor (#35184)
Adding logger.info about update_torch_dtype in some quantizers (#35046)
bugfix Idefics3 processor - handle gracefully cases with text and no images (#35363)
HIGGS Quantization Support (#34997)
add bnb support for Ascend NPU (#31512)
Fix : VPTQ test (#35394)
Fix typing in docstring for `PaliGemmaProcessor` (#35278)
Scale loss before backward (#35207)
Deprecate _is_quantized_training_enabled (#34991)
uniformize kwargs for SAM (#34578)
Patch GPTNeoX to use adequate FA2 if position_ids is provided (#35318)
make LlamaModel._update_causal_mask torch compilable (#35187)
bitsandbytes: simplify 8bit dequantization (#35068)
Fix new FA2 if `is_causal` is passed explicitly (#35390)
owlvit/2 dynamic input resolution (#34764)
[docs] Follow up register_pipeline (#35310)
Improved Documentation Of Audio Classification (#35368)
Improve modular transformers documentation (#35322)
Make `test_generate_with_static_cache` even less flaky (#34995)
Use `weights_only=True` with `torch.load` for `transfo_xl` (#35241)
Update test fetcher when we want to test all (#35364)
update codecarbon (#35243)
bugfix: torch.export failure caused by `_make_causal_mask` (#35291)
Aurevoir PyTorch 1 (#35358)
fix zoedepth initialization error under deepspeed zero3 (#35011)
Add Tensor Parallel support for Qwen2VL (#35050)
Cleaner attention interfaces (#35342)
Implement AsyncTextIteratorStreamer for asynchronous streaming (#34931)
Reduce CircleCI usage (#35355)
FEAT : Adding VPTQ quantization method to HFQuantizer (#34770)
[`Mamba2`] Fix caching, slow path, and multi-gpu (#35154)
fix onnx export of speech foundation models (#34224)
[`docs`] Add link to ModernBERT Text Classification GLUE finetuning script (#35347)
Modernbert Release Fixes (#35344)
Fix some fa2 tests (#35340)
Add ModernBERT to Transformers (#35158)
PaliGemma: Make sure to add <eos> to suffix if <image> is present in `text` (#35201)
Update comment CI bot (#35323)
Fix documentation for ColPali (#35321)
Add the Bamba Model (#34982)
feat: add `benchmarks_entrypoint.py` (#34495)
üö®All attention refactorüö® (#35235)
[Whisper] fix docstrings typo (#35319)
change bnb tests (#34713)
[Whisper] üö® Fix whisper decoding üö® (#34135)
Trigger GitHub CI with a comment on PR (#35211)
[tests] make cuda-only tests device-agnostic   (#35222)
Fix loading with only state dict and low_cpu_mem_usage = True (#35217)
[docs] Improve register_pipeline (#35300)
Fixed typo in audio_classification.md (#35305)
Add Cohere2 docs details (#35294)
Fix remove unused parameter in docs (#35306)
Fix image preview in multi-GPU inference docs (#35303)
Fix typos in translated quicktour docs (#35302)
üö®üö®üö® Limit backtracking in Nougat regexp (#35264)
remove `benchmark` job in `push-important-models.yml` (#35292)
üö®üö®üö® Delete conversion scripts when making release wheels (#35296)
Support for SDPA for SAM models (#34110)
Add sdpa for Beit (#34941)
Add Falcon3 documentation (#35307)
Add ColPali to ü§ó transformers (#33736)
fix modular order (#35297)
Improved documentation of Automatic speech recognition (#35268)
Fix wrongs in quicktour[zh] (#35272)
Translating "translate perf_infer_gpu_multi.md" to Chinese (#35271)
Fix typos in Translated Audio Classification Docs (#35287)
[Whisper] patch float type on mps (#35295)
Delete redundancy for loop checks. (#35288)
Temporarily disable amd push ci (#35293)
Fix : model used to test ggml conversion of Falcon-7b is incorrect (#35083)
Blip: fix offloading and MP tests  (#35239)
Aggeregate test summary files in CircleCI workflow runs (#34989)
Fall back to slow image processor in ImageProcessingAuto when no fast processor available (#34785)
[i18n-Chinese] Translating perf_train_cpu.md to Chinese (#35242)
don't use no_sync when deepspeed doesn't support it for certain zero stages (#35157)
Fix FSDP no longer working (#35212)
Translating agents_advanced.md to Chinese (#35231)
Fixed typos in Audio Classification Documentation (#35263)
Update AMD docker image (rocm 6.1) (#35259)
Use `rsfE` with `pytest` (#35119)
[tests] fix "Tester object has no attribute '_testMethodName'" (#34910)
skip Fuyu from test_generate (#35246)
Add Cohere2 model (#35224)
Run model as compressed/uncompressed mode (#34719)
Fix typo in chat template example (#35250)
[Init refactor] Modular changes (#35240)
Change back to `Thread` for SF conversion (#35236)
Refactoring `AssistedCandidateGenerator` for Improved Modularity and Reusability (#35009)
Support Python 3.10+ Union style in chat template type hints parsing (#35103)
Fix type hints for apply_chat_template (#35216)
Fixed typo of 'indentifier' in audio_utils.py (#35226)
docs: clarify initializer_range parameter description in Idefics3VisionConfig (#35215)
Fix seamless TTS generate (#34968)
Fix CI (#35208)
Cleanup: continue the init refactor (#35170)
Add TimmWrapper (#34564)
[PEFT] Better Trainer error when prompt learning with loading best model at the end (#35087)
üßπ Remove deprecated RotaryEmbedding parts in the Attention layers (#34858)
BLIP: enable device map (#34850)
[i18n-<languageCode>] Translating agents.md to Chinese  (#35139)
Update data collator docstrings to accurately reference Nvidia tensor core compute capability version (#35188)
[docs] Fix FlashAttention link (#35171)
[i18n-<languageCode>] Translating Benchmarks.md to Chinese (#35137)
Only import torch.distributed if it is available (#35133)
Multiple typo fixes in NLP, Audio docs (#35181)
[i18n-ar] Translated file : `docs/source/ar/community.md` into Arabic (#33027)
Fixing GGUF support for StableLm (#35060)
Fix DBRX LayerNorm init method (#35177)
Remove unnecessary masked_fill in deberta models (#35182)
Support BatchNorm in Hubert pos_conv_emb as in fairseq (#34389)
Fix file path for shard_num 1 with mllama converter (#35053)
Assisted decoding multi-gpu (#35116)
Fix `num_items_in_batch` not being an integer (#35115)
[CI] Fix bnb quantization tests with accelerate>=1.2.0 (#35172)
Fixed typo of 'avilable' in prompts.py (#35145)
Super tiny fix logging message (#35132)
Cleanup: continue the init refactor (#35167)
Fix typo in EETQ Tests (#35160)
Option to set 'non_blocking' for to(device) in BatchEncoding and BatchFeature (#34883)
Corrected typo in agent system prompts (#35143)
[I-JEPA] Update docs (#35148)
Fix GA loss bugs and add unit test (#35121)
Update I-JEPA checkpoints path (#35120)
Add feature dim attributes to BitLinear for easier PEFT integration (#34946)
Add Aria (#34157)
Fix private forked repo. CI (#35114)
[docs] top_p, top_k, temperature docstrings (#35065)
[docs] Update Python version in translations (#35096)
Dev version
Fix signatures for processing kwargs (#35105)
Adaptive dynamic number of speculative tokens (#34156)
Fix flaky Hub CI (`test_trainer.py`) (#35062)
[`trainer`] fix the GA `model_accepts_loss_kwargs` (#34915)
BLIP: this is correct now (#35081)
Add I-JEPA (#33125)
Deprecate quanto and switch to optimum-quanto (#35001)
Fix `tie_word_embeddings` handling for GGUF models (#35085)
Update Mistral conversion script (#34829)
[`tokenizers`] bump to 0.21 (#34972)
[Whisper] Fix whisper tokenizer (#34537)
Informative (#35059)
[docs] Increase visibility of torch_dtype="auto" (#35067)
[docs] add a comment that offloading requires CUDA GPU (#35055)
Support for easier multimodal use of modular (#35056)
[`GPTNeoX`] Flex Attention + Refactor (#34896)
Add Pytorch Tensor Parallel support for Qwen2, Qwen2Moe, Starcoder2 (#35007)
Fix `pad_token_tensor` is None in warning (#34005)
[docs] use device-agnostic API instead of hard-coded cuda (#35048)
[docs] use device-agnostic instead of `cuda` (#35047)
Translate community.md into Chinese (#35013)
[docs] fix example code bug (#35054)
fix speecht5 failure issue in test_peft_gradient_checkpointing_enable‚Ä¶ (#34454)
Fix `BertGeneration` (#35043)
Add token cost + runtime monitoring to Agent and HfEngine children (#34548)
Automatic compilation in generate: do not rely on inner function (#34923)
Translate bertlogy.md into Chinese (#34908)
[docs] add the missing import for Image and bug fix (#34776)
[i18n-ar] Translated file : `docs/source/ar/notebooks.md` into Arabic (#33049)
add docstring example for compute_loss_func (#35020)
Multiple typo fixes in Tutorials docs (#35035)
Fix `test_eager_matches_sdpa_inference` for `XPU` backend (#34889)
Add type hints for forward functions in Gemma2 (#35034)
Typo in warning switching to optimum-quanto (#35028)
Optimize memory usage of mllama encoder (#34930)
fix variable undefined bug when return_tensors is not specified in llava processing (#34953)
Only cast `cu_seqlens` when tracing (#35016)
Update `FillMaskPipeline.__call__` signature and docstring (#35006)
fix: double verbs (#35008)
Update timm version (#35005)
üö®üö®üö® Uniformize kwargs for TrOCR Processor (#34587)
Let server decide default repo visibility (#34999)
Fix docker CI : install autogptq from source (#35000)
Improve `.from_pretrained` type annotations (#34973)
Add optimized `PixtralImageProcessorFast` (#34836)
Fix `utils/check_bad_commit.py` (for auto ping in CI) (#34943)
Offloaded cache: fix generate (#34921)
Allow compressed-tensors quantized model to be trained (#34520)
Refine the code of Universal Assisted Generation (#34823)
üö®üö®üö® Changed DINOv2Config default patch size to 14 (#34568)
Fix `save_pretrained` for partially offloaded models (#34890)
[PEFT] Set eval mode when loading PEFT adapter (#34509)
Fixed typo in `VisitWebpageTool` (#34978)
Fix typo in code block in vipllava.md (#34957)
[i18n-zh]Translated perf_train_special.md into Chinese (#34948)
[docs] add explanation to `release_memory()` (#34911)
üåê [i18n-KO] Translated encoder-decoder.md to Korean (#34880)
Fix flaky test execution caused by `Thread` (#34966)
Avoid calling `get_max_length` (#34971)
Fix : Add PEFT from source to CI docker (#34969)
[`FlexAttention`] Update gemma2 (#34942)
[i18n-zh]Translated tiktoken.md into chinese (#34936)
docs: HUGGINGFACE_HUB_CACHE -> HF_HUB_CACHE (#34904)
[doc] use full path for run_qa.py  (#34914)
[docs] use device-agnostic API instead of cuda  (#34913)
[i18n-ar] Translated file : `docs/source/ar/benchmarks.md` into Arabic (#33023)
Update the Python version in the Chinese README to match the English README.  (#34870)
Fix torch.onnx.export of Qwen2-VL vision encoder (#34852)
Separate chat templates into a single file (#33957)
change apply_rotary_pos_emb of Glmmodel for GLM-Edge Series model (#34629)
Add Pytorch Tensor Parallel support for Mistral (#34927)
[Whisper] Fix whisper integration tests (#34111)
Skipping aqlm non working inference tests till fix merged (#34865)
VideoLLaVA: add default values (#34916)
Fix import structure for Fast Image processors (#34859)
making gpt2 fx traceable (#34633)
Updated documentation and added conversion utility (#34319)
Fix failling GGML test (#34871)
Upgrade torch version to 2.5 in dockerfile for quantization CI (#34924)
Fix `test_auto_backbone_timm_model_from_pretrained` (#34877)
fix static cache data type miss-match (#34799)
[AWQ, CI] Bump AWQ version used in docker image (#34922)
Fix : BitNet tests (#34895)
Rename OLMo November to OLMo2 (#34864)
Bump tornado from 6.4.1 to 6.4.2 in /examples/research_projects/lxmert (#34917)
Fix Qwen2 failing tests (#34819)
[`peft`] Given that `self.active_adapter` is deprecated, avoid using it (#34804)
Fix convert_tokens_to_string when decoder is None (#34569)
chore: fix some typos (#34891)
Bump tornado from 6.4.1 to 6.4.2 in /examples/research_projects/visual_bert (#34887)
prepare_fa2_from_position_ids function bugfix (#33269)
allow unused input parameters passthrough when chunking in asr pipelines (#33889)
Sum gathered input tokens (#34554)
üî¥ Mllama: fix base prefix (#34874)
[`Deberta/Deberta-v2`] Refactor code base to support compile, export, and fix LLM (#22105)
BLIP: fix generation after hub update (#34876)
Cache: init empty cache when `use_cache` (#34274)
Add safe_globals to resume training on PyTorch 2.6 (#34632)
Fix: Enable prefill phase key value caching of nemotron/minitron models (#34742)
Fix support for image processors modifications in modular (#34866)
Bitnet test fix to avoid using gated  model (#34863)
[CI] Skip EETQ tests while package is broken with latest transformers (#34854)
smol improvements to support more flexible usage (#34857)
Speculative decoding: Test the target distribution (to prevent issues like #32867) (#34553)
Auto compile when static cache (#34247)
Remove quantization related config from dequantized model (#34856)
Update checks for torch.distributed.tensor to require torch >= 2.5 (#34816)
Watermarking: fix order (#34849)
Refactor StarCoder2 using modular (#34015)
Fix heuristic scheduling for UAG (#34805)
Fix ds nvme (#34444)
Improve gguf tensor processing (#34515)
Add Nemotron GGUF Loading Support (#34725)
Change logging level from warning to info for `max_steps` overriding `num_train_epochs` (#34810)
VLMs: enable generation tests - last batch (#34484)
Fix CI slack reporting issue (#34833)
Fix CI by tweaking torchao tests (#34832)
Fix hyperparameter search when optuna+deepseed (#34642)
Torchao weights only + prequantized compability (#34355)
Fix: take into account meta device (#34134)
fix(DPT,Depth-Anything) `torch.export` (#34103)
Fix the memory usage issue of logits in generate() (#34813)
Fix low memory beam search (#34746)
LLaVA OV: fix unpadding precision (#34779)
Translate attention.md into Chinese (#34716)
Added image-text-to-text pipeline to task guide (#34783)
Fix `check_training_gradient_checkpointing` (#34806)
Run `test_medium_seamless_m4t_pt` in `subprocess` to avoid many failures (#34812)
Add Image Processor Fast Deformable DETR (#34353)
Add support for OpenAI api "image_url" input in chat for image-text-to-text pipeline (#34562)
Bump aiohttp from 3.10.2 to 3.10.11 in /examples/research_projects/decision_transformer (#34792)
fix crash in tiiuae/falcon-11B-vlm image-to-text generation (#34728)
Fix post process function called in the instance segmentation example of mask2former (#34588)
Add do_convert_rgb to vit (#34523)
Feature: print tokens per second during training (#34507)
üö®üö®üö® fix(Mask2Former): torch export üö®üö®üö® (#34393)
MLU devices : Checks if mlu is available via an cndev-based check which won't trigger the drivers and leave mlu (#34326)
Modular fix (#34802)
Fix cache_utils for optimum.quanto kvcache quantization  (#34750)
Gemma capping (#34282)
Self-speculation (Layer-Skip Llama) (#34240)
fix cpu bnb path (#34647)
Fix: siglip image processor rgb_convert is not being applied correctly. (#34301)
Support gradient checkpointing in Qwen2VL ViT (#34724)
feat: allow to use hf-hub models for timm backbone (#34729)
Trainer hyperparameter search kwargs docs update (#34459)
protect tensor parallel usage (#34800)
Fix Whisper CI (#34617)
Allow handling files as args for a tool created with Tool.from_space (#34687)
Simplify Tensor Parallel implementation with PyTorch TP (#34184)
fix: Wrong task mentioned in docs (#34757)
Fix callback key name (#34762)
fix: Update pixel_values parameter in hf_model input (#34782)
[tests] add XPU part to testing (#34778)
[docs] add XPU besides CUDA, MPS etc. (#34777)
[docs] make `empty_cache` device-agnostic (#34774)
make sure to disable gradients for integer tensor (#32943)
Fix skip of test_training_gradient_checkpointing (#34723)
fix a typo bug where 'id2label' was incorrectly written as 'i2label' when reading config (#34637)
Fix broken link (#34618)
VLMs: `patch_size` -> `num_image_tokens` in processing (#33424)
Add OLMo November 2024 (#34551)
üßº remove v4.44 deprecations (#34245)
Remove FSDP wrapping from sub-models. (#34452)
FSDP grad accum fix (#34645)
add xpu path for awq (#34712)
fix(wandb): pass fake dataset to avoid exception in trainer (see #34455) (#34720)
Update llava.md (#34749)
Retain newlines in chat template when `continue_final_message=True` (#34253)
[docs] add xpu device check  (#34684)
Fix example in EsmConfig docstring (#34653)
[docs] Broken link in generation_strategies (#34717)
üåê [i18n-KO] Translated marian.md to Korean (#34698)
Agents: Small fixes in streaming to gradio + add tests (#34549)
[i18n-ar] Translated file : `docs/source/ar/torchscript.md` into Arabic (#33079)
[docs] update not-working model revision (#34682)
Agents: turn any Space into a Tool with `Tool.from_space()` (#34561)
Update llm_engine.py (#33332)
[i18n-ar] Translated file : `docs/source/ar/trainer.md` into Arabic (#33080)
üåê [i18n-KO] Translated bert.md to Korean  (#34627)
üåê [i18n-KO] Translated `timesformer.md` to Korean (#33972)
fix(dvclive): pass fake dataset to avoid exception in trainer init (#34455)
üåê [i18n-KO] Translated `convbert.md` to Korean (#34599)
Fix `use_parallel_residual` and `qkv_bias` for StableLM GGUF config extraction (#34450)
Fix torchvision interpolation CI (#34539)
Changing __repr__ in torchao to show quantized Linear (#34202)
Remove `@slow` for `test_eager_matches_sdpa_inference` (#34558)
Fix  #34494 assistant tokens when truncated (#34531)
Revert "Fix Whisper CI" (#34605)
Remove unused test_dataset (#34516)
DistilBERT is ExecuTorch compatible (#34475)
Load sub-configs from composite configs (#34410)
FIX: Broken repr of TorchAoConfig (#34560)
Skip DeepSpeed ZeRO Stage 3 model initialization when bnb (#34395)
Fix Whisper CI (#34541)
fix TrainerState doc because num_input_tokens_seen is unused by defau‚Ä¶ (#34593)
üåê [i18n-KO] Update README_ko.md (#33098)
üåê [i18n-KO] Translated perf_train_special.md to Korean (#34590)
[i18n-HI] Translated TFLite page to Hindi (#34572)
Add text support to the Trainer's TensorBoard integration (#34418)
MPS: `isin_mps_friendly` can support 0D tensors (#34538)
VLM: special multimodal Tokenizer (#34461)
Update trainer for easier handling of accumulate, compile fixes, and proper reporting (#34511)
[i18n-HI] Translated accelerate page to Hindi (#34443)
Large modular logic refactoring (#34487)
:red_circle: :red_circle:  fix `query_pre_attn_scalar` different of `num_heads` in default gemma2 config (#34540)
BLIP: enable generation tests (#34174)
Blip: get/set input embeddings correctly (#34152)
[i18n-ar] Translated file : `docs/source/ar/multilingual.md` into Arabic (#33048)
update doc (#34478)
[CLIPSeg] Make interpolate_pos_encoding default to True (#34419)
Add image text to text pipeline (#34170)
Bug Fix for issue #34294 (#34295)
make `test_eager_matches_sdpa_inference `less flaky (#34512)
feat: add benchmarks pg indexes (#34536)
fix(DPT,Depth-Anything) Address expected_slice errors inside inference tests (#34518)
Qwen2VL: skip base `input_ids`-`inputs_embeds` equivalence check (#34535)
avoid calling `gc.collect` and `cuda.empty_cache` (#34514)
Fix step shifting when accumulate gradient (#33673)
Fix: img size mismatch caused by incorrect unpadding in LLaVA-Next (#34522)
enable QA bf16 pipeline (#34483)
UPDATE Documentation for #TRANSLATING.md Documentation into Multiple Languages.(Changes made) (#34226)
Add Image Processor Fast RT-DETR (#34354)
Fix super tiny extra space typo (#34440)
Add GGUF for Mamba (#34200)
Use torch 2.5 in scheduled CI (#34465)
fix pixtral processor (#34486)
Tests: move `generate` tests to the right mixin and delete redundant tests (#34464)
VLMs: fix number of image tokens (#34332)
Mllama: update docs (#34334)
Fix format mistake in string repr of tokenizer objects (#34493)
Roberta is ExecuTorch compatible (#34425)
Un-deprecate timeout arg in pipelines (#34382)
fix incorrect warning (#34416)
Fix performance in get_imports regexp (#34298)
Bump werkzeug from 3.0.3 to 3.0.6 in /examples/research_projects/decision_transformer (#34420)
Adding `optimizer_cls_and_kwargs` to `Trainer.__init__` (#34358)
Albert is ExecuTorch compatible (#34476)
MobileBERT is ExecuTorch compatible (#34473)
Bug fix for drop path decay rate in swin transformer (#34291)
fix-qwen2vl-no-position_ids (#33487)
manual `head_dim` for `mixtral` model (#34281)
Bert is ExecuTorch compatible (#34424)
Fix regression loading dtype (#34409)
Fixes for Modular Converter on Windows (#34266)
Fix perplexity computation in perplexity.md (#34387)
Simplify running tests in a subprocess (#34213)
üö®üö®üö® [SuperPoint] Fix keypoint coordinate output and add post processing (#33200)
use a tinymodel to test generation config which aviod timeout (#34482)
Fix CI (#34458)
Generation: fix test (#34369)
LLaVA: latency issues (#34460)
Add `post_process_depth_estimation` for GLPN (#34413)
feat: run benchmarks on A100 (#34287)
enable average tokens across devices (#34373)
[i18n-ar] Translated file : `docs/source/ar/fast_tokenizers.md` into Arabic (#33034)
Apply linting to the important code blocks to make it readable (#34449)
üåê [i18n-KO] Translated `model_doc/barthez.md` to Korean (#33980)
[docs] update input documentation for MAMBA2 and MISTRAL models to include cache_position and attention_mask details (#34322)
New option called `"best"` for `args.save_strategy`. (#31817)
exclude fsdp from delay_optimizer_creation (#34140)
Fix batch size handling in prediction_loop for DataLoaderShard (#34343)
Tiny update after #34383 (#34404)
pin `tensorflow_probability<0.22` in docker files (#34381)
Fix pix2struct (#34374)
[docs] Cache implementations (#34325)
Fix typos in agents_advanced.md (#34405)
Avoid check expected exception when it is on CUDA (#34408)
Fix bnb training test failure (#34414)
Tests: upgrade `test_eager_matches_sdpa_generate` (#34386)
SynthID: better example (#34372)
no filter (#34391)
Fix right padding in LLaVA models (#34305)
Fix onnx non-expotable inplace aten op (#34376)
Use non nested images and batched text Idefics2/3  (#34222)
Fix glm  (#34388)
[auto. ping] Avoid sending empty info + add more team members (#34383)
Correct the new defaults (#34377)
Fix `torch.fx` issue related to the new `loss_kwargs` keyword argument (#34380)
[PEFT] Add warning for missing key in LoRA adapter (#34068)
Ignore unsupported kwarg in ProcessorMixin call (#34285)
refactor: remove redundant if-condition and improve type correctness for `convert_tokens_to_ids` (#34030)
Add code sample docstrings and checkpoint reference for GLM models (#34360)
Fix pil_torch_interpolation_mapping import in image_processing_detr_fast (#34375)
Add T5 GGUF loading support (#33389)
add code generation to natural language processing section (#34333)
Zamba is an LM (#34342)
CI: fix failures (#34371)
translated gguf.md into chinese (#34163)
v4.47.0.dev0
Drop support for Python 3.8 (#34314)
Better defaults (#34026)
Remove graph breaks for torch.compile() in flash_attention_forward when Lllama Model is padding free tuned (#33932)
Add SynthID (watermerking by Google DeepMind) (#34350)
Fix red CI: benchmark script (#34351)
skip `test_pipeline_depth_estimation` temporarily (#34316)
Enable Gradient Accumulation fix across all models + trainer fully in forward() (#34283)
Support boolean tool args (#34208)
Added Deberta model type support (#34308)
[docs] Fix Korean toctree (#34324)
Example doc for token classification of Llama and Dependent/Copied Models (#34139)
üåê [i18n-KO] Translated `model_doc/bartpho.md` to Korean (#33981)
üåê [i18n-KO] Translated `bert japanese.md` to Korean (#33890)
üåê [i18n-KO] Translated `executorch.md` to Korean (#33888)
[docs] fix typo  (#34235)
fix error in _get_eval_sampler when group_by_length enabled (#34237)
Fix continue_final_message for image-text-to-text chat templates (#34236)
Feature: Add `MLFLOW_MAX_LOG_PARAMS` to `MLflowCallback` (#34279)
Add option for running ffmpeg_microphone_live as a background process (#32838)
Olmo is ExecuTorch Compatible (#34181)
Qwen2.5 is ExecuTorch Compatible (#34102)
Add post_process_depth_estimation to image processors and support ZoeDepth's inference intricacies (#32550)
Fix: tensor of examples of the same length triggers invalid stacking (#34166)
Fix FA2 attention for models supporting sliding window (#34093)
[RT-DETR] Fix onnx inference bug for Optype (Where) (#33877)
Update PR templates (#34065)
Sync video classification pipeline with huggingface_hub spec (#34288)
Fix Korean doc _toctree.yml (#34293)
[docs] Fix GenerationConfig params (#34299)
T5 compile compatibilty (#34089)
VLM: add more modularity (#34175)
Attn implementation for composite models (#32238)
Fix method name which changes in tutorial (#34252)
Add a doc section on writing generation prompts (#34248)
Add DetrImageProcessorFast (#34063)
Change Paligemma import logging to work with modular  (#34211)
Generation tests: don't rely on main input name (#34228)
Only cast logits to float when computing loss (#34147)
Fix UDOP dtype issue (#34180)
add Glm (#33823)
Informative 2 (#34154)
Fix broken test decorator `require_torch_up_to_2_accelerators` (#34201)
BLIP: fix input expansion logic (#34225)
Fix-red-ci (#34230)
Enable users to use their own loss functions + deal with prefetching for grad accum (#34198)
Support Llama 3.2 conversion (text models) (#33778)
Fix Gradient Accumulation issue (#34191)
Generate: visit non-llm `prepare_inputs_for_generation` (#34199)
Fix bus error when using GPT2 on M1 macs (#34031)
Llama3 and Llama2 are ExecuTorch compatible (#34101)
removes decord  (#33987)
Fix for tokenizer.apply_chat_template with continue_final_message=True (#34214)
fix(Wav2Vec2ForCTC): torch export (#34023)
Ping team members for new failed tests in daily CI (#34171)
Fix warning message for fp32_cpu_offloading in bitsandbytes configs (#34079)
Update `trainer._get_eval_sampler()` to support `group_by_length` arg (#33514)
Revert "Fix FSDP resume Initialization issue" (#34193)
Avoid using torch's Tensor or PIL's Image in chat template utils if not available (#34165)
Fix wrong name for llava onevision and qwen2_vl in tokenization auto (#34177)
Revert `accelerate` error caused by `46d09af` (#34197)
[fix] fix token healing tests and usage errors (#33931)
Moshi integration (#33624)
IDEFICS: support inputs embeds (#34043)
üåê [i18n-KO] Translated `blip-2.md` to Korean (#33516)
üåê [i18n-KO] Translated `trainer_utils.md` to Korean (#33817)
üåê [i18n-KO] Translated `gemma2.md` to Korean (#33937)
üåê [i18n-KO] Translated `vivit.md` to Korean (#33935)
[feat] LlavaNext add feature size check to avoid CUDA Runtime Error (#33608)
Fix optuna ddp hp search (#34073)
Add support for inheritance from class with different suffix in modular (#34077)
Generate: move `logits` to same device as `input_ids` (#34076)
Fix default behaviour in TextClassificationPipeline for regression problem type (#34066)
Fix FSDP resume Initialization issue (#34032)
 Add sdpa for Vivit (#33757)
Idefics: enable generation tests (#34062)
Update README.md with Enterprise Hub (#34150)
Add documentation for docker (#33156)
Specify that users should be careful with their own files (#34153)
Fixed error message in mllama (#34106)
Add GGUF for starcoder2 (#34094)
Fix a typo (#34148)
Mistral-related models for QnA (#34045)
Generate: Fix modern llm `generate` calls with `synced_gpus` (#34095)
fix(ci): benchmarks dashboard was failing due to missing quotations (#34100)
refactor: benchmarks (#33896)
Avoid many test failures for `LlavaNextVideoForConditionalGeneration` (#34070)
Generate: move `prepare_inputs_for_generation` in encoder-decoder llms (#34048)
Fix flaky tests (#34069)
Fix NaNs in cost_matrix for mask2former (#34074)
avoid many failures for ImageGPT (#34071)
Fix PushToHubMixin when pusing to a PR revision (#34090)
Fix failing conversion (#34010)
Fix DAC slow tests (#34088)
Fix flax failures (#33912)
Tests: upcast `logits` to `float()` (#34042)
Update SSH workflow file (#34084)
Idefics: fix position ids (#33907)
Generate using exported model and enable gemma2-2b in ExecuTorch (#33707)
Default `synced_gpus` to `True` when using `FullyShardedDataParallel` (#33483)
Small Fix to modular converter (#34051)
provide trust_remote_code for search feat extractor in model config (#34036)
Update Blip2 `is_pipeline_test_to_skip` method signature (#34067)
[TESTS] ASR pipeline (#33925)
Fix data_seed unused (#33731)
[Docs] Update compressed_tensors.md (#33961)
check if eigenvalues of covariance matrix are complex.  (#34037)
Universal Assisted Generation: Assisted generation with any assistant model (by Intel Labs) (#33383)
Specifying torch dtype in Qwen2VLForConditionalGeneration (#33953)
Sync QuestionAnsweringPipeline (#34039)
Add gguf support for gpt2 (#34044)
Fix pipelines tests (#34049)
HfArgumentParser: allow for hyhenated field names in long-options (#33990)
Phi3: fix attn for sliding window (#33586)
add sdpa to OPT (#33298)
Add Translate docs into Arabic - section files CONCEPTUAL GUIDES (#33982)
üåê [i18n-KO] Translated `generation_utils.md` to Korean (#33818)
üåê [i18n-KO] Translated `main_classes/callback.md` to Korean (#33572)
üåê [i18n-KO] Translated `text_generation.md` to Korean (#33777)
üåê [i18n-KO] Translated `model_doc/patchtst.md` to Korean (#33589)
üåê [i18n-KO] Translated `main_classes/data_collator.md` to Korean (#33954)
üåê [i18n-KO] Translated `modeling_utils.md` to Korean (#33808)
üåê [i18n-KO] Translated `model_doc/graphormer.md` to Korean (#33569)
üåê [i18n-KO] Translated `model_doc/informer.md` to Korean (#33585)
üåê [i18n-KO] Translated `model_doc/time_series_transformer.md` to Korean (#33596)
üåê [i18n-KO] Translated `model_doc/trajectory_transformer.md` to Korean (#33597)
üåê [i18n-KO] Translated `main_classes/model.md` to Korean (#33606)
üåê [i18n-KO] Translated `model_doc/mamba2.md` to Korean (#33629)
üåê [i18n-KO] Translated `main_classes/keras_callbacks.md` to Korean (#33955)
üåê [i18n-KO] Translated `model_doc/deberta.md` to Korean (#33967)
üåê [i18n-KO] Translated `model_doc/bart.md` to Korean (#33893)
FEAT : Adding BitNet quantization method to HFQuantizer (#33410)
Make `pipeline` able to load `processor` (#32514)
Fix PIL dep for tests (#34028)
Mllama: fix tests (#34000)
Generate: remove most decoder-only LLMs `prepare_inputs_for_generation` (#33870)
Fix Failed tests with mobile bert resize tokens embedding (#33950)
Add gguf support for StableLM (#33793)
[`Patch helper`] update to not have to checkout main (#34006)
üåê [i18n-KO] Translated `modular_transformers.md` to Korean (#33772)
üåê [i18n-KO] Translated `image_processing_utils.md` to Korean (#33804)
üåê [i18n-KO] Translated output.md to Korean (#33607)
üåê [i18n-KO] Translated `blip.md` to Korean (#33515)
üåê [i18n-KO] Translated `biogpt.md` to Korean (#33773)
üåê [i18n-KO] Translated `openai-gpt.md` to Korean (#33801)
üåê [i18n-KO] Translated `file_utils.md` to Korean (#33803)
üåê [i18n-KO] Translated `swin.md` to Korean (#33510)
üåê [i18n-KO] Translated `tokenization_utils.md` to Korean (#33813)
üåê [i18n-KO] Translated `main_classes/onnx.md` to Korean (#33601)
üåê [i18n-KO] Translated `model_doc/deberta-v2.md` to Korean (#33968)
üåê [i18n-KO] Translated `model_doc/dbrx.md` to Korean  (#33951)
üåê [i18n-KO] Translated `model_doc/cohere.md` to Korean (#33885)
üåê [i18n-KO] Translated `model_doc/mistral.md` to Korean (#33648)
üåê [i18n-KO] Translated `model_doc/llama3.md` to Korean (#33635)
üåê [i18n-KO] Translated `model_doc/paligemma.md` to Korean (#33612)
üåê [i18n-KO] Translated `model_doc/clip.md` to Korean (#33610)
üåê [i18n-KO] Translated `model_doc/patchtsmixer.md` to Korean (#33587)
üåê [i18n-KO] Translated `model_doc/autoformer.md` to Korean (#33574)
üåê [i18n-KO] Translated `model_doc/mamba.md` to Korean (#33626)
üåê [i18n-KO] Translated `main_classes/configuration.md` to Korean  (#33952)
üåê [i18n-KO] Translated `main_classes/quantization.md` to Korean (#33959)
üåê [i18n-KO] Translated `rag.md` to Korean (#33989)
üåê [i18n-KO] Translated `gpt_neox_japanese.md` to Korean (#33894)
üåê [i18n-KO] Translated `bertweet.md` to Korean (#33891)
üåê [i18n-KO] Translated `feature_extractor.md` to Korean (#33775)
Fix `trainer_seq2seq.py`'s `__init__` type annotations (#34021)
Remove `decoder_config=None` (#34014)
fix awq tests due to ipex backend (#34011)
Fix typing issue (#34012)
Fixup DeepSpeed things (#34007)
Improve modular converter (#33991)
BatchFeature.to() supports non-tensor keys (#33918)
Image pipelines spec compliance (#33899)
Add auto model for image-text-to-text (#32472)
Processors: don't default padding side (#33942)
Add support for __all__ and potentilly deleting functions (#33859)
Cache: slight change in naming (#32421)
üåê [i18n-KO] Translated `gemma.md` to Korean (#33936)
üåê [i18n-KO] Translated `vit.md` to Korean (#33884)
üåê [i18n-KO] Translated `swin2sr.md` to Korean (#33795)
üåê [i18n-KO] Translated `auto.md` to Korean (#33590)
üåê [i18n-KO] Translated `logging.md` to Korean (#33543)
üåê [i18n-KO] Translated `chameleon.md` to Korean (#33799)
üåê [i18n-KO] Translated `trainer.md` to Korean (#33797)
üåê [i18n-KO] Translated `pipelines_utils.md` to Korean (#33809)
üåê [i18n-KO] Translated `time_series_utils.md` to Korean (#33806)
üåê [i18n-KO] Translated `esm.md` to Korean (#33796)
üåê [i18n-KO] Translated `audio_utils.md` to Korean (#33802)
üåê [i18n-KO] Translated `swinv2.md` to Korean (#33566)
üåê [i18n-KO] Translated `gguf.md` to Korean (#33764)
Fix undefined default_config in configuration_utils.py (#33934)
[`pytes collection`] Fix flax test collection (#34004)
Enable customized optimizer for DeepSpeed (#32049)
properly fix and RUN_SLOW (#33965)
Fix Tensor + Embedding error in some cases when using SiglipVisionModel (#33994)
[`Red CIs`] Fix hub failures (#34001)
[Docs] Add Developer Guide: How to Hack Any Transformers Model (#33979)
[Docs] Improve VLM docs (#33393)
Flash-attn performance: remove cuda sync during inference (#33570)
Add position ids in forward pass to opt model (#33121)
[WIP] Add Tokenizer for MyT5 Model (#31286)
[`TF`] Fix Tensorflow XLA Generation on limited seq_len models (#33903)
Bug fix gguf qwen2moe (#33940)
add test for Jamba with new model jamba-tiny-dev (#33863)
Updating `char_to_token` documentation to note behaviour when `trim_offsets` is True (#33919)
Paligemma: fix static cache test (#33941)
Cache: revert DynamicCache init for BC (#33861)
fix red check-copies (#33964)
Add Zamba (#30950)
PhiMoE (#33363)
hot fix `self.position_embeddings->self.position_embedding` (#33958)
Fix attn mask ignore logic in training-time trace (#32613)
Removed unnecessary transpose in Switch Transformer Routing (#33582)
üî¥ üö®  Resizing tokens embeddings: initialize from old embeddings' normal distribution. (#33325)
Enables CPU AWQ model with IPEX version. (#33460)
Add a section on writing tool templates to the chat template docs (#33924)
[`PR run-slow`]  (#33939)
Ignore keys on `validate_rope` (#33753)
[i18n-ru] Fixes typo in the README_ru.md (#33882)
[Doc]: Broken link in Kubernetes doc (#33879)
Fix distil whisper segment computation (#33920)
Minor error condition bug fix (#33781)
Remove `logits.float()` (#33902)
Uniformize kwargs for Idefics/2 processors (#32568)
Config: lower `save_pretrained` exception to warning (#33906)
Add support for `weights_only` flag when loading state_dict (#32481)
add setter for trainer processor (#33911)
[PEFT] Support low_cpu_mem_usage option for PEFT loading adapters (#33725)
[Tests] Diverse Whisper fixes (#33665)
Fix: use unidic-lite instead of ipadic as the tokenizer dictionary for Japanese (#33372)
Generate tests: modality-agnostic input preparation (#33685)
Add `SplinterTokenizer` unit test (#32652)
Fix module initialization for root module under Zero3 (#33632)
Migrate the CI runners to the new clusters (#33849)
VLM Generate: tag `test_static_cache_matches_dynamic` as flaky (#33630)
Update an keyerror on _save_check_point prevent confusion of missing ‚Ä¶ (#33832)
Fix dt proj bias reassigned (#33314)
uniformize processor Mllama (#33876)
rename all test_processing_*.py to test_processor_*.py (#33878)
Handle Trainer `tokenizer` kwarg deprecation with decorator (#33887)
Optim deformable detr (#33600)
[Quantization] Switch to optimum-quanto  (#31732)
Trainer - deprecate tokenizer for processing_class (#32385)
Add sdpa for DistilBert (#33724)
Fix kwargs passed by AutoQuantizationConfig.from_pretrained (#33798)
Allow for nightly packages of `compressed_tensors` (#33828)
Add falcon gguf (#33437)
populate quantization_config for kv-cache-scheme only configs (#33874)
Don't run reminder bot for now (#33883)
Uniformize model processors (#31368)
Fix: typo  (#33880)
Add support for custom inputs and batched inputs in ProcessorTesterMixin (#33711)
Repo consistency fix after #33339 (#33873)
[Fix] ViViT interpolate_pos_encoding (#33815)
Move weight initilization deformabledetr (#33339)
Make ASR pipeline compliant with Hub spec + add tests (#33769)
fix: repair depth estimation multiprocessing (#33759)
Avoid using context that is not accessable from external contributors (#33866)
Add include_loss_for_metrics (#33088)
Validate the eval dataset in advance. (#33743)
Raise `accelerate` dependency error in case of defaulting `low_cpu_mem_usage=True` (#33830)
This PR contains additional changes for #33143 (#33581)
Fix device mismatch errors (#33851)
Workaround for bark issue in pipelines (#33824)
add attention weight up-cast to float32 in chameleon (#33822)
fix: skip dropout in eval for flash_attn in various models (#33844)
Refactor image features selection in LlaVa (#33696)
Generate: move llama `prepare_inputs_for_generation` to `GenerationMixin` (#33677)
post reminder comment only once (#33848)
fix check for hidden size in text model for deepspeed zero3 auto entries (#33829)
Fix passing str dtype to static cache (#33741)
Fix Mamba slow path bug with dtype mismatch. (#32691)
Bump torch from 1.13.1 to 2.2.0 in /examples/research_projects/lxmert (#33821)
minor typo fix (#33784)
Fix link in gguf.md (#33768)
Fixes for issue #33763 in idefics2 model (#33766)
Fix ViT-MAE decoder interpolate (#33330)
[`modular`] fixes!  (#33820)
Add Slow CI reminder bot (#33506)
Hqq serialization (#33141)
Fix typo in documentation  (#33805)
Enable non-safetensor ser/deser for TorchAoConfig quantized model üî¥  (#33456)
Fix typing in `load_balancing_loss_func` function of `modeling_mixtral.py`. (#33641)
Make audio classification pipeline spec-compliant and add test (#33730)
Model addition timeline (#33762)
Cleanup return_text and return_full_text options in TextGenerationPipeline (#33542)
remove warning v2 (#33761)
Bump torch from 1.13.1 to 2.2.0 in /examples/flax/vision (#33748)
Add gguf support for bloom (#33473)
Paligemma support for multi-image (#33447)
Make siglip examples clearer and error free (#33667)
[`MllamaImageProcessing`] Update doc (#33747)
[`clean_up_tokenization_spaces`] Pl bart was failing, updating (#33735)
Doc and config mismatch for DeBERTa (#33713)
Update Albumentations Versions (#33704)
fix trainer tr_loss add error (#33651)
Fix modular model converter unable to generate Processor classes (#33737)
fix: add docstring for `image_size` in Convnextv2 config (#33734)
clean_up_tokenization_spaces=False if unset (#31938)
Generate: `can_generate()` recursive check (#33718)
Fix position embeddings singular/plural (#33678)
Fix docs and docstrings Omdet-Turbo (#33726)
fix: use correct var names for check_tokenizers script (#33702)
[`MllamaProcessor`] Update errors and API with multiple image (#33715)
Uniformize kwargs for chameleon processor (#32181)
Add Idefics 3! (#32473)
Dev release
adding positional encoder changes and tests (#32600)
Add MLLama (#33703)
Add OmDet-Turbo (#31843)
Corrected max number for bf16 in transformer/docs (#33658)
Add AdEMAMix optimizer (#33682)
Add SDPA support for M2M100 (#33309)
Fix Megatron-LM tokenizer path (#33344)
HFQuantizer implementation for compressed-tensors library (#31704)
fix code quality after merge
[Pixtral] Improve docs, rename model (#33491)
bump tokenizers, fix added tokens fast (#32535)
tests: fix pytorch tensor placement errors (#33485)
üö®üö® Setting default behavior of assisted decoding (#33657)
Uniformize kwargs for image-text-to-text processors (#32544)
Fix ByteLevel alphabet missing when Sequence pretokenizer is used (#33556)
Gemma2: fix config initialization (`cache_implementation`) (#33684)
Improve Error Messaging for Flash Attention 2 on CPU (#33655)
Generation tests: update imagegpt input name, remove unused functions (#33663)
Fixed docstring for cohere model regarding unavailability of prune_he‚Ä¶ (#33253)
Fix CIs post merging modular transformers (#33681)
Modular `transformers`: modularity and inheritance for new model additions (#33248)
uniformize git processor (#33668)
Fix error string after refactoring into get_chat_template (#33652)
Enable BNB multi-backend support (#31098)
Generation: deprecate `PreTrainedModel` inheriting from `GenerationMixin`  (#33203)
Uniformize kwargs for Udop processor and update docs (#33628)
Fix Llava conversion for LlavaQwen2ForCausalLM with Clip vision tower (#33613)
add back self.max_position_embeddings = config.max_position_embeddings (#33550)
handle dependency errors in check_imports (#33622)
Fix DPT /Dinov2 sdpa regression on main (#33660)
Clean up Unpack imports (#33631)
Sdpa dino v2 (#33403)
Pixtral update example checkpoint (#33633)
Granitemoe (#33207)
enable low-precision pipeline (#31625)
Fix typos (#33583)
Fix qwen2vl float16 inference bug (#33312)
Update daily ci to use new cluster (#33627)
Fix some missing tests in circleci (#33559)
Generate: assistant should sample when the main model samples (#33534)
Fix contrastive search to correctly handle input with padding (#33507)
Add support for args to ProcessorMixin for backward compatibility (#33479)
Fix missing test in `torch_job` (#33593)
VLM generate: tests can't generate image/video tokens (#33623)
Add sdpa for BioGpt (#33592)
Remove unnecessary CPM model tests (#33621)
Generate: remove flakyness in `test_generate_from_inputs_embeds_decoder_only` (#33602)
Update modeling_mamba2.py, fix pad size (#32599)
[tests] make more tests device-agnostic (#33580)
Allow CI could be run on private forked repositories (e.g. new model additions) (#33594)
Fix CircleCI nightly run (#33558)
Docs: add the ability to manually trigger jobs (#33598)
Fix Llama 3 TikToken conversion (#33538)
[tests] enable GemmaIntegrationTest on XPU  (#33555)
[tests] skip tests for xpu  (#33553)
Uniformize kwargs for Paligemma processor and update docs (#33571)
Cache: don't throw warnings on `gemma2` when instantiating a new cache (#33595)
[`Mamba2`] Move dt calculations to kernel (#33520)
change sequence_bias type of SequenceBiasLogitsProcessor to list, add‚Ä¶ (#33375)
Generate: check that `attention_mask` is 2D (#33575)
add uniform processors for altclip + chinese_clip (#31198)
fix tests with main revision and read token (#33560)
Cache: don't show warning in forward passes when `past_key_values` is None (#33541)
rag: fix CI (#33578)
VLMs: enable generation tests (#33533)
Load and save video-processor from separate folder (#33562)
Codec integration (#33565)
Fix bnb dequantization  (#33546)
Improve compiled RT-DETR inference speed  (#33412)
enforce original size to be a list (#33564)
Return attention mask in ASR pipeline to avoid warnings (#33509)
Pipeline: no side-effects on `model.config` and `model.generation_config` üî´  (#33480)
Added support for bfloat16 to zero-shot classification pipeline (#33554)
Fix tests in ASR pipeline (#33545)
fix the wandb logging issue (#33464)
[i18n-ur] Added README_ur.md file (#33461)
Fix missing head_dim in llama config from gguf model (#33526)
Chat template: save and load correctly for processors (#33462)
Fix for slow the bug tokenizer adding spaces to single id decodes (#32564)
Decorator for easier tool building (#33439)
Support LLaVa-OV-Chat (#33532)
fix patch_attention_mask incorrect setting which leads to the differe‚Ä¶ (#33499)
Add revision to trainer push_to_hub (#33482)
Uniformize kwargs for Pixtral processor (#33521)
Fix missing `sequences_scores` in the Whisper beam search output  (#32970)
fix to jamba config, asserting attention and expert offset (#33316)
CI Build image - move runners (#33530)
Add explicit example for RAG chat templating (#33503)
Update chameleon.md ‚Äî fix runtime type error (#33494)
idefics2 enable_input_require_grads not aligned with disable_input_re‚Ä¶ (#33194)
chore: migrate coverage cfg to pyproject.toml (#32650)
Fix number of patch check for different vision feature select strategy (#32494)
Fix parametrization-based weight norm (#33275)
Replace `accelerator.use_fp16` in examples (#33513)
Updated Trainer's liger-kernel integration to call correct patching API (#33502)
Fix: Qwen2-VL training on video datasets (#33307)
[Whisper test] Fix some failing tests (#33450)
[i18n-ar] Add File : `docs/source/ar/_toctree.yml`  (#32696)
`Agents, supercharged - Multi-agents, External tools, and more` docs typo fixed (#33478)
Uniformize kwargs for LLaVa processor and update docs (#32858)
Add keypoint-detection task guide (#33274)
Fix SSH workflow (#33451)
Cohere: update RoPE structure (#33408)
Add support for Pixtral (#33449)
chore: fix typo in comment in tokenization_utils_base.py (#33466)
Corrected `Agents and tools` documentation links typos (#33471)
Enable finetuning with torchao quantized model  (#33361)
Fix the initialization of the cache when we have multi gpu (#33303)
[Phi-3] Bug on stale kv cache  (#33129)
Mitigate a conflict when using sentencepiece (#33327)
Enable `padding_side` as call time kwargs (#33385)
add a callback hook right before the optimizer step (#33444)
Return image hidden states (#33426)
[docs] refine the doc for `train with a script` (#33423)
[whisper] Clarify error message when setting max_new_tokens (#33324)
Qwen2-VL: clean-up and add more tests (#33354)
Correct Whisper's beam search scores computation (#32336)
Allow send `SSH into runner` info. to DM (#33346)
Revive AMD scheduled CI (#33448)
Fix default revision for pipelines (#33395)
Clean-up deprecated code (#33446)
Fix flax whisper tokenizer bug (#33151)
Fix incomplete sentence in `Zero-shot object detection` documentation (#33430)
Docs - update formatting of llama3 model card (#33438)
Update stale.yml (#33434)
[docs] add the missing tokenizer when pushing models to huggingface hub (#33428)
[docs] add the missing huggingface hub username (#33431)
Fix: Cast prefetch_bucket_size to integer for deepspeed >= 0.15 (#33402)
Dynamic number of speculative tokens in order to accelerate speculative decoding (#33258)
Remove deprecated task in load_dataset (#33433)
Fix failing windows (#33436)
Fix `FbgemmFp8Linear` not preserving tensor shape (#33239)
use diff internal model in tests (#33387)
Make StaticCache configurable at model construct time (#32830)
Update WhisperTokenizer Doc: Timestamps and Previous Tokens Behaviour (#33390)
Bug Fix: Update hub.py to fix NoneType error (#33315)
Add support for GGUF Phi-3 (#31844)
fixed Mask2Former image processor segmentation maps handling (#33364)
VLM: fixes after refactor (#32907)
Import structure & first three model refactors (#31329)
Fix import of `FalconMambaForCausalLM` (#33381)
Remove repeated prepare_images in processor tests (#33163)
Adjust templates (#33384)
Compile compatibilty for decoder-only models (#32617)
Fixed Majority of the Typos in `transformers[en]` Documentation (#33350)
Add visit webpage tool (#33353)
schedulefree optimizers (#30079)
Fix quantized cache tests (#33351)
add sdpa mbart (#32033)
Update author for QLorA/PEFT community notebook (#33338)
Fix Prefill docs (#33352)
RoPE: fix BC warning (#33331)
red-ci on main, fix copies (#33356)
Support reading tiktoken tokenizer.model file (#31656)
support 3D attention mask in bert (#32105)
add self.head_dim for VisionAttention in Qwen2-VL (#33211)
Add validation for maximum sequence length in modeling_whisper.py (#33196)
support loading model without config.json file (#32356)
Load dynamic module (remote code) only once if code isn't change (#33162)
fix qwen2vl vision eager-attention (#33213)
[whisper] alternative fix for long-form timestamps (#32131)
Docs: add more cross-references to the KV cache docs (#33323)
Fix: StaticCache & `inputs_embeds` (#32932)
Add a community notebook for fine-tuning with QLoRA, PEFT, and MLflow (#33319)
simple align qwen2vl kv_seq_len calculation with qwen2 (#33161)
Add Qwen2Moe GGUF loading support  (#33264)
Update SECURITY.md (#32680)
üö® Fix `torch.jit.trace` for `interpolate_pos_encoding` in all vision models (#33226)
Add paper link (#33305)
Fix: Fix `FalconMamba` training issues due to incompatible kernels (#33195)
Llava Onevision: add model (#32673)
Add validate images and text inputs order util for processors and test_processing_utils (#33285)
Fix excessive CPU memory usage with FSDP and cpu_ram_efficient_loading (#33154)
[BUG] fix upper nltk version (#33301)
Add new documentation page for advanced agent usage (#33265)
Add a warning to the chat template docs about the tool_calls format (#33277)
Multi agents with manager (#32687)
[InstructBLIP] qformer_tokenizer is required input (#33222)
Bump cryptography from 42.0.0 to 43.0.1 in /examples/research_projects/decision_transformer (#33286)
Bugfix/alexsherstinsky/fix none check for attention factor in rope scaling 2024 08 28 0 (#33188)
wait 15m before SSH into runner workflow stops (#33300)
[fix] LlavaNextProcessor '_get_unpadded_features' method (#33263)
Config: unified logic to retrieve text config (#33219)
Cache docs: update (#32929)
Fix: multigpu training (#33271)
Add OLMoE (#32406)
Repo checks: check documented methods exist (#32320)
fix the parallel number of CI nodes when it is smaller than number of tests (#33276)
Only disallow DeepSpeed Zero-3 for auto bs finder (#31731)
Add sdpa support for Albert (#32092)
Bump opencv-python from 4.4.0.42 to 4.8.1.78 in /examples/research_projects/visual_bert (#33251)
Update chat template docs to remove Blenderbot (#33254)
üö® Support dequantization for most GGML types (#32625)
Fix Bark saving (#33266)
Fix: `num_logits_to_keep` in composite models (#33168)
remove torch input dependant control flow (#33245)
Fix: use `torch.from_numpy()` to create tensors for np.ndarrays (#33201)
Fixed typo repeated word in DETR docs (#33250)
remove to restriction for 4-bit model (#33122)
Generate: fix assistant in different device (#33257)
Add assistant prefill for chat templates and TextGenerationPipeline (#33198)
Bump opencv-python from 4.4.0.42 to 4.8.1.78 in /examples/research_projects/lxmert (#33227)
docs: Replace package abbreviations with full name(`bitsandbytes`) in docstrings (#33230)
Fix: Suppressed 'use_reentrant=False' warning (#33208)
Add duckduckgo search tool (#32882)
Add GraniteRMSNorm (#33177)
Add video text to text docs (#33164)
Generate: throw warning when `return_dict_in_generate` is False but should be True (#33146)
Test fetcher: missing return on filtered tests; don't write empty files (#33224)
Fix red amin (#33220)
üåê [i18n-KO] Translated `llm_optims.md` to Korean (#32325)
Create local Transformers Engine (#33218)
Refactor CI: more explicit (#30674)
Fix local repos with remote code not registering for pipelines (#33100)
Add warning for stop string edge case (#33169)
Add missing quotes in modeling_llava_next_video.py (#33214)
Bump torch from 1.13.1 to 2.2.0 in /examples/research_projects/decision_transformer (#33215)
Bump torch from 1.13.1 to 2.2.0 in /examples/research_projects/codeparrot (#33173)
Pipeline: fix bad generation kwargs docs (#33205)
use a single for loop (#33148)
Add a static cache that offloads to the CPU or other device (#32161)
Mamba2 conversion script for original models (#32580)
pass module to Params4bit.from_prequantized to ensure quant_state (#32524)
added quick clarification (#33166)
update push CI workflow files for security (#33142)
Fix spell mistakes (#33149)
Customise the separator used for splicing in DataCollatorWithFlattening (#33114)
Zero-shot pipelines: minor doc changes (#33127)
Fix import paths for test_module (#32888)
[RoBERTa-based] Add support for sdpa (#30510)
[whisper] pass attention_mask to generate_with_fallback() (#33145)
Fix: Jamba batched generation (#32914)
fix model name and copyright (#33152)
Granite language models (#31502)
üö® Add Blip2ForImageTextRetrieval (#29261)
Very small change to one of the function parameters (#32548)
üåê [i18n-KO] Translated `conversations.md` to Korean (#32468)
update torch req for 4-bit optimizer (#33144)
fix redundant checkpointing in example training scripts (#33131)
Llama: make slow tests green üü¢  (#33138)
Add a fix for custom code tokenizers in pipelines (#32300)
fix Idefics2VisionConfig type annotation (#33103)
Update stateful_callbacks state before saving checkpoint (#32115)
[docs] add quick usage snippet to Whisper. (#31289)
Log additional test metrics with the CometCallback (#33124)
Bump torch from 1.13.1 to 2.2.0 in /examples/research_projects/jax-projects/hybrid_clip (#33137)
CI: fix `efficientnet` pipeline timeout and prevent future similar issues due to large image size (#33123)
disable scheduled daily CI temporarily (#33136)
fix: multilingual midel convert to tflite get wrong token (#32079)
fix: Fixed CodeGenTokenizationTest::test_truncation failing test (#32850)
Fixup py 38 type hints for mps friendly (#33128)
quickfix documentation (#32566)
fix: Fixed `pydantic` required version in dockerfiles to make it compatible with DeepSpeed (#33105)
Add changes for uroman package to handle non-Roman characters (#32404)
mps: add `isin_mps_friendly`, a wrapper function for `torch.isin` (#33099)
Test: add higher `atol` in `test_forward_with_num_logits_to_keep` (#33093)
CI: add torchvision to the consistency image (#32941)
support qwen2-vl (#32318)
Updated the custom_models.md changed cross_entropy code (#33118)
Update Jinja docs with new functions and general cleanup (#33097)
added doctring to SchedulerType class (#32898)
DeviceGuard added to use Deformable Attention more safely on multi-GPU (#32910)
Enable some Jinja extensions and add datetime capabilities (#32684)
Integrate Liger (Linkedin GPU Efficient Runtime) Kernel to Trainer (#32860)
Forbid `PretrainedConfig` from saving `generate` parameters; Update deprecations in `generate`-related code üßπ  (#32659)
Reducing memory usage: removing useless logits computation in generate() (#31292)
docs: fix outdated link to TF32 explanation (#32947)
Generate: Deprecate returning legacy cache by default; Handle `use_cache=False` (#32863)
üåê [i18n-KO] Translated `knowledge_distillation_for_image_classification.md to Korean"  (#32334)
Fix regression on `Processor.save_pretrained` caused by #31691 (#32921)
[run_slow] idefics2 (#32840)
Gemma2: eager attention by default (#32865)
fix: (issue #32689) `AttributeError` raised when using `Trainer` with `eval_on_start=True` in Jupyter Notebook. (#32849)
Add chat_template for tokenizer extracted from GGUF model (#32908)
Improve greedy search memory usage (#32895)
Fix benchmark script (#32635)
Add SynCode to llm_tutorial (#32884)
FIX / Hub: Also catch for `exceptions.ConnectionError` (#31469)
CI: separate step to download nltk files (#32935)
FEAT / Trainer: Add adamw 4bit optimizer (#31865)
fix: no need to dtype A in jamba (#32924)
fix: Added missing `huggingface_hub` installation to workflows (#32891)
Jamba: update integration tests (#32250)
Update docker image building (#32918)
fix: [whisper] don't overwrite GenerationConfig's `return_timestamps` when `return_timestamps` is not passed to `generate` function (#31296)
[i18n-ar] add README_ar.md to README.md (#32583)
link for optimizer names (#32400)
Replace `tensor.norm()` with decomposed version for CLIP executorch export (#32887)
Bump nltk from 3.7 to 3.9 in /examples/research_projects/decision_transformer (#32903)
Fix: Mamba2 `norm_before_gate` usage (#32686)
fix: jamba cache fails to use torch.nn.module (#32894)
Fix repr for conv (#32897)
üö®üö®üö® Update min version of accelerate to 0.26.0 (#32627)
Allow-head-dim (#32857)
Add tip to clarify tool calling (#32883)
Docs: Fixed `whisper-large-v2` model link in docs (#32871)
Fix: Mamba2 generation mismatch between input_ids and inputs_embeds (#32694)
Mamba / FalconMamba: Fix mamba left padding (#32677)
Fix incorrect vocab size retrieval in GGUF config (#32551)
RT-DETR parameterized batchnorm freezing (#32631)
Support save/load ckpt for XLA FSDP (#32311)
Add __repr__ for Conv1D (#32425)
[tests] make `test_sdpa_can_compile_dynamic` device-agnostic (#32519)
support torch-speech (#32537)
Add Descript-Audio-Codec model (#31494)
Add Flax Dinov2 (#31960)
generate: missing `to` in DoLa body, causing exceptions in multi-gpu generation (#32856)
Make beam_constraints.Constraint.advance() docstring more accurate (#32674)
Reduce the error log when using core models that need their weights renamed, and provide a step forward (#32656)
fix multi-gpu with static cache (#32543)
Revert PR 32299, flag users when Zero-3 was missed (#32851)
improve _get_is_as_tensor_fns (#32596)
Fix AutoConfig and AutoModel support for Llava-Next-Video (#32844)
Cache: use `batch_size` instead of `max_batch_size` (#32657)
[tests] make test_sdpa_equivalence device-agnostic (#32520)
Generate: unify `LogitsWarper` and `LogitsProcessor` (#32626)
Use head_dim if in config for RoPE (#32495)
add back the position ids (#32554)
VLMs: small clean-up for cache class (#32417)
fix: update doc link for runhouse in README.md (#32664)
fix: Corrected ` falcon-mamba-7b` model checkpoint name (#32837)
reopen: llava-next fails to consider padding_side during Training (#32679)
Updated workflows to the latest versions (#32405)
Unpin deepspeed in Docker image/tests (#32572)
fix: Fixed unknown pytest config option `doctest_glob` (#32475)
Update the distributed CPU training on Kubernetes documentation (#32669)
Fix `JetMoeIntegrationTest` (#32332)
Add TorchAOHfQuantizer (#32306)
Update translation docs review (#32662)
fix: Fixed failing tests in `tests/utils/test_add_new_model_like.py` (#32678)
Support MUSA (Moore Threads GPU) backend in transformers (#31913)
Fix tests recurrent (#32651)
TF_Deberta supporting mixed precision (#32618)
Modify ProcessorTesterMixin for better generalization (#32637)
Fix: Fixed directory path for utils folder in `test_tokenization_utils.py` (#32601)
Add Depth Anything V2 Metric models (#32126)
Add support for GrokAdamW optimizer (#32521)
fix tensors on different devices in `WhisperGenerationMixin` (#32316)
Fix tests (#32649)
Automatically add `transformers` tag to the modelcard (#32623)
Expand inputs in processors for VLMs (#30962)
fix: Updated the `is_torch_mps_available()` function to include `min_version` argument (#32545)
"to be not" -> "not to be" (#32636)
Bump tensorflow from 2.11.1 to 2.12.1 in /examples/research_projects/decision_transformer (#32341)
fix: Fixed failing `test_find_base_model_checkpoint` (#32638)
üåê [i18n-KO] Translated `awq.md`to Korean (#32324)
üåê [i18n-KO] Translated `deepspeed.md` to Korean (#32431)
Cleanup tool calling documentation and rename doc (#32337)
Bump torch from 1.13.1 to 2.2.0 in /examples/research_projects/visual_bert (#32220)
Bump aiohttp from 3.9.4 to 3.10.2 in /examples/research_projects/decision_transformer (#32569)
Fix `.push_to_hub(..., create_pr=True, revision="my-branch")` when creating PR on not-owned repo (#32094)
fix: Fixed conditional check for `encodec` model names (#32581)
Fix sliding window attention used in Gemma2FlashAttention2 (#32522)
Fix: FA2 with packed training (#32487)
Add new model (#32615)
üåê [i18n-KO] Translated `agent.md` to Korean (#32351)
fix non contiguous tensor value error in save_pretrained (#32422)
fix slow integration gemma2 test (#32534)
Fix a bug in Qwen2Audio (#32552)
Gemma2: fix FA2 generation (#32553)
[docs] Translation guide (#32547)
Fix code example to load bigcode starcoder2 7b (#32474)
Fixed test `test_static_cache_exportability` with torch 2.4.0 (#32516)
Fix generate with `inputs_embeds` as input (#32493)
üåê [i18n-KO] Translated `bitsandbytes.md` to Korean (#32408)
üåê [i18n-KO] Translated `fsdp.md` to Korean (#32261)
üåê [i18n-KO] Translated `eetq.md` to Korean (#32352)
üåê [i18n-KO] Translated `trainer.md` to Korean (#32260)
üåê [i18n-KO] Translated `ko-llm_tutorial_optimization.md` to Korean (#32372)
filter flash_attn optional imports loading remote code (#30954)
Add Qwen2-Audio (#32137)
Fix add-new-model-like (#31773)
 Uniformize kwargs for processors - GroundingDINO (#31964)
Change Phi3 `_supports_sdpa` to True (#32457)
Fix issue #32518: Update llm_tutorial.md (#32523)
Fix typo: depracted -> deprecated (#32489)
Fix link to autoclass_tutorial.md in i18n.md (#32501)
üåê [i18n-KO] Translated `chat_templating.md` to Korean (#32362)
Docs: Fixed WhisperModel.forward‚Äôs docstring link (#32498)
Fix references to model google mt5 small (#32497)
üåê [i18n-KO] Translated `image_feature_extraction.md` to Korean (#32239)
üåê [i18n-KO] Translated `quantization/quanto.md` to Korean (#32281)
üåê [i18n-KO] Translated `prompting.md` to Korean (#32294)
üåê [i18n-KO] Translated `gptq.md` to Korean (#32293)
Docs: alert for the possibility of manipulating logits (#32467)
fix broken link in docs (#32491)
Agents use grammar (#31735)
Fix typo in tokenization_utils_base.py (#32484)
enable xla fsdp (#32048)
Gemma2: add cache warning (#32279)
Cache: new Cache format in decoder-only models (#31421)
üåê [i18n-KO] Translated `image_to_image.md` to Korean (#32327)
üåê [i18n-KO] Translated `idefics.md` to Korean (#32258)
üåê [i18n-KO] Translated `mask_generation.md` to Korean (#32257)
Revert "fixes to properly shard FSDP across cpu and meta for cpu_effcient_loading for prequantized 4bit (#32276)" (#32477)
`is_torchdynamo_compiling` -- cast a wide exception net (#32476)
dev version 4.45.0
Documentation: BOS token_id deprecation change for NLLB (#32443)
Migrate import checks not need accelerate, and be more clear on min versions (#32292)
Add codestral mamba2 (#32080)
Generate: fix end to end compilation (#32465)
Add Nemotron HF Support (#31699)
Dependencies: fix typo (#32389)
Fix get large model config for Switch Transformer encoder only tester (#32438)
Update kwargs validation for `preprocess` with decorator (#32024)
add the missing flash attention test marker (#32419)
Llava: fix checkpoint_doc (#32458)
Cache: create docs (#32150)
Fix documentation links and code reference to model llava-next (#32434)
Respect the config's attn_implementation if set (#32383)
fix: Updated `test_embeded_special_tokens` for luke and mluke models (#32413)
Persist embedding type of BART and mBART models after resize (#32242)
Fix documentation references to google/bit-50 model (#32407)
add values for neftune (#32399)
#32184 save total_vocab_size (#32240)
Phi3 tests: fix typing for Python 3.8 (#32388)
fix: SeamlessM4TFeatureExtractor stride remainder (#32088)
Bump keras from 2.8.0 to 2.13.1 in /examples/research_projects/decision_transformer (#32393)
MixtralFlashAttention2: put "plus 1" inside parentheses when calculating rotary_seq_len, allowing None position_ids input. (#31500)
fix: (issue #32124) Exception raised when running `transformers/examples/flax/language-modeling/t5_tokenizer_model.py`. (#32157)
[generate] only require an attention mask for mps with torch<2.4 (#32367)
RoPE: Add numerical tests ‚ú®  (#32380)
Update docs (#32368)
Yell at the user if zero-3 init wasn't performed, but expected to have been done (#32299)
Fixed Hybrid Cache Shape Initialization. (#32163)
Docker: add `speech` dep to the consistency docker image (#32374)
Offloaded KV Cache (#31325)
Fix conflicting key in init kwargs in PreTrainedTokenizerBase (#31233)
Empty list in defaults for LLaMA special tokens during weights conversion (#32342)
update clean_up_tokenization_spaces warning (#32371)
Check device map for saving tokenizer config on TPU (fix for issue #31971) (#32043)
add missing attribute _supports_param_buffer_assignment for gpt-j. (#32359)
Remove size check between attn_weights and kv_seq_len for phi3 (#32339)
[whisper] compile compatibility with long-form decoding (#31772)
[enc-dec cache] fix bug in indexing (#32370)
LLaVa: add cache class attribute (#32278)
fix: warmup_steps check for training_args (#32236)
fix: Removed unnecessary `@staticmethod` decorator (#32361)
>3-5x faster torch.compile forward compilation for autoregressive decoder models (#32227)
Fix error when streaming to gradio with non-string tool arguments (#32360)
Gemma 2: support assisted generation (#32357)
[Idefics2] - Fix FA2 call for Perceiver layer (#32275)
Llama 3.1: Fix incorrect `inv_freq` assignment (#32330)
Gemma2 and flash-attention (#32188)
LLaVA-NeXT: fix anyres shapes (#32314)
Fix slow GemmaTokenizer and improve SPM slow -> fast conversion process (#32191)
Repo checks: skip docstring checks if not in the diff (#32328)
fixes #32329 : The Torch code is correct - to get an average of 10% o‚Ä¶ (#32335)
fixes to properly shard FSDP across cpu and meta for cpu_efficient_loading for prequantized 4bit (#32276)
fix: Added missing raise keyword for few exceptions (#32333)
Alternative agent plan (#32295)
Docs: formatting nits (#32247)
Fix M4T for ASR pipeline (#32296)
feat(ci): set `fetch-depth: 0` in trufflehog checkout step (#31663)
Cast epochs_trained to int when resuming training (#32286)
Fix GGUF dequantize for `gguf==0.9.1` (#32298)
Docs: fix GaLore optimizer code example (#32249)
use torch 2.4 in 2 CI jobs (#32302)
Add stream messages from agent run for gradio chatbot (#32142)
Make static cache compatible with torch.export (#32168)
[pipeline] fix padding for 1-d tensors (#31776)
Whisper tokenizer word level timestamps (#32197)
Generate: end-to-end compilation (#30788)
fix(docs): Fixed a link in docs (#32274)
make `p_mask` a numpy array before passing to `select_starts_ends` (#32076)
Repo: remove exceptions in `check_docstrings` (#32259)
fix: Fixed wrong argument passed to `convert_blip_checkpoint` function call (#32262)
Optimize t5 tokenize logic to avoid redundant calls (#32270)
Upload new model failure report to Hub (#32264)
üö® Bloom support for cache class (#31445)
Llama 3.1: replace for loop by tensor ops at inv_freq initialization (#32244)
More flexible trigger condition (#32251)
Flash-Attn: fix generation when no attention mask or no pading (#32241)
[tests] fix `static` cache implementation is not compatible with `attn_implementation==flash_attention_2` (#32039)
Add check for `target_sizes is None` in `post_process_image_guided_detection` for owlv2 (#31934)
Adds: extra_repr for RMSNorm layers in most models (#32204)
Refactor: Removed un-necessary `object` base class (#32230)
don't log base model architecture in wandb if log model is false (#32143)
Resize embeds with DeepSpeed  (#32214)
Llava: generate without images (#32183)
Generation: stop at `eos` for assisted decoding (#31301)
Fix code snippet for Grounding DINO (#32229)
Allow a specific microphone to be used by the ffmpeg audio pipeline utility functions. Default to using the currently active microphone on Mac (#31846)
translate philosophy.md to chinese (#32177)
Follow up for #31973 (#32025)
[warnings] fix E721 warnings (#32223)
[BigBird Pegasus] set _supports_param_buffer_assignment to False (#32222)
Update question_answering.py (#32208)
remove unnecessary guard code related with pytorch versions 1.4.2 ~ 1.7.0 (#32210)
[whisper] fix short-form output type (#32178)
fix: Replaced deprecated `unittest method` with the correct one (#32198)
:rotating_light: No more default chat templates (#31733)
Support dequantizing GGUF FP16 format (#31783)
Fix float8_e4m3fn in modeling_utils (#32193)
Fix resize embedding with Deepspeed (#32192)
let's not warn when someone is running a forward  (#32176)
RoPE: relaxed rope validation (#32182)
Remove conversational pipeline tests (#32099)
Update qwen2.md (#32108)
fix: default value reflects the runtime environment variables rather than the ones present at import time. (#32153)
adds: extra_repr() to MambaRMSNorm to include hidden size / size of weights in the layer (#32171)
[docs] change temperature to a positive value (#32077)
fix: Fixed an if condition that is always evaluating to true (#32160)
fix (#32162)
Llama 3.1 conversion
Dev version: v4.44.0.dev0
Updated `ruff` to the latest version (#31926)
Enhancing SFT Training Efficiency Using Packing and FlashAttention2 with Position IDs (#31629)
Added additional kwarg for successful running of optuna hyperparameter search (#31924)
feat(cache): StaticCache uses index_copy_ to avoid useless copy (#31857)
Fix typing to be compatible with later py versions (#32155)
Revert "Incorrect Whisper long-form decoding timestamps " (#32148)
Rename Phi-3 rope scaling type (#31436)
Added mamba.py backend (#30139)
Fix video batching to videollava (#32139)
Fix flash attention speed issue (#32028)
gguf conversion add_prefix_space=None for llama3 (#31937)
Llama: RoPE refactor (#32135)
Modify resize_token_embeddings to ensure output type is same as input (#31979)
Disable quick init for TapasPreTrainedModel (#32149)
Add YaRN and Dynamic-YaRN RoPE Scaling Methods (#30910)
Add method to retrieve used chat template (#32032)
Fix mask creations of `GPTNeoX` and `GPT2` (#31944)
[modelling] remove un-necessary transpose for fa2 attention (#31749)
Remove `trust_remote_code` when loading Libri Dummy (#31748)
LLaVaNeXT: pad on right if training (#32134)
Add llama3-llava-next-8b to llava_next conversion script (#31395)
Add new quant method (#32047)
set warning level to info for special tokens have been added (#32138)
Don't default to other weights file when use_safetensors=True (#31874)
Return assistant generated tokens mask in apply_chat_template  (#30650)
[RoBERTa] Minor clarifications to model doc (#31949)
fix: Fixed raising `TypeError` instead of `ValueError` for invalid type (#32111)
Update `ko/_toctree.yml` and remove `custom_tools.md` to reflect latest changes (#31969)
Fix failing test with race condition (#32140)
[generate] fix eos/pad id check on mps devices (#31695)
Mention model_info.id instead of model_info.modelId (#32106)
fix: Replaced deprecated `mktemp()` function (#32123)
Generate: store special token tensors under a unique variable name (#31980)
Fix shard order (#32023)
Agents planning (#31702)
Fix tests after `huggingface_hub` 0.24 (#32054)
Chameleon: not supported with fast load (#32091)
Disable quick init for deepspeed (#32066)
Support generating with fallback for short form audio in Whisper (#30984)
Add image-text-to-text task guide (#31777)
Fixes to chameleon docs (#32078)
Fix progress callback deepcopy (#32070)
VideoLLaVa: fix chat format in docs (#32083)
[mistral] Fix FA2 attention reshape for Mistral Nemo (#32065)
Incorrect Whisper long-form decoding timestamps  (#32003)
[Chameleon, Hiera] Improve docs (#32038)
Llava: add default chat templates (#31691)
docs: Fixed 2 links in the docs along with some minor fixes (#32058)
fix: Removed `duplicate entries` in a dictionary (#32041)
Add torch.compile Support For Mamba (#31247)
[mistral] Support passing `head_dim` through config (and do not require `head_dim * num_heads == hidden_size`) (#32050)
Bump scikit-learn from 1.1.2 to 1.5.0 in /examples/research_projects/codeparrot/examples (#32052)
Bump scikit-learn from 1.0.2 to 1.5.0 in /examples/research_projects/decision_transformer (#31458)
Chameleon: minor fixes after shipping (#32037)
unpin `numpy<2.0` (#32018)
Add `sdpa` and  FA2 for CLIP (#31940)
Add language to word timestamps for Whisper (#31572)
Pass missing arguments to `SeamlessM4Tv2ConformerEncoderLayer.forward()` when gradient checkpointing is enabled (#31945)
doc: fix broken BEiT and DiNAT model links on Backbone page (#32029)
Fix typo in classification function selection logic to improve code consistency (#32031)
Fixed `log messages` that are resulting in TypeError due to too many arguments (#32017)
Fix tests skip (#32012)
Chameleon: add model (#31534)
SpeechEncoderDecoder doesn't support param buffer assignments (#32009)
Fix if else and *actually* enable superfast init (#32007)
Fix gather when collecting 'num_input_tokens_seen' (#31974)
Bug report update -- round 2 (#32006)
fix: Fixed incorrect dictionary assignment in `src/transformers/__init__.py` (#31993)
add flash-attn deterministic option to flash-attn>=2.4.1 (#31961)
Bug report update (#31983)
Tests: remove cuda versions when the result is the same üßπüßπ (#31955)
Fix bad test about slower init (#32002)
[tests] fix deepspeed zero3 config for `test_stage3_nvme_offload` (#31881)
Speedup model init on CPU (by 10x+ for llama-3-8B as one example) (#31771)
Cambricon MLUs support SDPA and flash_attn (#31102)
Fix the incorrect permutation of gguf (#31788)
Generate: doc nits (#31982)
Masking: remove flakiness from test (#31939)
Avoid race condition (#31973)
Notify new docker images built for circleci (#31701)
fix: Fixed the arguments in `create_repo()` function call (#31947)
Generate: handle `logits_warper` update in models with custom generate fn (#31957)
fix: Removed a wrong key-word argument in `sigmoid_focal_loss()` function call (#31951)
Whisper: move to tensor cpu before converting to np array at decode time (#31954)
Generate: v4.42 deprecations üßπüßπ (#31956)
Generate: remove deprecated code due to `Cache` and `cache_position` being default (#31898)
Fix `GenerationMixin.generate` compatibility with pytorch profiler (#31935)
fix prompt strip to support tensors and np arrays (#27818)
Docker: TF pin on the consistency job (#31928)
[Bug Fix] fix qa pipeline tensor to numpy (#31585)
Adding hiera (#30356)
Allow `Trainer.get_optimizer_cls_and_kwargs` to be overridden (#31875)
üö® fix(SigLip): remove spurious exclusion of first vision output token (#30952)
Generate: fix `SlidingWindowCache.reset()` (#31917)
Refactor flash attention implementation in transformers (#31446)
Fix fx tests with inputs_embeds (#31862)
Add warning message for beta and gamma parameters (#31654)
add gather_use_object arguments II (#31799)
fix: Fixed the `1st argument` name in classmethods (#31907)
Fix missing methods for Fuyu (#31880)
[`Gemma2`] Support FA2 softcapping (#31887)
[`ConvertSlow`] make sure the order is preserved for addedtokens (#31902)
Processor accepts any kwargs (#31889)
Fixes to alternating SWA layers in Gemma2 (#31775)
InstructBlipVideo: Update docstring (#31886)
Add a condition for nested_detach (#31855)
Modify `warnings` in a `with` block to  avoid flaky tests (#31893)
[RT-DETR] Add resources (#31815)
Push sharded checkpoint to hub when `push_to_hub=True` in `TrainingArguments` (#31808)
fix: Removed `duplicate` field definitions in some classes (#31888)
Fix failed tests in #31851 (#31879)
Fix file type checks in data splits for contrastive training example script (#31720)
remove duplicate words in msg (#31876)
Add conversion for interleave llava (#31858)
add warning when using gradient_checkpointing with FSDP full shard (#31578)
Bump certifi from 2023.7.22 to 2024.7.4 in /examples/research_projects/visual_bert (#31872)
Revert "Fix `_init_weights` for `ResNetPreTrainedModel`" (#31868)
Add return type annotation to PreTrainedModel.from_pretrained (#31869)
Bump zipp from 3.7.0 to 3.19.1 in /examples/research_projects/decision_transformer (#31871)
Update depth estimation task guide (#31860)
Fix `_init_weights` for `ResNetPreTrainedModel` (#31851)
Generate: Add new decoding strategy "DoLa" in `.generate()` (#29619)
docs: typo in tf qa example (#31864)
Test loading generation config with safetensor weights (#31550)
save_pretrained: use tqdm when saving checkpoint shards from offloaded params (#31856)
chore: remove duplicate words (#31853)
[Grounding DINO] Add processor to auto mapping (#31845)
FX symbolic_trace: do not test decoder_inputs_embeds (#31840)
Deprecate `vocab_size` in other two VLMs (#31681)
Mamba & RecurrentGemma: enable strict signature (#31549)
Fix incorrect accelerator device handling for MPS in `TrainingArguments` (#31812)
Avoid failure `TFBlipModelTest::test_pipeline_image_to_text` (#31827)
transformers.fx.symbolic_trace supports inputs_embeds (#31574)
Fix typos (#31819)
Bump certifi from 2023.7.22 to 2024.7.4 in /examples/research_projects/lxmert (#31838)
Bump transformers from 4.26.1 to 4.38.0 in /examples/tensorflow/language-modeling-tpu (#31837)
Add FA2 and `sdpa` support for SigLIP (#31499)
Bump certifi from 2023.7.22 to 2024.7.4 in /examples/research_projects/decision_transformer (#31813)
Fix Seq2SeqTrainer crash when BatchEncoding data is None (#31418)
Add ZoeDepth (#30136)
Depth Anything: update conversion script for V2 (#31522)
Fix Wav2Vec2 Fairseq conversion (weight norm state dict keys) (#31714)
Fix galore lr display with schedulers (#31710)
Allow FP16 or other precision inference for Pipelines (#31342)
Repeating an important warning in the chat template docs (#31796)
Add training support for SigLIP (#31495)
Code agent: allow function persistence between steps (#31769)
Fix gemma tests (#31794)
Update CometCallback to allow reusing of the running experiment (#31366)
Exclude torch.compile time from metrics computation (#31443)
Make tensor device correct when ACCELERATE_TORCH_DEVICE is defined (#31751)
Fix serialization for offloaded model (#31727)
Fix ClapProcessor to merge feature_extractor output into the returned BatchEncoding (#31767)
Add torch_empty_cache_steps to TrainingArguments (#31546)
Fix Gemma2 types (#31779)
`pytest_num_workers=4` for some CircleCI jobs (#31764)
Fix RT-DETR weights initialization (#31724)
Fix RT-DETR cache for generate_anchors (#31671)
[fix bug] logits's shape different from label's shape in preprocess_logits_for_metrics (#31447)
Add ignore_errors=True to trainer.py rmtree in _inner_training_loop (#31668)
Gemma 2: Update slow tests (#31759)
handle (processor_class, None) returned by ModelPatterns (#31753)
Adds final answer tool for all agents (#31703)
Requires for torch.tensor before casting (#31755)
fix assisted decoding (#31401)
Fix documentation for Gemma2. (#31682)
Make tool JSON schemas consistent (#31756)
üö®üö® TextGenerationPipeline: rely on the tokenizer default kwargs (#31747)
[whisper] static kv cache (#31166)
Fix mistral ONNX export (#31696)
Move some test files (`tets/test_xxx_utils.py`) to `tests/utils` (#31730)
remove incorrect urls pointing to the llava repository (#31107)
dependencies: `keras-nlp<0.14` pin (#31684)
Add French version of run scripts tutorial (#31483)
Gemma capping is a must for big models (#31698)
add gather_use_object arguments (#31514)
Fix return_dict in encodec (#31646)
Fix Gemma2 4d attention mask (#31674)
don't zero out the attention_mask when using sliding window with flash attention (#31670)
[HybridCache] Fix `get_seq_length` method (#31661)
[docs] Llama3 (#31662)
Fix float out of range in owlvit and owlv2 when using FP16 or lower precision (#31657)
Fix post gemma merge (#31660)
v4.43.0.dev0
Add gemma 2 (#31659)
Remove deprecated config attribute in VLMs (#31655)
change anchor_image_size None for compatibility (#31640)
[QoL] Allow dtype str for torch_dtype arg of from_pretrained (#31590)
[`Llama`] Conversion: fix and simplify the script! (#31591)
Fix ONNX exports for Optimum compatible models (#31311)
Generation: past kv can be None (#31051)
Skip tests properly (#31308)
Fix dtype casting in swinv2 and swinv2sr to allow non-FP32 inference (#31589)
Generate: fix assisted generation with `past_key_values` passed as kwargs (#31644)
Fix paligemma detection inference (#31587)
Add LLaVa NeXT Video (#31252)
Fix RT-DETR inference with float16 and bfloat16 (#31639)
Llama et al. / FSDP : Fix breaking change in 4.40 for FSDP (#31161)
Update RT-DETR code snippet (#31631)
Fix llama gguf converter (#31575)
[`GPT-NeoX`] Add SDPA support (#31031)
Removed unnecessary `self.projection` call in `VivitTubeletEmbeddings` (#31632)
docs: move translations to `i18n` (#31584)
Add ViTImageProcessorFast to tests (#31424)
Improve error message for mismatched copies in code blocks  (#31535)
add preprocessing_num_workers to run_classification.py (#31586)
Add video modality for InstrucBLIP (#30182)
fix output data type of image classification (#31444)
Siglip: add `_no_split_module` (#31566)
Added version constraint on numpy for version <2.0 (#31569)
Fix is_torch_xpu_available for torch < 2.3 (#31573)
Fix doc typo in `TrainingArguments` (#31503)
Add Jinja as a requirement with the right version cutoff (#31536)
Fix bug about add_special_tokens and so on (#31496)
Fix the error caused by incorrect use of logger in pipeline (#31565)
Update git templates (#31539)
chore: fix typos (#31559)
Add implementation of `spectrogram_batch` (#27159)
Correct @is_flaky test decoration (#31480)
Update mask_generation.md (#31543)
New model support RTDETR (#29077)
Removed torch.cuda.empty_cache from train loop. (#31530)
SPLIT PR:  add user defined symbols and control symbols (#31305)
Deprecate legacy cache + use cache position (#31491)
Bump urllib3 from 1.26.18 to 1.26.19 in /examples/research_projects/lxmert (#31524)
Revive Nightly/Past CI (#31159)
unskip 2 tests in cohere (#31517)
RWKV: enable generation tests (#31490)
Fix mismatched ` in doc & other common typos (#31516)
GGUF: Fix llama 3 GGUF (#31358)
Fix a teeny-tiny typo in `tokenization_utils_base.py`'s docstring (#31510)
Add valid columns check in _remove_unused_columns method (#31466)
Consider inheritance in type checking for tensors (#31378)
Fix `wandb` integration with `SetFit` model (#30021)
Fix typo: pas_token_id (#30894)
auto-detect device when no device is passed to pipeline (#31398)
Add docs on zeroshot image classification prompt templates (#31343)
Update object_detection.md (#31488)
Mamba: add generative tests (#31478)
Docs  / AQLM: Clarify `torch.compile` support for AQLM (#31473)
[tests] rename `test_config_object` to `test_ds_config_object` (#31403)
Use self.config_tester.run_common_tests() (#31431)
Fix autocast incompatibility in RecurrentGemma (#30832)
[`GPT2`] Add SDPA support (#31172)
Update perf_train_gpu_many.md (#31451)
Give more useful `metric_for_best_model` errors (#31450)
Fix documentation typos (#31476)
Bump urllib3 from 1.26.18 to 1.26.19 in /examples/research_projects/visual_bert (#31472)
Improve `PreTrainedTokenizerFast` loading time when there are many added tokens (#31404)
Update chat template docs and bump Jinja version (#31455)
Fix single letter stop strings (#31448)
Make "tool_use" the default chat template key when tools are passed (#31429)
Donut: fix `generate` call from local path (#31470)
Bump urllib3 from 1.26.18 to 1.26.19 in /examples/research_projects/decision_transformer (#31459)
Agents: Improve python interpreter (#31409)
Fix typing errors in `Qwen2ForTokenClassification` (#31440)
simple fix (#31456)
üö® Remove dataset with restrictive license (#31452)
Pass datasets trust_remote_code (#31406)
Support multiple validation datasets when `dataloader_persistent_workers=True` (#30627)
Bump idna from 2.8 to 3.7 in /examples/research_projects/visual_bert (#30201)
[tests] make `TestDeepSpeedModelZoo` device-agnostic (#31402)
Bump idna from 2.8 to 3.7 in /examples/research_projects/lxmert (#30200)
Bump idna from 3.3 to 3.7 in /examples/research_projects/decision_transformer (#30203)
Generate: fix `tokenizer` being popped twice (#31427)
Rename misnamed image processor test files (#31430)
Fix Bark logits processors device misplacement (#31416)
Musicgen special tokens in tensors (#31420)
xpu: support xpu backend from stock pytorch (>=2.4) (#31238)
Remove empty create_and_test_config_common_properties tests (#31359)
Install the tensorflow example requirements in docker (#31428)
Remove duplicate image processor in auto map (#31383)
Change potential `inputs_embeds` padding `logger.warning` to `logger.warning_once` (#31411)
Fix SpeechT5 `decoder_attention_mask` shape (#28071)
Set seed for M4T retain grad test (#31419)
Fix MusicGen SDPA (#31208)
Pin datasets<2.20.0 for examples (#31417)
Support Clip QKV for MPT (#31307)
Temporarily pin datasets upper version to fix CI (#31407)
Add missing French translation of tutoriel_pipeline.md (#31396)
add initial design for uniform processors + align model (#31197)
Make chat templates part of ProcessorMixin (#30744)
[QoL fix] [Image processing] Add warning on assumption of channel dim and avoid infering when inputs are PIL.Image (#31364)
feat(ci): add trufflehog secrets detection (#31344)
Change JSON serialization to custom json.dumps (#31100)
Bump jupyter-core from 4.6.3 to 4.11.2 in /examples/research_projects/visual_bert (#31386)
Use huggingface_hub helper function to split state dict (#31091)
Update comment in modeling_utils.py (#31299)
README underline between badges fix  (#31376)
backbone_utils - fix relative import (#31382)
docs: fix broken link (#31370)
[Bug Fix] Renamed loss to losses to suppress UnboundLocalError (#31365)
Fix idefics cache (#31377)
Add support to declare imports for code agent (#31355)
Add french translation of AutoBackbone (#31300)
Fast image processor (#28847)
Chat Template support for function calling and RAG (#30621)
Bump jupyter-core from 4.6.3 to 4.11.2 in /examples/research_projects/lxmert (#31360)
Fix gradio tool demos (#31230)
Bump transformers from 3.5.1 to 4.38.0 in /examples/research_projects/pplm (#31352)
Bump tornado from 6.3.3 to 6.4.1 in /examples/research_projects/lxmert (#31353)
üö® FLAVA: Remove double softmax (#31322)
Fix Cohere CI (#31263)
Improve error msg when using bitsandbytes (#31350)
Decorators for deprecation and named arguments validation (#30799)
docs/zh: fix style (#31334)
Fix paligemma inverted mask (#31207)
docs: fix style (#31340)
Use unused prepare_img() function in dinov2 conversion script (#31335)
Rename test_model_common_attributes -> test_model_get_set_embeddings (#31321)
Bump transformers from 3.5.1 to 4.38.0 in /examples/research_projects/adversarial (#31320)
interpolation added for TVP. (#30863)
Bump pillow from 10.2.0 to 10.3.0 in /examples/research_projects/decision_transformer (#31319)
Remove ConversationalPipeline and Conversation object (#31165)
Bump transformers from 3.5.1 to 4.38.0 in /examples/research_projects/bert-loses-patience (#31291)
Bump aiohttp from 3.9.0 to 3.9.4 in /examples/research_projects/decision_transformer (#31317)
Bump tornado from 6.3.3 to 6.4.1 in /examples/research_projects/visual_bert (#31298)
Implement JSON dump conversion for torch_dtype in TrainingArguments (#31224)
Extend save_pretrained to offloaded models (#27412)
Fix jetmoe model (#31279)
Fixed Wav2Vec2ProcessorWithLM decoding error (#31188)
Enable HF pretrained backbones (#31145)
Update text-to-speech.md (#31269)
Fix SwinLayer / DonutSwinLayer / ClapAudioLayer attention mask device (#31295)
Bump transformers from 3.5.1 to 4.38.0 in /examples/research_projects/bertabs (#31290)
Pipeline VQA: Add support for list of images and questions as pipeline input (#31217)
Bump transformers from 4.19.0 to 4.38.0 in /examples/research_projects/codeparrot (#31285)
Mark MobileNetV1ModelTest::test_batching_equivalence as flaky (#31258)
Enable dynamic resolution input for Beit (#31053)
fix accelerate tests for roberta xl (#31288)
Fix _save_tpu: use _maybe_convert_to_cpu instead of to cpu. (#31264)
Bump transformers from 3.5.1 to 4.38.0 in /examples/research_projects/bertology (#31256)
fix: `str` should be used not `int` when setting env variables (#31272)
Switch from `cached_download` to `hf_hub_download` in remaining occurrences (#31284)
Generation: fix handling of special tokens (#31254)
Make mamba use cache (#31116)
fix loading special_tokens_map_file (#31012)
[`SwitchTransformer`] Significant performance improvement on MoE blocks (#31173)
no need for explicit EXTRA_TOKENS in processing_paligemma.py (#31022)
Skip failing JetMOE generation tests (#31266)
Reduce by 2 the memory requirement in `generate()` üî•üî•üî• (#30536)
Add condition to `benchmark` job in `push-important-models.yml` (#31259)
Fix circular reference issue in CLIPTokenizerFast (#31075)
Add missing Flaubert tokenizer tests (#30492)
enable deterministic mode for npu (#31253)
doc: add info about wav2vec2 bert in older wav2vec2 models. (#31120)
Bump transformers from 3.5.1 to 4.38.0 in /examples/research_projects/deebert (#31244)
Early labels validation (#31240)
Benchmark GitHub Actions workflow (#31163)
Fixing `name 'torch' is not defined` in `bitsandbytes` integration (#31243)
Specify dtype=torch.bool to avoid xla error (#31191)
Bump transformers from 4.26.0 to 4.38.0 in /examples/research_projects/vqgan-clip (#31242)
Upload (daily) CI results to Hub (#31168)
Move out common backbone config param validation (#31144)
Blip: Deprecate `BlipModel` (#31235)
Fix `MistralIntegrationTest` (#31231)
add no split modules for xlmrobertaxl (#31223)
Add new line switch before logging ***** Running {description} ***** (#31225)
Fix pipeline tests - torch imports (#31227)
fix bf16 issue in text classification pipeline (#30996)
Add dynamic resolution input/interpolate position embedding to deit (#31131)
Video-LLaVa: handle any number of frames (#31221)
fix(PatchTST): Wrong dropout used for PretainHead (#31117)
Fix sentence fragment within test comments (#31218)
Pass device in Logits Processor's init (#29804)
[docs] Spanish translation of tokenizer_summary.md (#31154)
Fix GPU OOM for `mistral.py::Mask4DTestHard` (#31212)
Set greater_is_better to False if metric_for_best_model ends with "loss" (#31142)
Cohere: Fix copied from (#31213)
Wrong translation FR : Contents = Contenu (#31186)
Rename sanity_evaluation to eval_on_start (#31192)
Fix typo in utils (#31169)
fix the get_size_with_aspect_ratio in max_size situation (#30902)
Add Qwen2 GGUF loading support (#31175)
Fix `test_compile_static_cache` (#30991)
üö® [Mistral and friends] Update MLP (#31057)
SlidingWindowCache: reduce differences to other Cache classes (#30970)
Ignore non-causal mask in more cases with SDPA (#30138)
Fix Cannot convert [array()] to EagerTensor of dtype int64 (#31109)
[`GemmaModel`] fix small typo (#31202)
Token healing (#30081)
Remove copied froms for deprecated models (#31153)
Fix typo: use_safetenstors to use_safetensors (#31184)
Diff converter v2 (#30868)
Added description of quantization_config (#31133)
Instance segmentation examples (#31084)
Add streaming, various fixes (#30838)
[trainer] add sanity evaluation option  (#31146)
Quantization: Enhance bnb error message (#31160)
Update sam.md (#31130)
Fix quantized cache output (#31143)
pytest -rsfE (#31140)
helper (#31152)
Workflow: Remove `IS_GITHUB_CI` (#31147)
Docs / Quantization: Replace all occurences of `load_in_8bit` with bnb config (#31136)
fix get_scheduler when name is warmup_stable_decay (#31128)
FIX / Quantization: Add extra validation for bnb config (#31135)
Cleanup docker build (#31119)
Add on_optimizer_step to callback options (#31095)
Add VLM generation default contributor (#31115)
FIX / Docs: Fix GPTQ expected number of bits (#31111)
Fix nightly circleci (#31114)
Rm maintainer + migrate (#31089)
Fix faulty rstrip in module loading (#31108)
Fix env.py in cases where torch is not present (#31113)
Improve `transformers-cli env` reporting (#31003)
Use `HF_HUB_OFFLINE` + fix has_file in offline mode (#31016)
FEAT: Add mistral v3 conversion script (#30981)
Quantized KV cache: update quanto (#31052)
Deprecate low use models (#30781)
Docs / Quantization: Redirect deleted page (#31063)
TST: Fix instruct-blip tests (#31088)
Fix DeepSpeed compatibility with weight_norm (#30881) (#31018)
Fix PretrainedConfig docstring with deprecated resume_download (#31014)
skip `test_multi_gpu_data_parallel_forward` for `vit` and `deit` (#31086)
FIX / OPT: Fix OPT multi-GPU training for `OPTForQuestionAnswering` (#31092)
FIX: Add `accelerate` as a hard requirement (#31090)
Render chat template tojson filter as unicode (#31041)
Docs / PEFT: Add PEFT API documentation (#31078)
Watermark: fix tests (#30961)
Fix failing tokenizer tests (#31083)
[SuperPoint, PaliGemma] Update docs (#31025)
Fix typo in trainer.py (#31048)
Fix OWLv2 post_process_object_detection for multiple images (#31082)
Remove float64 cast for OwlVit and OwlV2 to support MPS device (#31071)
fix from_pretrained in offline mode when model is preloaded in cache (#31010)
Remove redundant backend checks in training_args.py (#30999)
Update quicktour.md to fix broken link to Glossary (#31072)
fix "piano" typo (#31027)
Remove `ninja` from docker image build (#31080)
use `@main` (#31065)
skip `test_model_parallelism` for 2 model test classes (#31067)
Fix pad_to_max_length Whisper (#30787)
Fix quanto tests (#31062)
Update feature request label in template (#30940)
Follow up: Fix link in dbrx.md (#30514)
unpin uv (#31055)
Redirect transformers_agents doc to agents (#31054)
Paligemma- fix devices and dtype assignments (#31008)
Add split special tokens (#30772)
added interpolation for vitmae model in pytorch as well as tf. (#30732)
save the list of new model failures (#31013)
Quantization / TST: Fix remaining quantization tests (#31000)
Fix resume_download future warning (#31007)
allow multi-gpu (#31011)
FIX / TST: Fix expected results on Mistral AWQ test  (#30971)
[tests] make `test_model_parallelism` device-agnostic   (#30844)
Perceiver interpolate position embedding (#30979)
pin `uv==0.1.45` (#31006)
Do not trigger autoconversion if local_files_only (#31004)
Fix training speed regression introduced by "optimize VRAM for calculating pos_bias in LayoutLM v2, v3 (#26139)" (#30988)
add prefix space ignored in llama #29625 (#30964)
Bugfix: WandbCallback uploads initial model checkpoint (#30897)
Remove deprecated properties in tokenization_nllb.py and tokenization_nllb_fast.py (#29834)
[Port] TensorFlow implementation of Mistral (#29708)
Update 4 `MptIntegrationTests` expected outputs (#30989)
Add a check that warmup_setps is either 0 or >= 1 (#30764)
[tests] add `torch.use_deterministic_algorithms` for XPU (#30774)
Fix accelerate failing tests (#30836)
FIX / Docs: Minor changes in quantization docs (#30985)
Finish adding support for torch.compile dynamic shapes (#30919)
test_custom_4d_attention_mask skip with sliding window attn (#30833)
Docs / Quantization: refactor quantization documentation (#30942)
Quantized KV Cache (#30483)
Bump requests from 2.31.0 to 2.32.2 in /examples/research_projects/visual_bert (#30983)
Push ci image (#30982)
Using assistant in AutomaticSpeechRecognitionPipeline with different encoder size (#30637)
Update object detection with latest resize and pad strategies (#30955)
Paligemma causal attention mask (#30967)
Fix link in Pipeline documentation (#30948)
[Whisper] Strip prompt before finding common subsequence (#27836)
Generation: get special tokens from model config (#30899)
legacy to init the slow tokenizer when converting from slow was wrong (#30972)
Finally fix the missing new model failure CI report (#30968)
üö® out_indices always a list (#30941)
Paligemma - fix slow tests, add bf16 and f16 slow tests (#30851)
[whisper] only trigger forced ids warning once (#30966)
Avoid extra chunk in speech recognition (#29539)
[doc] Add references to the fine-tuning blog and distil-whisper to Whisper. (#30938)
Fix low cpu mem usage tests (#30808)
Update video-llava docs (#30935)
Bump requests from 2.31.0 to 2.32.2 in /examples/research_projects/lxmert (#30956)
Update build ci image [push-ci-image] (#30933)
update ruff version (#30932)
üö® [Idefics2] Update ignore index (#30898)
Fix inhomogeneous shape error in example (#30434)
Fix swin embeddings interpolation (#30936)
TST / Workflows: Get slack notifications for docker image build (#30891)
[Benchmark] Reuse `optimum-benchmark` (#30615)
fix: center_crop occasionally outputs off-by-one dimension matrix (#30934)
Enforce saving at end of training if saving option chosen (#30160)
CI:  AMD MI300 tests fix (#30797)
PaliGemma - fix processor with no input text (#30916)
Bump requests from 2.31.0 to 2.32.0 in /examples/research_projects/decision_transformer (#30925)
FEAT / Trainer: LOMO optimizer support (#30178)
FIX / TST: Fix expected results on Mistral slow test (A10) (#30909)
[docs] Spanish translation of model_memory_anatomy.md (#30885)
Add torch.compile for Mistral (#30642)
Introduce configured_state arg for accelerator_config (#29781)
`tokenizer_class = "AutoTokenizer"` Llava Family (#30912)
Fix a shape annotation and typos in `mamba` slow forward (#30691)
Add AutoFeatureExtractor support to Wav2Vec2ProcessorWithLM (#28706)
fix for custom pipeline configuration  (#29004)
separate kwargs in processor (similar to #30193) (#30905)
Fix num_hidden_layers in initialization of new model in Mamba (#30403)
add return_token_timestamps to WhisperProcessor (#30812)
DeformableDETR two stage support bfloat16  (#30907)
LLaVa-Next: Update docs with batched inference (#30857)
Add support for torch.compile dynamic shapes (#30560)
FIX / Quantization: Fix Dockerfile build (#30890)
Add TokenClassification for Mistral, Mixtral and Qwen2 (#29878)
Enable dynamic resolution input for Swin Transformer and variants (#30656)
v4.42.dev.0
Add fixed resize and pad strategy for object detection (#30742)
update release script (#30880)
Support arbitrary processor (#30875)
[whisper] fix multilingual fine-tuning (#30865)
Fix dependencies for image classification example (#30842)
Enable device map (#30870)
Remove deprecated logic and warnings (#30743)
TEST: Add llama logits tests (#30835)
Fix VideoLlava imports (#30867)
TST / Quantization: Reverting to torch==2.2.1 (#30866)
Docs: update example with assisted generation + sample (#30853)
Video-LLaVa: Fix docs (#30855)
Make `Gemma` work with `torch.compile` (#30775)
Disable the FA backend for SDPA on AMD GPUs (#30850)
Cache: add new flag to distinguish models that `Cache` but not static cache (#30800)
[Idefics2] Improve docs, add resources (#30717)
add sdpa to ViT [follow up of #29325] (#30555)
[LLaVa-NeXT] Small fixes (#30841)
Fix llama model sdpa attention forward function masking bug when output_attentions=True (#30652)
Use `torch 2.3` for CI (#30837)
FEAT / Bitsandbytes: Add `dequantize` API for bitsandbytes quantized models (#30806)
Deprecate models script - correctly set the model name for the doc file (#30785)
Better llava next. (#29850)
Update ds_config_zero3.json (#30829)
Missing `Optional` in typing. (#30821)
Jamba - Skip 4d custom attention mask test (#30826)
Loading GGUF files support (#30391)
Add Video Llava  (#29733)
Remove unused module DETR based models (#30823)
Support mixed-language batches in `WhisperGenerationMixin` (#29688)
Add missing dependencies in image classification example (#30820)
Add support for custom checkpoints in MusicGen (#30011)
Add PaliGemma (#30814)
Added the necessay import of module (#30804)
Add JetMoE model (#30005)
[T5] Adding `model_parallel = False` to `T5ForTokenClassification` and `MT5ForTokenClassification` (#30763)
Deprecate TF weight conversion since we have full Safetensors support now (#30786)
CI: more models wo cache support (#30780)
Add Watermarking LogitsProcessor and WatermarkDetector (#29676)
PEFT: Access active_adapters as a property in Trainer (#30790)
Fix cache type in Idefics2 (#30729)
Fix OWLv2 Doc (#30794)
CI: update to ROCm 6.0.2 and test MI300 (#30266)
skip low_cpu_mem_usage tests (#30782)
Deprecate models script (#30184)
Save other CI jobs' result (torch/tf pipeline, example, deepspeed etc) (#30699)
Generate: assistant should be greedy in assisted decoding (#30778)
Port IDEFICS to tensorflow (#26870)
Generate: remove near-duplicate sample/greedy copy (#30773)
[Object detection pipeline] Lower threshold (#30710)
enable Pipeline to get device from model  (#30534)
Qwen: incorrect setup flag (#30776)
Generation / FIX: Fix multi-device generation (#30746)
Llama: fix custom 4D masks, v2 (#30348)
[GroundingDino] Adding ms_deform_attn kernels (#30768)
Support for Falcon2-11B (#30771)
Blip dynamic input resolution (#30722)
Workflow: Replace `actions/post-slack` with centrally defined workflow (#30737)
[awq] replace scale when we have GELU (#30074)
hqq - fix weight check in check_quantized_param (#30748)
[docs] Update link in es/pipeline_webserver.md (#30745)
PEFT / Trainer: Make use of `model.active_adapters()` instead of deprecated `model.active_adapter` whenever possible (#30738)
mlp_only_layers is more flexible than decoder_sparse_step (#30552)
Update llama3.md, fix typo (#30739)
[docs] Update es/pipeline_tutorial.md (#30684)
Update CodeLlama references (#30218)
Generate: consistently handle special tokens as tensors (#30624)
KV cache is no longer a model attribute (#30730)
Fix image post-processing for OWLv2 (#30686)
Generate: add `min_p` sampling (#30639)
Removal of deprecated maps (#30576)
Enable dynamic resolution for vivit (#30630)
Add dynamic resolution input/interpolate position embedding to SigLIP (#30719)
Cache: models return input cache type (#30716)
Immutability for data collators (#30603)
Update object detection guide (#30683)
Add installation of examples requirements in CI (#30708)
Llava: remove dummy labels (#30706)
[BitsandBytes] Verify if GPU is available (#30533)
Add examples for detection models finetuning (#30422)
Patch CLIP image preprocessor (#30698)
Pin deepspeed (#30701)
Add safetensors to model not found error msg for default use_safetensors value (#30602)
Rename artifact name `prev_ci_results` to `ci_results` (#30697)
Update `workflow_id` in `utils/get_previous_daily_ci.py` (#30695)
Separate tokenizer tests (#30675)
Bump tqdm from 4.48.2 to 4.66.3 in /examples/research_projects/lxmert (#30644)
Reboot Agents (#30387)
Bump tqdm from 4.48.2 to 4.66.3 in /examples/research_projects/visual_bert (#30645)
Bump tqdm from 4.63.0 to 4.66.3 in /examples/research_projects/decision_transformer (#30646)
Updated docs of `forward` in `Idefics2ForConditionalGeneration` with correct `ignore_index` value (#30678)
Word-level timestamps broken for short-form audio (#30325)
Fix `cache_position` initialisation for generation with `use_cache=False` (#30485)
Adding _tie_weights() to prediction heads to support low_cpu_mem_usage=True (#29024)
Bump werkzeug from 3.0.1 to 3.0.3 in /examples/research_projects/decision_transformer (#30679)
Bump jinja2 from 3.1.3 to 3.1.4 in /examples/research_projects/decision_transformer (#30680)
top-k instead of top-p in MixtralConfig docstring (#30687)
Respect `resume_download` deprecation  (#30620)
Fix typo: llama3.md (#30653)
Trainer - add cache clearing and the option for batched eval metrics computation (#28769)
Trainer._load_from_checkpoint - support loading multiple Peft adapters (#30505)
Fix llava next tie_word_embeddings config  (#30640)
Quantization / HQQ: Fix HQQ tests on our runner (#30668)
Hotfix-change-ci (#30669)
Check if the current compiled version of pytorch supports MPS (#30664)
[`CI update`] Try to use dockers and no cache (#29202)
Avoid duplication in PR slow CI model list (#30634)
Prevent `TextGenerationPipeline._sanitize_parameters` from overriding previously provided parameters (#30362)
HQQ: PEFT support for HQQ (#30632)
Fix W&B run name (#30462)
add mlp bias for llama models (#30031)
Fix CI after #30410 (#30612)
Add HQQ quantization support (#29637)
Output `None` as attention when layer is skipped (#30597)
Fix FX tracing issues for Llama (#30619)
Generate: fix `SinkCache` on Llama models (#30581)
Docs: add missing `StoppingCriteria` autodocs (#30617)
Docs: fix `generate`-related rendering issues (#30600)
phi3 chat_template does not support system role (#30606)
Use `contiguous()` in clip checkpoint conversion script (#30613)
fix:missing `output_router_logits` in SwitchTransformers (#30573)
Fix copies for DBRX - neuron fix (#30610)
üö® Update image_processing_vitmatte.py (#30566)
Fix memory leak with CTC training script on Chinese languages (#30358)
Fix for Neuron (#30259)
Fix: failing CI after #30568 (#30599)
Bump torch from 1.9.0+cpu to 1.13.1 in /examples/flax/vision (#21168)
Bump pillow from 10.0.1 to 10.2.0 in /examples/research_projects/decision_transformer (#28655)
Bump torch from 1.9.0+cpu to 1.13.1 in /examples/research_projects/jax-projects/hybrid_clip (#21167)
Bump torch from 1.11.0 to 1.13.1 in /examples/research_projects/decision_transformer (#21171)
Fix llava half precision and autocast issues (#29721)
Generate: remove deprecated public decoding functions and streamline logic üßº  (#29956)
Improve object detection task guideline (#29967)
Fix image segmentation example - don't reopen image (#30481)
Bump torch from 1.6.0 to 1.13.1 in /examples/research_projects/visual_bert (#21172)
Bump torch from 1.11.0 to 1.13.1 in /examples/research_projects/codeparrot (#21170)
Bump torch from 1.6.0 to 1.13.1 in /examples/research_projects/lxmert (#21174)
Bump pyarrow from 1.0.1 to 15.0.0 in /examples/research_projects/lxmert (#30584)
Bump pyarrow from 1.0.1 to 15.0.0 in /examples/research_projects/visual_bert (#30583)
Bump pyarrow from 7.0.0 to 15.0.0 in /examples/research_projects/decision_transformer (#30582)
Bump gitpython from 3.1.32 to 3.1.41 in /examples/research_projects/distillation (#30586)
Bump grpcio from 1.44.0 to 1.53.2 in /examples/research_projects/decision_transformer (#30585)
Bump gitpython from 3.1.32 to 3.1.41 in /examples/research_projects/decision_transformer (#30587)
Gemma: update activation warning (#29995)
Fix canonical model --model_type in examples (#30480)
remove jax example (#30498)
Fix QA example (#30580)
Refactor default chat template warnings (#30551)
Fix Marian model conversion (#30173)
Encoder-decoder models: move embedding scale to nn.Module (#30410)
Use text config's vocab size in testing models (#30568)
Remove `use_square_size` after loading (#30567)
General PR slow CI (#30540)
Fix generation doctests (#30263)
Add chat templating support for KeyDataset in text-generation pipeline (#30558)
BlipModel: get_multimodal_features method (#30438)
Fix seq2seq collator padding (#30556)
DBRX: make fixup (#30578)
Generate: update links on LLM tutorial doc (#30550)
Cache: Static cache as a standalone object (#30476)
Enable multi-device for more models (#30409)
Pass `use_cache` in kwargs for GPTNeoX (#30538)
Include safetensors as part of `_load_best_model` (#30553)
Reenable SDPA's FA2 During Training with torch.compile (#30442)
Fix repo. fetch/checkout in PR slow CI job (#30537)
Update runner tag for PR slow CI (#30535)
Fix broken link to Transformers notebooks (#30512)
Pass attn_implementation when using AutoXXX.from_config (#30507)
Allow boolean FSDP options in fsdp_config (#30439)
Fix link in dbrx.md (#30509)
[SegGPT] Fix seggpt image processor (#29550)
load_image - decode b64encode and encodebytes strings (#30192)
Fix GroundingDINO, DPR after BERT SDPA update (#30506)
[examples] update whisper fine-tuning (#29938)
[`DETR`] Remove timm hardcoded logic in modeling files (#29038)
Remove skipping logic now that set_epoch exists (#30501)
[`BERT`] Add support for sdpa (#28802)
Use the Keras set_random_seed in tests (#30504)
Update `dtype_byte_size` to handle torch.float8_e4m3fn/float8_e5m2 types (#30488)
Fix the `bitsandbytes` error formatting ("Some modules are dispatched on ...") (#30494)
FEAT: PEFT support for EETQ (#30449)
[docs] Spanish translation of pipeline_tutorial.md (#30252)
Quantization: `HfQuantizer` quant method update (#30484)
Add sidebar tutorial for chat models (#30401)
Do not use deprecated `SourceFileLoader.load_module()` in dynamic module loading (#30370)
Fix Llava for 0-embeddings (#30473)
Introduce Stateful Callbacks (#29666)
Make accelerate install non-torch dependent (#30463)
 Fix Issue #29817 Video Classification Task Guide Using Undeclared Variables (#30457)
Add WSD scheduler (#30231)
üö® Add training compatibility for Musicgen-like models (#29802)
Prevent crash with `WandbCallback` with third parties (#30477)
Don't run fp16 MusicGen tests on CPU (#30466)
Fix SigLip classification doctest (#30475)
Script for finding candidate models for deprecation (#29686)
[fix codellama conversion]  (#30472)
FIX / Workflow: Fix SSH workflow bug (#30474)
FIX / Workflow: Change tailscale trigger condition (#30471)
Workflow / ENH: Add SSH into our runners workflow (#30425)
consistent job / pytest report / artifact name correspondence (#30392)
Non blocking support to torch DL's (#30465)
Enable fp16 on CPU (#30459)
Neuron: When save_safetensor=False, no need to move model to CPU (#29703)
[`research_project`] Most of the security issues come from this requirement.txt (#29977)
Fix wrong indent in `utils/check_if_new_model_added.py` (#30456)
Phi-3 (#30423)
Add `paths` filter to avoid the chance of being triggered (#30453)
[SegGPT] Fix loss calculation (#30421)
fix jamba slow foward for multi-gpu (#30418)
fix uncaught init of linear layer in clip's/siglip's for image classification models (#30435)
[tests] make test device-agnostic (#30444)
[`Llava`] + CIs fix red cis and llava integration tests (#30440)
Fix YOLOS image processor resizing (#30436)
Add llama3 (#30334)
New model PR needs green (slow tests) CI (#30341)
Remove mentions of models in the READMEs and link to the documentation page in which they are featured. (#30420)
Remove add-new-model in favor of add-new-model-like (#30424)
Remove task guides auto-update in favor of links towards task pages (#30429)
[`LlamaTokenizerFast`] Refactor default llama (#28881)
Fix use_cache for xla fsdp (#30353)
Rename torch.run to torchrun (#30405)
Remove old TF port docs (#30426)
Fix LayoutLMv2 init issue and doctest (#30278)
FIX: re-add bnb on docker image (#30427)
Make EosTokenCriteria compatible with mps (#30376)
fix for itemsize => element_size() for torch backwards compat (#30133)
Fix on "cache position" for assisted generation (#30068)
Jax: scipy version pin (#30402)
[tests] add `require_torch_sdpa` for test that needs sdpa support (#30408)
fix: link to HF repo/tree/revision when a file is missing (#30406)
remove redundant logging from longformer (#30365)
[Grounding DINO] Add support for cross-attention in GroundingDinoMultiHeadAttention (#30364)
Add inputs embeds in generation (#30269)
show `-rs` to show skip reasons (#30318)
[docs] LLM inference (#29791)
[FEAT]: EETQ quantizer support (#30262)
Add sdpa and fa2 the Wav2vec2 family.  (#30121)
FIX / PEFT: Pass device correctly to peft (#30397)
Fix DETA save_pretrained (#30326)
Jamba: fix left-padding test (#30389)
Fix layerwise GaLore optimizer hard to converge with warmup scheduler  (#30372)
Terminator strings for generate() (#28932)
Update docstrings for text generation pipeline (#30343)
`Llama` family, fix `use_cache=False` generation (#30380)
Add FSDP config for CPU RAM efficient loading through accelerate (#30002)
GenerationConfig: warn if pad token is negative (#30187)
Enable multi-device for more models (#30379)
Nits for model docs (#29795)
[Grounding DINO] Add resources (#30232)
Add TF swiftformer (#23342)
Fix config + attn_implementation in AutoModelForCausalLM.from_pretrained (#30299)
Do not remove half seq length in generation tests (#30016)
Update unwrap from accelerate (#29933)
Restore casting of masked_spec_embed (#30336)
Deprecate default chat templates (#30346)
Transformers Metadata (#30344)
parallel job limit for doctest (#30342)
[Whisper] Fix slow tests (#30152)
Pipeline: fix `pad_token_id` again (#30338)
[Feature Extractors] Fix kwargs to pre-trained (#30260)
feat: Upgrade Weights & Biases callback (#30135)
Enable multi-device for some models (#30207)
[UDOP] Add special tokens to tokenizer (#29594)
Fix `AssertionError` in clip conversion script (#30321)
Avoid `jnp` import in `utils/generic.py`  (#30322)
üö®üö®üö®Deprecate `evaluation_strategy` to `eval_strategy`üö®üö®üö® (#30190)
Fix test transposing image with EXIF Orientation tag (#30319)
disable use_cache if using gradient checkpointing (#30320)
fix Parameter dtype in audio models (#30310)
Fix: remove `pad token id` in pipeline forward arguments  (#30285)
Fix missing `prev_ci_results` (#30313)
Dev version
FIX: Fixes unexpected behaviour for Llava / LLama & AWQ Fused modules + revert #30070 at the same time (#30317)
Add DBRX Model (#29921)
Do not drop mask with SDPA for more cases (#30311)
Revert "Re-enable SDPA's FA2 path (#30070)" (#30314)
Fix RecurrentGemma device_map (#30273)
Add atol for sliding window test (#30303)
Add jamba (#29943)
Fix all torch pipeline failures except one (#30290)
Fix donut token2json multiline (#30300)
Add Flash Attention 2 to M2M100 model (#30256)
Fix quality Olmo + SDPA (#30302)
Re-enable SDPA's FA2 path (#30070)
Add OLMo model family (#29890)
Upgrading to tokenizers 0.19.0 (#30289)
Add strategy to store results in evaluation loop (#30267)
Add token type ids to CodeGenTokenizer (#29265)
FIX: Fix push important models CI (#30291)
Fix `Fatal Python error: Bus error` in `ZeroShotAudioClassificationPipelineTests` (#30283)
Fix test `ExamplesTests::test_run_translation` (#30281)
Enable fx tracing for Mistral (#30209)
Configuring Translation Pipelines documents update #27753 (#29986)
FIX / AWQ: Fix failing exllama test (#30288)
Fix SpeechT5 forward docstrings (#30287)
Fix SDPA sliding window compatibility (#30127)
Fix test fetcher (doctest) + `Idefics2`'s doc example (#30274)
fix: Fixed a `raise` statement (#30275)
BLIP - fix pt-tf equivalence test (#30258)
Raise relevent err when wrong type is passed in as the accelerator_config (#29997)
add `push_to_hub` to pipeline (#29172)
Workflow: Update tailscale to release version (#30268)
Allow for str versions of dicts based on typing (#30227)
FIX: Fix 8-bit serialization tests (#30051)
FIX: Fix corner-case issue with the important models workflow (#30212)
More fixes for doctest (#30265)
Update `ko/_toctree.yml`  (#30062)
Remove incorrect arg in codellama doctest (#30257)
[Docs] Update recurrent_gemma.md for some minor nits (#30238)
Add Idefics2 (#30253)
[tests] add the missing `require_torch_multi_gpu` flag (#30250)
update github actions packages' version to suppress warnings (#30249)
round epoch only in console (#30237)
Fix doctest more (for `docs/source/en`) (#30247)
Separate out kwargs in processor (#30193)
fix: Fixed `type annotation` for compatability with python 3.8 (#30243)
Refactor doctest (#30210)
fix: Replaced deprecated `typing.Text`  with `str` (#30230)
Set pad_token in run_glue_no_trainer.py #28534 (#30234)
fix: Replace deprecated `assertEquals` with `assertEqual` (#30241)
Add test for parse_json_file and change typing to os.PathLike (#30183)
Fixed config.json download to go to user-supplied cache directory (#30189)
Fix/Update for doctest (#30216)
Update modeling_bark.py (#30221)
Fix `RecurrentGemmaIntegrationTest.test_2b_sample` (#30222)
fix fuyu doctest (#30215)
fix typo (#30220)
fix: Replaced deprecated `logger.warn` with `logger.warning` (#30197)
Fix pipeline logger.warning_once bug (#30195)
ENH: [`CI`] Add new workflow to run slow tests of important models on push main if they are modified (#29235)
Docs PR template (#30171)
Falcon: make activation, ffn_hidden_size configurable (#30134)
Update output of SuperPointForKeypointDetection (#29809)
[Processor classes] Update docs (#29698)
fix: Fixed `ruff` configuration to avoid deprecated configuration warning (#30179)
chore: remove repetitive words (#30174)
Guard XLA version imports (#30167)
Fix Llava chat template examples (#30130)
Adding grounding dino (#26087)
Fixed typo in comments/documentation for Pipelines documentation (#30170)
Update config class check in auto factory (#29854)
FIX / bnb: fix torch compatiblity issue with `itemize` (#30162)
Fix natten install in docker (#30161)
Fixing a bug when MlFlow try to log a torch.tensor (#29932)
Add recurrent gemma (#30143)
Fix typing annotation in hf_argparser (#30156)
Fix accelerate kwargs for versions <0.28.0 (#30086)
[UDOP] Improve docs, add resources (#29571)
[UDOP] Fix tests (#29573)
Add str to TrainingArguments report_to type hint (#30078)
[tests] make 2 tests device-agnostic  (#30008)
[CI] Quantization workflow fix (#30158)
Fix and simplify semantic-segmentation example (#30145)
Fix length related warnings in speculative decoding (#29585)
[CI] Fix setup (#30147)
[docs] Fix image segmentation guide (#30132)
Fix quantization tests (#29914)
Send headers when converting safetensors (#30144)
Fix slow tests for important models to be compatible with A10 runners (#29905)
[Trainer] Undo #29896 (#30129)
[Trainer] Fix default data collator (#30142)
Revert workaround for TF safetensors loading (#30128)
Fix docs Pop2Piano (#30140)
Add datasets.Dataset to Trainer's train_dataset and eval_dataset type hints (#30077)
Fix failing DeepSpeed model zoo tests (#30112)
[`StableLm`] Add QK normalization and Parallel Residual Support (#29745)
Adding `mps` as device for `Pipeline` class (#30080)
Fix typo at ImportError (#30090)
Make vitdet jit trace complient (#30065)
Trainer / Core : Do not change init signature order (#30126)
Fix falcon with SDPA, alibi but no passed mask (#30123)
fix learning rate display in trainer when using galore optimizer (#30085)
Accept token in trainer.push_to_hub() (#30093)
[#29174] ImportError Fix: Trainer with PyTorch requires accelerate>=0.20.1 Fix (#29888)
Patch fix - don't use safetensors for TF models (#30118)
fixing issue 30034 - adding data format for run_ner.py (#30088)
[tests] add `require_bitsandbytes` marker (#30116)
updated examples/pytorch/language-modeling scripts and requirements.txt to require datasets>=2.14.0 (#30120)
Make MLFlow version detection more robust and handles mlflow-skinny (#29957)
Change log level to warning for num_train_epochs override (#30014)
[Whisper] Computing features on GPU in batch mode for whisper feature extractor.    (#29900)
doc: Correct spelling mistake (#30107)
Fix whisper kwargs and generation config (#30018)
Fix auto tests (#30067)
Add docstrings and types for MambaCache (#30023)
Refactor daily CI workflow (#30012)
Fix `torch.fx` symbolic tracing for LLama (#30047)
[test fetcher] Always include the directly related test files (#30050)
Update quantizer_bnb_4bit.py: In the ValueError string there should be "....you need to set `llm_int8_enable_fp32_cpu_offload=True`...." instead of "`load_in_8bit_fp32_cpu_offload=True`". (#30013)
[bnb] Fix offload test (#30039)
[Trainer] Allow passing image processor (#29896)
Fix mixtral ONNX Exporter Issue. (#29858)
if output is tuple like facebook/hf-seamless-m4t-medium, waveform is ‚Ä¶ (#29722)
skip `test_encode_decode_fast_slow_all_tokens` for now (#30044)
Add `whisper` to `IMPORTANT_MODELS` (#30046)
Refactor Cohere Model (#30027)
[`ProcessingIdefics`] Attention mask bug with padding (#29449)
Add a converter from mamba_ssm -> huggingface mamba (#29705)
Enable multi-device for efficientnet (#29989)
Make clearer about zero_init requirements (#29879)
[`Main CIs`] Fix the red cis  (#30022)
Superpoint imports fix (#29898)
[docs] Fix audio file (#30006)
Fix vipllava for generation (#29874)
Fix probability computation in `WhisperNoSpeechDetection` when recomputing scores (#29248)
Fix `kwargs` handling in `generate_with_fallback` (#29225)
Fix Qwen2Tokenizer (#29929)
Fix Swinv2ForImageClassification NaN output (#29981)
Make EncodecModel.decode ONNX exportable (#29913)
Update `tests/utils/tiny_model_summary.json` (#29941)
Fix `remove_columns` in `text-classification` example (#29351)
Generate: fix logits processors doctests (#29718)
Hard error when ignoring tensors. (#27484) (#29906)
Fix `skip_special_tokens` for `Wav2Vec2CTCTokenizer._decode` (#29311)
[Docs] Make an ordered list prettier in add_tensorflow_model.md (#29949)
Add Flash Attention 2 support to Musicgen and Musicgen Melody (#29939)
Adding FlaxNoRepeatNGramLogitsProcessor (#29677)
[bnb] Fix bug in `_replace_with_bnb_linear` (#29958)
Fix 29807 sinusoidal positional encodings in Flaubert, Informer and XLM (#29904)
[`generate`] fix breaking change for patch (#29976)
[docs] Big model loading (#29920)
Generate: move misplaced test (#29902)
[tests] fix the wrong output in `ImageToTextPipelineTests.test_conditional_generation_llava` (#29975)
Fix copies main ci (#29979)
Fix FA2 tests (#29909)
Rework tests to compare trainer checkpoint args (#29883)
[`BC`] Fix BC for AWQ quant (#29965)
Update model card and link of blog post. (#29928)
Reset alarm signal when the function is ended (#29706)
fix: get mlflow version from mlflow-skinny (#29918)
Add warning message for `run_qa.py` (#29867)
Fix rope theta for OpenLlama (#29893)
Super tiny fix 12 typos about "with with" (#29926)
Mark `test_eager_matches_sdpa_generate` flaky for some models (#29479)
Update installs in image classification doc (#29947)
[`LlamaSlowConverter`] Slow to Fast better support (#29797)
Fix doc issue #29758 in DebertaV2Config class (#29842)
[`BC`] Fix BC for other libraries (#29934)
Allow GradientAccumulationPlugin to be configured from AcceleratorConfig (#29589)
[ `TokenizationLlama`] fix the way we convert tokens to strings to keep leading spaces üö® breaking fix (#29453)
[`Mamba`] from pretrained issue with `self.embeddings` (#29851)
RoPE models: add numerical sanity-check test for RoPE scaling (#29808)
add functions to inspect model and optimizer status to trainer.py (#29838)
Safe import of LRScheduler (#29919)
Add beam search visualizer to the doc (#29876)
Tests: replace `torch.testing.assert_allclose` by `torch.testing.assert_close` (#29915)
[doc] fix some typos and add `xpu` to the testing documentation (#29894)
Adding Flash Attention 2 Support for GPT2 (#29226)
[`pipeline`]. Zero shot add doc warning (#29845)
[`GptNeox`] don't gather on pkv when using the trainer (#29892)
[`make fix-copies`] update and help (#29924)
Fix typo in T5Block error message (#29881)
MixtralSparseMoeBlock: add gate jitter (#29865)
add Cambricon MLUs support (#29627)
Move `eos_token_id` to stopping criteria (#29459)
fix fuyu device_map compatibility (#29880)
Reimplement "Automatic safetensors conversion when lacking these files" (#29846)
Fix 29807, sinusoidal positional encodings overwritten by post_init() (#29813)
Mamba `slow_forward` gradient fix (#29563)
Add Qwen2MoE (#29377)
Support `num_attention_heads` != `num_key_value_heads` in Flax Llama Implementation (#29557)
Set custom_container in build docs workflows (#29855)
Disable AMD memory benchmarks (#29871)
Add `cosine_with_min_lr` scheduler in Trainer (#29341)
Allow `bos_token_id is None` during the generation with `inputs_embeds` (#29772)
[docs] Indent ordered list in add_new_model.md (#29796)
Fix header in IFE task guide (#29859)
Replace 'decord' with 'av' in VideoClassificationPipeline (#29747)
Add warnings if training args differ from checkpoint trainer state (#29255)
remove quotes in code example (#29812)
[`revert commit`] revert  00a09ed448082da3d6d35fb23a37b7d04f7b4dcd
fix üò≠
Populate torch_dtype from model to pipeline (#28940)
Fix the behavior of collecting 'num_input_tokens_seen' (#29099)
Remove static pretrained maps from the library's internals (#29112)
model_summary.md - Restore link to Harvard's Annotated Transformer. (#29702)
[DOCS] Fix typo for llava next docs (#29829)
[`SuperPoint`] Fix doc example (#29816)
Complete security policy with mentions of remote code (#29707)
[`cleanup`] vestiges of causal mask (#29806)
replaced concatenation to f-strings to improve readability and unify ‚Ä¶ (#29785)
Generate: remove unused attributes in `AssistedCandidateGenerator` (#29787)
rm input dtype change in CPU (#28631)
Correct llava mask & fix missing setter for `vocab_size` (#29389)
Enable AMD docker build CI (#29803)
Fix type hint for train_dataset param of Trainer.__init__() to allow IterableDataset.  Issue 29678 (#29738)
[`quality`] update quality check to make sure we check imports üòà  (#29771)
Change in-place operations to out-of-place in LogitsProcessors (#29680)
Prepend `bos token` to Blip generations (#29642)
Llama: always convert the causal mask in the SDPA code path (#29663)
Generate: remove legacy generation mixin imports (#29782)
Add support for `torch_dtype` in the run_mlm example (#29776)
Add deterministic config to `set_seed` (#29778)
Silence deprecations and use the DataLoaderConfig (#29779)
Cast bfloat16 to float32 for Numpy conversions (#29755)
[`LlavaNext`] Fix llava next unsafe imports (#29773)
Fix docker image build for `Latest PyTorch + TensorFlow [dev]` (#29764)
fix issue with logit processor during beam search in Flax (#29636)
Allow `-OO` mode for `docstring_decorator` (#29689)
OWL-ViT box_predictor inefficiency issue (#29712)
Fixed typo in quantization_config.py (#29766)
[docs] Remove redundant `-`  and `the` from custom_tools.md (#29767)
[`BC 4.37 -> 4.38`] for Llama family, memory and speed  (#29753)
[`BitsAndBytesConfig`] Warning for unused `kwargs` & safety checkers for `load_in_4bit` and `load_in_8bit` (#29761)
Fix docker image build (#29762)
Update test reqs to include sentencepiece (#29756)
Add LLaVa-1.6, bis (#29586)
Add correct batched handling for apply_chat_template (#29222)
SuperPointModel -> SuperPointForKeypointDetection (#29757)
v4.40.0.dev.0
Support sharded safetensors in TF (#29350)
fix jinja2 package version check (#29754)
Update Mamba types and pass through use_cache attr to MambaModel (#29605)
[Tests] Remove unused code (#29737)
fix galore layerwise with frozen params (#29743)
fixed the issue of DPO trainer that using one node and mutiple GPUs and set the device_map='auto' (#29695)
Larger runner on CircleCI (#29750)
Tests: Musicgen tests + `make fix-copies` (#29734)
Fix `check_copies` not capturing the diff in model/paper title and link (#29724)
Llama: partial 4d masks (#29731)
Clean-up generation tests after moving methods to private (#29582)
Implementation of SuperPoint and AutoModelForKeypointDetection (#28966)
[`GemmaConverter`] use user_defined_symbols (#29473)
[`Gemma`]  final fixes to the modeling (#29729)
[tests] add more tests to `NOT_DEVICE_TESTS`  (#29670)
FEAT / Optim: Add GaLore optimizer (#29588)
Use logging.warning instead of warnings.warn in pipeline.__call__ (#29717)
Update the pipeline tutorial to include `gradio.Interface.from_pipeline` (#29684)
FIX [`bnb`] Make `unexpected_keys` optional (#29420)
Fix `filter_models` (#29710)
Add MusicGen Melody (#28819)
CI / generate: batch size computation compatible with all models (#29671)
[docs] Spanish translation of attention.md (#29681)
Revert "Fix wrong condition used in `filter_models`" (#29682)
[FIX] Fix speech2test modeling tests (#29672)
Generate: replace breaks by a loop condition (#29662)
[Quantization] Quanto quantizer (#29023)
Rename `glue` to `nyu-mll/glue` (#29679)
fix: typos (#29653)
Fix wrong condition used in `filter_models` (#29673)
[tests] ensure device-required software is available in the testing environment before testing     (#29477)
Fix AutoformerForPrediction example code (#29639)
[tests] remove deprecated tests for model loading (#29450)
Cohere Model Release (#29622)
Pipeline: use tokenizer pad token at generation time if the model pad token is unset. (#29614)
Trainer: fail early in the presence of an unsavable `generation_config` (#29675)
Extend import utils to cover "editable" torch versions (#29000)
Inaccurate code example within inline code-documentation (#29661)
Allow apply_chat_template to pass kwargs to the template and support a dict of templates (#29658)
Generate: handle `cache_position` update in `generate` (#29467)
Fix PVT v2 tests (#29660)
Add `dataset_revision` argument to `RagConfig` (#29610)
Fix TPU checkpointing inside Trainer (#29657)
[`PEFT`] Fix `save_pretrained` to make sure adapters weights are also saved on TPU (#29388)
Add newly added PVTv2 model to all README files. (#29647)
[docs] Remove broken ChatML format link from chat_templating.md (#29643)
Add PvT-v2 Model (#26812)
Fix `multi_gpu_data_parallel_forward` for `MusicgenTest` (#29632)
Fix batching tests for new models (Mamba and SegGPT) (#29633)
Refactor TFP call to just sigmoid() (#29641)
[tests] make `test_trainer_log_level_replica` to run on accelerators with more than 2 devices (#29609)
[`Mask2Former`] Move normalization for numerical stability (#29542)
Add support for FSDP+QLoRA and DeepSpeed ZeRO3+QLoRA (#29587)
[docs] Spanish translate chat_templating.md & yml addition (#29559)
[PyTorch/XLA] Fix extra TPU compilations introduced by recent changes (#29158)
Llama: allow custom 4d masks (#29618)
[`MaskFormer`, `Mask2Former`] Use einsum where possible (#29544)
Fix minor typo: infenrece => inference (#29621)
[generate] deprecate forced ids processor (#29487)
Adds pretrained IDs directly in the tests (#29534)
Warn about tool use (#29628)
[Whisper] Deprecate forced ids for v4.39 (#29485)
Core: Fix copies on main (#29624)
[Flash Attention 2] Add flash attention 2 for GPT-J (#28295)
[`Gemma`] Supports converting directly in half-precision (#29529)
Examples: check `max_position_embeddings` in the translation example (#29600)
Fix: handle logging of scalars in Weights & Biases summary (#29612)
Add tests for batching support (#29297)
Fix typo ; Update quantization.md (#29615)
Update flava tests (#29611)
Set env var to hold Keras at Keras 2 (#29598)
Update legacy Repository usage in various example files (#29085)
Implemented add_pooling_layer arg to TFBertModel (#29603)
Fix typo (determine) (#29606)
Stop passing None to compile() in TF examples (#29597)
Fix minor typo: softare => software (#29602)
Fix Fuyu doc typos (#29601)
Experimental loading of MLX files (#29511)
Tiny improvement for doc (#29581)
Fixed broken link (#29558)
Add missing localized READMEs to the copies check (#29575)
fix error: TypeError: Object of type Tensor is not JSON serializable ‚Ä¶ (#29568)
Don't use a subset in test fetcher if on `main` branch (#28816)
 [Docs] Fix FastSpeech2Conformer model doc links (#29574)
Make torch xla available on GPU (#29334)
Bark model Flash Attention 2 Enabling to pass on check_device_map parameter to super() (#29357)
Add Fill-in-the-middle training objective example - PyTorch (#27464)
[`Docs`] fixed minor typo (#29555)
[`Mamba doc`] Post merge updates  (#29472)
feat: use `warning_advice` for tensorflow warning (#29540)
Fix eval thread fork bomb (#29538)
[tests] use the correct `n_gpu` in `TrainerIntegrationTest::test_train_and_eval_dataloaders` for XPU (#29307)
Fix WhisperNoSpeechDetection when input is full silence  (#29065)
fix typos in FSDP config parsing logic in `TrainingArguments` (#29189)
Make sliding window size inclusive in eager attention (#29519)
StableLM: Fix dropout argument type error (#29236)
[tests] use `torch_device` instead of `auto` for model testing  (#29531)
Typo fix in error message (#29535)
fix image-to-text batch incorrect output issue (#29342)
[tests] add the missing `require_sacremoses` decorator  (#29504)
Generate: left-padding test, revisited (#29515)
Typo in mlx tensor support (#29509)
Fix `VisionEncoderDecoder` Positional Arg (#29497)
Set `inputs` as kwarg in `TextClassificationPipeline` (#29495)
test_generation_config_is_loaded_with_model  - fall back to pytorch model for now (#29521)
Add support for metadata format MLX (#29335)
Flava multimodal add attention mask (#29446)
fix: Avoid error when fsdp_config is missing xla_fsdp_v2 (#29480)
Revert "Automatic safetensors conversion when lacking these files (#2‚Ä¶ (#29507)
v4.39 deprecations üßº  (#29492)
Enable BLIP for auto VQA (#29499)
Fix: Disable torch.autocast in RotaryEmbedding of Gemma and LLaMa for MPS device (#29439)
Substantially reduce memory usage in _update_causal_mask for large batches by using .expand instead of .repeat [needs tests+sanity check] (#29413)
Fix `TextGenerationPipeline.__call__` docstring (#29491)
added the max_matching_ngram_size to GenerationConfig (#29131)
Generate: torch.compile-ready generation config preparation (#29443)
Fix test failure on DeepSpeed (#29444)
Avoid dummy token in PLD to optimize performance (#29445)
Generate: get generation mode from the generation config instance üßº (#29441)
Generate: add tests for caches with `pad_to_multiple_of`  (#29462)
Fix TrainingArguments regression with torch <2.0.0 for dataloader_prefetch_factor (#29447)
[`docs`] Add starcoder2 docs (#29454)
[`Docs` / `Awq`] Add docs on exllamav2 + AWQ (#29474)
[FIX] `offload_weight()` takes from 3 to 4 positional arguments but 5 were given (#29457)
üåê [i18n-KO] Translated generation_strategies.md to Korean (#29086)
[i18n-zh] Translate add_new_pipeline.md into Chinese (#29432)
Automatic safetensors conversion when lacking these files (#29390)
Update pytest `import_path` location  (#29154)
Fix bug with passing capture_* args to neptune callback (#29041)
[`Add Mamba`] Adds support for the `Mamba` models (#28094)
Generate: inner decoding methods are no longer public (#29437)
[`Udop imports`] Processor tests were not run. (#29456)
Revert-commit 0d52f9f582efb82a12e8d9162b43a01b1aa0200f (#29455)
more fix
[`UdopTokenizer`] Fix post merge imports (#29451)
[tests] enable test_pipeline_accelerate_top_p on XPU    (#29309)
[docs] Update starcoder2 paper link (#29418)
Fix max length for BLIP generation (#29296)
Exllama kernels support for AWQ models (#28634)
FIX [`Generation`] Fix some issues when running the MaxLength criteria on CPU (#29317)
[Docs] Spanish Translation -Torchscript md & Trainer md (#29310)
Add UDOP (#22940)
DeformableDETR support bfloat16 (#29232)
Avoid edge case in audio utils (#28836)
Fix grad_norm unserializable tensor log failure (#29212)
üö® Fully revert atomic checkpointing üö® (#29370)
Fix OneFormer `post_process_instance_segmentation` for panoptic tasks (#29304)
Fix: Fixed the previous tracking URI setting logic to prevent clashes with original MLflow code. (#29096)
Convert SlimSAM checkpoints (#28379)
Workaround for #27758 to avoid ZeroDivisionError (#28756)
Add mlx support to BatchEncoding.convert_to_tensors (#29406)
[Mixtral] Fixes attention masking in the loss (#29363)
update path to hub files in the error message (#29369)
[tests] enable automatic speech recognition pipeline tests on XPU  (#29308)
Correct zero division error in inverse sqrt scheduler (#28982)
Fix deprecated arg issue (#29372)
Fix llama + gemma accelete tests (#29380)
Support subfolder with `AutoProcessor` (#29169)
[`YOLOS`] Fix - return padded annotations (#29300)
üö®üö®[Whisper Tok] Update integration test (#29368)
[`Llama + AWQ`] fix `prepare_inputs_for_generation`  ü´† (#29381)
FIX [`quantization` / `ESM`] Fix ESM 8bit / 4bit with bitsandbytes (#29329)
Fix Base Model Name of LlamaForQuestionAnswering (#29258)
Expose `offload_buffers` parameter of `accelerate` to `PreTrainedModel.from_pretrained` method (#28755)
Fix @require_read_token in tests (#29367)
Patch YOLOS and others (#29353)
Avoid using uncessary `get_values(MODEL_MAPPING)` (#29362)
FIX [`CI`] `require_read_token` in the llama FA2 test (#29361)
FIX [`CI`]: Fix failing tests for peft integration (#29330)
FIX [`CI` / `starcoder2`] Change starcoder2 path to correct one for slow tests (#29359)
[i18n-zh] Sync source/zh/index.md (#29331)
Better SDPA unmasking implementation (#29318)
[CI] Quantization workflow (#29046)
check if position_ids exists before using it (#29306)
RoPE loses precision for Llama / Gemma + Gemma logits.float() (#29285)
Idefics: generate fix (#29320)
Disable Mixtral `output_router_logits` during inference (#29249)
[`Llama ROPE`] Fix torch export but also slow downs in forward (#29198)
[`T5 and Llama Tokenizer`] remove warning (#29346)
[`require_read_token`] fix typo (#29345)
Remove numpy usage from owlvit (#29326)
FIX [`Gemma` / `CI`] Make sure our runners have access to the model (#29242)
simplify get_class_in_module and fix for paths containing a dot (#29262)
Starcoder2 model - bis (#29215)
[i18n-zh] Translate fsdp.md into Chinese (#29305)
Fix a few typos in `GenerationMixin`'s docstring (#29277)
Token level timestamps for long-form generation in Whisper (#29148)
Add compatibility with skip_memory_metrics for mps device (#29264)
Use torch 2.2 for deepspeed CI (#29246)
[tests] enable benchmark unit tests on XPU  (#29284)
Fix `attn_implementation` documentation (#29295)
Image Feature Extraction docs (#28973)
Cleaner Cache `dtype` and `device` extraction for CUDA graph generation for quantizers compatibility (#29079)
Add generate kwargs to VQA pipeline (#29134)
GenerationConfig validate both constraints and force_words_ids (#29163)
Adding SegGPT (#27735)
Fixed Deformable Detr typo when loading cuda kernels for MSDA (#29294)
[i18n-zh] Translated task/asr.md into Chinese (#29233)
[i18n-vi] Translate README.md to Vietnamese (#29229)
üåê [i18n-ZH] Translate chat_templating.md into Chinese (#28790)
[i18n-zh] Translated torchscript.md into Chinese (#29234)
[docs] Spanish translation of tasks_explained.md (#29224)
Track each row separately for stopping criteria (#29116)
Generate: v4.38 removals and related updates (#29171)
Use `torch.bool` instead of `torch.int64` for non-persistant causal mask buffer (#29241)
Add feature extraction mapping for automatic metadata update (#28944)
Add `non_device_test` pytest mark to filter out non-device tests (#29213)
Use `DS_DISABLE_NINJA=1` (#29290)
Cache `is_vision_available` result (#29280)
Use torch 2.2 for daily CI (model tests) (#29208)
Allow remote code repo names to contain "." (#29175)
[`Doc`] update model doc qwen2 (#29238)
Improve _update_causal_mask performance (#29210)
Fix missing translation in README_ru (#29054)
fix(mlflow): check mlflow version to use the synchronous flag (#29195)
Fix `torch.compile` with `fullgraph=True` when `attention_mask` input is used (#29211)
[Mistral, Mixtral] Improve docs (#29084)
[Gemma] Fix eager attention (#29187)
Add training version check for AQLM quantizer. (#29142)
FIX [`Gemma`] Fix bad rebase with transformers main (#29170)
[ `gemma`] Adds support for Gemma üíé (#29167)
[`Maskformer`] safely get backbone config (#29166)
support SDPA Attention in stablelm (#29106)
`torch.compile` compatibility with `generate` + static cache (#29114)
üö® Llama: update rope scaling to match static cache changes (#29143)
v4.39.dev.0
[`pipeline`] Add pool option to image feature extraction pipeline (#28985)
Fix drop path being ignored in DINOv2 (#29147)
Added image_captioning version in es and included in toctree file (#29104)
Generate: missing generation config eos token setting in encoder-decoder tests (#29146)
Raise unused kwargs image processor (#29063)
[Phi] Add support for sdpa (#29108)
Save (circleci) cache at the end of a job (#29141)
Add support for fine-tuning CLIP-like models using contrastive-image-text example (#29070)
Revert low cpu mem tie weights (#29135)
[`Core tokenization`]  `add_dummy_prefix_space` option to help with latest issues (#28010)
FIX [`PEFT` / `Trainer` ] Handle better peft + quantized compiled models (#29055)
[`cuda kernels`] only compile them when initializing (#29133)
Generate: unset GenerationConfig parameters do not raise warning (#29119)
Llama: fix batched generation (#29109)
FIX [`bnb` / `tests`] Propagate the changes from #29092 to 4-bit tests (#29122)
Abstract image processor arg checks. (#28843)
FEAT [`Trainer` / `bnb`]: Add RMSProp from `bitsandbytes` to HF `Trainer` (#29082)
Move misplaced line (#29117)
[`gradient_checkpointing`] default to use it for torch 2.3 (#28538)
Fixed nll with label_smoothing to just nll (#28708)
storing & logging gradient norm in trainer (#27326)
Fix two tiny typos in `pipelines/base.py::Pipeline::_sanitize_parameters()`'s docstring (#29102)
Bnb test fix for different hardwares (#29066)
ENH: added new output_logits option to generate function (#28667)
[Docs] Add resources (#28705)
change version (#29097)
Fix a typo in `examples/pytorch/text-classification/run_classification.py` (#29072)
Fix the `bert-base-cased` tokenizer configuration test (#29105)
fix the post-processing link (#29091)
FIX [`bnb` / `tests`]: Fix currently failing bnb tests (#29092)
[`Awq`] Add peft support for AWQ (#28987)
[Docs] Spanish translation of task_summary.md (#28844)
Add chat support to text generation pipeline (#28945)
Fix trainer test wrt DeepSpeed + auto_find_bs (#29061)
Feature: Option to set the tracking URI for MLflowCallback. (#29032)
Honor trust_remote_code for custom tokenizers (#28854)
`auto_find_batch_size` isn't yet supported with DeepSpeed/FSDP. Raise error accrodingly. (#29058)
fix failing trainer ds tests (#29057)
fix num_assistant_tokens with heuristic schedule (#28759)
Support : Leverage Accelerate for object detection/segmentation models  (#28312)
Fix max_length criteria when using inputs_embeds (#28994)
Update important model list (#29019)
Update all references to canonical models (#29001)
add test marker to run all tests with @require_bitsandbytes (#28278)
Fix a tiny typo in `generation/utils.py::GenerateEncoderDecoderOutput`'s docstring (#29044)
Removed obsolete attribute setting for AQLM quantization. (#29034)
Patch to skip failing `test_save_load_low_cpu_mem_usage` tests (#29043)
FIX: Fix error with `logger.warning` + inline with recent refactor (#29039)
Fix copies between DETR and DETA (#29037)
DeformableDetrModel support fp16 (#29013)
Add cuda_custom_kernel in DETA (#28989)
Fix static generation when compiling!  (#28937)
[`CLeanup`] Revert SDPA attention changes that got in the static kv cache PR (#29027)
FIX [`Trainer` / tags]: Fix trainer + tags when users do not pass `"tags"` to `trainer.push_to_hub()` (#29009)
[TPU] Support PyTorch/XLA FSDP via SPMD (#28949)
Backbone kwargs in config (#28784)
Add tie_weights() to LM heads and set bias in set_output_embeddings() (#28948)
Mask Generation Task Guide (#28897)
Fix flaky test vision encoder-decoder generate (#28923)
Introduce AcceleratorConfig dataclass (#28664)
Set the dataset format used by `test_trainer` to float32 (#28920)
[`Doc`] Fix docbuilder - make `BackboneMixin` and `BackboneConfigMixin` importable from `utils`.  (#29002)
AQLM quantizer support (#28928)
Add SiglipForImageClassification and CLIPForImageClassification (#28952)
Add `StableLM` (#28810)
ENH [`AutoQuantizer`]: enhance trainer + not supported quant methods (#28991)
ENH: Do not pass warning message in case `quantization_config` is in config but not passed as an arg (#28988)
[`DETR`] Update the processing to adapt masks & bboxes to reflect padding (#28363)
Update configuration_llama.py: fixed broken link (#28946)
Static Cache: load models with MQA or GQA (#28975)
Add sudachi_projection option to BertJapaneseTokenizer (#28503)
[`NllbTokenizer`] refactor with added tokens decoder (#27717)
[i18n-de] Translate CONTRIBUTING.md to German (#28954)
[Docs] Add video section (#28958)
[Docs] Add language identifiers to fenced code blocks (#28955)
Clean up staging tmp checkpoint directory (#28848)
Always initialize tied output_embeddings if it has a bias term (#28947)
Updated requirements for image-classification samples: datasets>=2.14.0 (#28974)
Tests: tag `test_save_load_fast_init_from_base` as flaky (#28930)
[`pipelines`] updated docstring with vqa alias (#28951)
Convert `torch_dtype` as `str` to actual torch data type (i.e. "float16" ‚Ä¶to `torch.float16`) (#28208)
[Docs] Update README and default pipelines (#28864)
[Nougat] Fix pipeline (#28242)
[i18n-de] Translate README.md to German (#28933)
Fix type annotations on neftune_noise_alpha and fsdp_config TrainingArguments parameters (#28942)
Fix a wrong link to CONTRIBUTING.md section in PR template (#28941)
Fix max_position_embeddings default value for llama2 to 4096 #28241 (#28754)
[Docs] Fix broken links and syntax issues (#28918)
Support batched input for decoder start ids (#28887)
pass kwargs in stopping criteria list (#28927)
fix: torch.int32 instead of torch.torch.int32 (#28883)
Remove dead TF loading code (#28926)
[`Core generation`] Adds support for static KV cache (#27931)
Fix utf-8 yaml load for marian conversion to pytorch in Windows (#28618)
[Docs] Revert translation of '@slow' decorator (#28912)
[Docs] Fix placement of tilde character (#28913)
Add npu device for pipeline (#28885)
Update the cache number (#28905)
‚ö†Ô∏è Raise `Exception` when trying to generate 0 tokens ‚ö†Ô∏è (#28621)
Fix Keras scheduler import so it works for older versions of Keras (#28895)
fix Starcoder FA2 implementation (#28891)
fix: Fixed the documentation for `logging_first_step` by removing "evaluate" (#28884)
[Docs] Add missing language options and fix broken links (#28852)
Hotfix - make `torchaudio` get the correct version in `torch_and_flax_job` (#28899)
[Docs] Fix backticks in inline code and documentation links (#28875)
Explicit server error on gated model (#28894)
unpin torch (#28892)
Revert "[WIP] Hard error when ignoring tensors." (#28898)
Fix `FastSpeech2ConformerModelTest` and skip it on CPU (#28888)
Raise error when using `save_only_model` with `load_best_model_at_end` for DeepSpeed/FSDP (#28866)
Fix LongT5ForConditionalGeneration initialization of lm_head (#28873)
[Docs] Update project names and links in awesome-transformers (#28878)
Bump cryptography from 41.0.2 to 42.0.0 in /examples/research_projects/decision_transformer (#28879)
Adds LlamaForQuestionAnswering class in modeling_llama.py along with AutoModel Support  (#28777)
Do not use mtime for checkpoint rotation. (#28862)
ClearMLCallback enhancements: support multiple runs and handle logging better (#28559)
Image Feature Extraction pipeline (#28216)
Correct wav2vec2-bert inputs_to_logits_ratio (#28821)
[`Doc`] update contribution guidelines (#28858)
[WIP] Hard error when ignoring tensors. (#27484)
Ability to override clean_code_for_run (#28783)
[Docs] Fix bad doc: replace save with logging (#28855)
Support custom scheduler in deepspeed training (#26831)
Bump dash from 2.3.0 to 2.15.0 in /examples/research_projects/decision_transformer (#28845)
Mark `test_encoder_decoder_model_generate` for `vision_encoder_deocder` as flaky (#28842)
Reduce GPU memory usage when using FSDP+PEFT (#28830)
Use `-v` for `pytest` on CircleCI  (#28840)
fix / skip (for now) some tests before switch to torch 2.2 (#28838)
Fix issues caused by natten (#28834)
Add missing None check for hf_quantizer (#28804)
Explicitly check if token ID's are None in TFBertTokenizer constructor (#28824)
[Docs] Fix spelling and grammar mistakes (#28825)
[docs] HfQuantizer (#28820)
[docs] Backbone (#28739)
Add models from deit (#28302)
[docs] fix some bugs about parameter description (#28806)
enable graident checkpointing in DetaObjectDetection and add tests in Swin/Donut_Swin (#28615)
Add tip on setting tokenizer attributes (#28764)
Fix symbolic_trace with kv cache (#28724)
Make `is_torch_bf16_available_on_device` more strict (#28796)
Adding [T5/MT5/UMT5]ForTokenClassification (#28443)
[docs] Correct the statement in the docstirng of compute_transition_scores in generation/utils.py (#28786)
Split daily CI using 2 level matrix (#28773)
Add artifact name in job step to maintain job / artifact correspondence (#28682)
DeepSpeed: hardcode `torch.arange` dtype on `float` usage to avoid incorrect initialization (#28760)
Flax mistral (#26943)
Wrap Keras methods to support BatchEncoding (#28734)
canonical repos moves (#28795)
Resolve DeepSpeed cannot resume training with PeftModel (#28746)
[Whisper] Refactor forced_decoder_ids & prompt ids (#28687)
[`HFQuantizer`] Remove `check_packages_compatibility` logic (#28789)
don't initialize the output embeddings if we're going to tie them to input embeddings (#28192)
Prevent MLflow exception from disrupting training (#28779)
[`bnb`] Fix bnb slow tests (#28788)
Pin Torch to <2.2.0 (#28785)
Add tf_keras imports to prepare for Keras 3 (#28588)
Task-specific pipeline init args (#28439)
[`Backbone`] Use `load_backbone` instead of `AutoBackbone.from_config` (#28661)
Further pin pytest version (in a temporary way) (#28780)
Fix transformers.utils.fx compatibility with torch<2.0 (#28774)
Use Conv1d for TDNN (#25728)
[`HfQuantizer`] Move it to "Developper guides" (#28768)
`HfQuantizer` class for quantization-related stuff in `modeling_utils.py` (#26610)
Move CLIP _no_split_modules to CLIPPreTrainedModel (#27841)
Don't allow passing `load_in_8bit` and `load_in_4bit` at the same time (#28266)
Add French translation: french README.md (#28696)
Support saving only PEFT adapter in checkpoints when using PEFT + FSDP (#28297)
[Whisper] Make tokenizer normalization public (#28136)
Fix typo of `Block`. (#28727)
Mark test_constrained_beam_search_generate as flaky (#28757)
Pin pytest version <8.0.0 (#28758)
small doc update for CamemBERT (#28644)
Enable Gradient Checkpointing in Deformable DETR (#28686)
PatchtTST and PatchTSMixer fixes (#28083)
[Docs] Fix Typo in English & Japanese CLIP Model Documentation (TMBD -> TMDB) (#28751)
Fix input data file extension in examples (#28741)
Fix `DepthEstimationPipeline`'s docstring (#28733)
Add serialization logic to pytree types (#27871)
[`Siglip`] protect from imports if sentencepiece not installed (#28737)
Generate: deprecate old src imports (#28607)
Falcon: removed unused function (#28605)
[Flax] Update no init test for Flax v0.7.1 (#28735)
[docs] Fix datasets in guides (#28715)
Unpin pydantic (#28728)
fix: suppress `GatedRepoError` to use cache file (fix #28558). (#28566)
Stop confusing the TF compiler with ModelOutput objects (#28712)
Fix `weights_only` (#28725)
Initialize _tqdm_active with hf_hub_utils.are_progress_bars_disabled(‚Ä¶ (#28717)
[`docs`] Update preprocessing.md (#28719)
fix: corrected misleading log message in save_pretrained function (#28699)
support PeftMixedModel signature inspect (#28321)
Fix duplicate & unnecessary flash attention warnings (#28557)
Don't fail when `LocalEntryNotFoundError` during `processor_config.json` loading (#28709)
[`docs`] Improve visualization for vertical parallelism (#28583)
[`Vilt`] align input and model dtype in the ViltPatchEmbeddings forward pass  (#28633)
Update question_answering.md (#28694)
Improve Backbone API docs (#28666)
[`chore`] Add missing space in warning (#28695)
Add Depth Anything (#28654)
[docs] Fix doc format (#28684)
improve efficient training on CPU documentation (#28646)
Improved type hinting for all attention parameters (#28479)
[docs] DeepSpeed (#28542)
Add back in generation types (#28681)
Use save_safetensor to disable safe serialization for XLA (#28669)
 Exclude the load balancing loss of padding tokens in Mixtral-8x7B (#28517)
Update README_es.md (#28612)
fix a hidden bug of `GenerationConfig`, now the `generation_config.json` can be loaded successfully (#28604)
Remove deprecated eager_serving fn (#28665)
Support single token decode for `CodeGenTokenizer` (#28628)
add dataloader prefetch factor in training args and trainer (#28498)
Fix windows err with checkpoint race conditions (#28637)
`tensor_size` - fix copy/paste error msg typo (#28660)
Enable instantiating model with pretrained backbone weights (#28214)
Enable safetensors conversion from PyTorch to other frameworks without the torch requirement (#27599)
integrations: fix DVCLiveCallback model logging (#28653)
get default device through `PartialState().default_device` as it has been officially released (#27256)
Fix phi model doc checkpoint (#28581)
[`SigLIP`] Only import tokenizer if sentencepiece available (#28636)
Update image_processing_deformable_detr.py (#28561)
[`GPTNeoX`] Fix GPTNeoX + Flash Attention 2 issue (#28645)
[`Llava`] Update convert_llava_weights_to_hf.py script (#28617)
Fix lr_scheduler in no_trainer training scripts (#27872)
Add config tip to custom model docs (#28601)
Avoid root logger's level being changed (#28638)
Add missing key to TFLayoutLM signature (#28640)
Fix id2label assignment in run_classification.py (#28590)
[`GPTNeoX`] Fix BC issue with 4.36 (#28602)
Fix auxiliary loss related code in transformers (#28406)
RWKV: raise informative exception when attempting to manipulate `past_key_values` (#28600)
Fix `_speculative_sampling` implementation (#28508)
Allow add_tokens for ESM (#28535)
[`Llava`] Fix convert_llava_weights_to_hf.py script (#28570)
[SigLIP] Don't pad by default (#28578)
Fix wrong xpu device in DistributedType.MULTI_XPU mode (#28386)
[Whisper] Finalize batched SOTA long-form generation (#27658)
feat: Sequential beam search (#26304)
Add w2v2bert to pipeline (#28585)
v4.38.dev.0
Don't save `processor_config.json` if a processor has no extra attribute  (#28584)
Making CTC training example more general (#28582)
[Whisper] Fix audio classification with weighted layer sum (#28563)
[Whisper Tok] Move token ids to CPU when computing offsets (#28485)
[ASR Pipe] Update init to set model type and subsequently call parent init method (#28486)
Fix the documentation checkpoint for xlm-roberta-xl (#28567)
Use `LoggingLevel` context manager in 3 tests (#28575)
Add new meta w2v2-conformer BERT-like model (#28165)
chore: Fix multiple typos (#28574)
[`Core Tokenization`] Support a fix for spm fast models (#26678)
Use `weights_only` only if torch >= 1.13 (#28506)
Save `Processor` (#27761)
Fix Switch Transformers When sparse_step = 1 (#28564)
Allow to train dinov2 with different dtypes like bf16 (#28504)
Fix SDPA tests (#28552)
Add qwen2 (#28436)
Fixes default value of `softmax_scale` in `PhiFlashAttention2`. (#28537)
symbolic_trace: add past_key_values, llama, sdpa support (#28447)
[Makefile] Exclude research projects from format (#28551)
Config: warning when saving generation kwargs in the model config (#28514)
Add is_model_supported for fx (#28521)
Clearer error for SDPA when explicitely requested (#28006)
[`SpeechT5Tokenization`]  Add copied from and fix the `convert_tokens_to_string` to match the fast decoding scheme (#28522)
[`TokenizationRoformerFast`]  Fix the save and loading (#28527)
[ `TokenizationUtils`] Fix `add_special_tokens` when the token is already there (#28520)
Fix/speecht5 bug (#28481)
Fix mismatching loading in from_pretrained with/without accelerate (#28414)
Improving Training Performance and Scalability Documentation (#28497)
Remove `task` arg in `load_dataset` in image-classification example (#28408)
SiLU activation wrapper for safe importing (#28509)
improve dev setup comments and hints (#28495)
fix: sampling in flax keeps EOS (#28378)
Generate: consolidate output classes (#28494)
Add a use_safetensors arg to TFPreTrainedModel.from_pretrained() (#28511)
Fixed minor typos (#28489)
[GPTQ] Fix test (#28018)
Tokenizer kwargs in textgeneration pipe (#28362)
Add the XPU device check for pipeline mode (#28326)
[`core`/ FEAT] Add the possibility to push custom tags using `PreTrainedModel` itself (#28405)
Don't set `finetuned_from` if it is a local path  (#28482)
[`chore`] Update warning text, a word was missing (#28017)
Fix paths to AI Sweden Models reference and model loading (#28423)
Generate: fix candidate device placement (#28493)
Adding Prompt lookup decoding (#27775)
Change progress logging to once across all nodes (#28373)
Fix docstrings and update docstring checker error message (#28460)
TF: purge `TFTrainer` (#28483)
Generate: refuse to save bad generation config files (#28477)
Docs: add model paths (#28475)
Generate: deprecate old public functions (#28478)
Fix torch.ones usage in xlnet (#28471)
Bump jinja2 from 2.11.3 to 3.1.3 in /examples/research_projects/decision_transformer (#28457)
[`Mixtral` / `Awq`] Add mixtral fused modules for Awq  (#28240)
Update metadata loading for oneformer (#28398)
Mark two logger tests as flaky (#28458)
[`Awq`] Add llava fused modules support (#28239)
Fix broken link on page (#28451)
Fix docstring checker issues with PIL enums (#28450)
Doc (#28431)
Byebye torch 1.10 (#28207)
Fix load balancing loss func for mixtral (#28256)
Correctly resolve trust_remote_code=None for AutoTokenizer (#28419)
[Phi] Extend implementation to use GQA/MQA. (#28163)
Optionally preprocess segmentation maps for MobileViT (#28420)
Set `cache_dir` for `evaluate.load()` in example scripts (#28422)
Fix docker file (#28452)
Use python 3.10 for docbuild (#28399)
Optimize the speed of the truncate_sequences function. (#28263)
Enable multi-label image classification in pipeline (#28433)
Assitant model may on a different device (#27995)
[Whisper] Fix slow test (#28407)
[docstring] Fix docstring for ErnieConfig, ErnieMConfig (#27029)
Fix load correct tokenizer in Mixtral model documentation (#28437)
Fix for checkpoint rename race condition (#28364)
update docs to add the `phi-2` example (#28392)
CI: limit natten version (#28432)
Fix number of models in README.md (#28430)
Support `DeepSpeed` when using auto find batch size (#28088)
Skip now failing test in the Trainer tests (#28421)
[BUG] BarkEosPrioritizerLogitsProcessor eos_token_id use list, tensor size mismatch (#28201)
Bump fonttools from 4.31.1 to 4.43.0 in /examples/research_projects/decision_transformer (#28417)
Use mmap option to load_state_dict (#28331)
Fix `_merge_input_ids_with_image_features` for llava model (#28333)
Fix initialization for missing parameters in `from_pretrained` under ZeRO-3 (#28245)
fix auxiliary loss training in DetrSegmentation (#28354)
[SDPA] Make sure attn mask creation is always done on CPU (#28400)
update warning for image processor loading (#28209)
Add SigLIP (#26522)
Add segmentation map processing to SAM Image Processor (#27463)
Remove shell=True from subprocess.Popen to Mitigate Security Risk (#28299)
[AttentionMaskConverter] fix sdpa unmask unattended (#28369)
Bugfix / ffmpeg input device (mic) not working on Windows (#27051)
remove two deprecated function (#28220)
Fix building alibi tensor when num_heads is not a power of 2 (#28380)
Enhancing Code Readability and Maintainability with Simplified Activation Function Selection. (#28349)
[Phi2] Add support for phi2 models (#28211)
chore: Fix typo s/exclusivelly/exclusively/ (#28361)
Update VITS modeling to enable ONNX export (#28141)
 fix FA2 when using quantization for remaining models (#28341)
[DETA] Improvement and Sync from DETA especially for training (#27990)
Fix pos_mask application and update tests accordingly (#27892)
Don't check the device when device_map=auto (#28351)
README: install transformers from conda-forge channel (#28313)
Fix error in M4T feature extractor (#28340)
enable training mask2former and maskformer for transformers trainer (#28277)
[docs] Sort es/toctree.yml | Translate performance.md (#28262)
Translate contributing.md into Chinese (#28243)
Remove token_type_ids from model_input_names (like #24788) (#28325)
Add FastSpeech2Conformer (#23439)
fix documentation for zero_shot_object_detection (#28267)
Bump tj-actions/changed-files from 22.2 to 41 in /.github/workflows (#28311)
Remove fast tokenization warning in Data Collators (#28213)
[Whisper] Fix errors with MPS backend introduced by new code on word-level timestamps computation (#28288)
fix bug:divide by zero in _maybe_log_save_evaluate() (#28251)
Fix trainer saving safetensors: metadata is None (#28219)
Update docs around mixing hf scheduler with deepspeed optimizer (#28223)
small typo (#28229)
fix FA2 when using quantization (#28203)
[`Awq`] Enable the possibility to skip quantization for some target modules (#27950)
[`Llava`] Fix llava index errors (#28032)
update the logger message with accordant weights_file_name (#28181)
Fixing visualization code for object detection to support both types of bounding box.  (#27842)
[Whisper] Fix word-level timestamps with bs>1 or num_beams>1 (#28114)
Drop `feature_extractor_type` when loading an image processor file (#28195)
Fix the check of models supporting FA/SDPA not run (#28202)
Bug: `training_args.py` fix missing import with accelerate with version `accelerate==0.20.1` (#28171)
Add Swinv2 backbone (#27742)
Fix: [SeamlessM4T - S2TT] Bug in batch loading of audio in torch.Tensor format in the SeamlessM4TFeatureExtractor class (#27914)
Fix ONNX export for causal LM sequence classifiers by removing reverse indexing (#28144)
Update `docs/source/en/perf_infer_gpu_one.md` (#28198)
[`Docs`]¬†Add 4-bit serialization docs (#28182)
Update YOLOS slow test values (#28187)
Fix slow backbone tests - out_indices must match stage name ordering (#28186)
Even more TF test fixes (#28146)
[`Mixtral` & `Mistral`] Add support for sdpa (#28133)
[Whisper] Use torch for stft if available (#26119)
Fix `input_embeds` docstring in encoder-decoder architectures (#28168)
[bnb] Let's make serialization of 4bit models possible  (#26037)
disable test_retain_grad_hidden_states_attentions on SeamlessM4TModelWithTextInputTest (#28169)
Fix yolos resizing (#27663)
Generate: fix speculative decoding (#28166)
[docs] Trainer docs (#28145)
Align backbone stage selection with out_indices & out_features (#27606)
Update FA2 exception msg to point to hub discussions (#28161)
Avoid unnecessary warnings when loading `CLIPConfig` (#28108)
Fix weights not properly initialized due to shape mismatch (#28122)
move code to Trainer.evaluate to enable use of that function with multiple datasets (#27844)
[gpt-neox] Add attention_bias config to support model trained without attention biases (#28126)
Fix FA2 integration (#28142)
Remove deprecated CPU dockerfiles (#28149)
[docs] Fix mistral link in mixtral.md (#28143)
Update modeling_utils.py (#28127)
[`Mixtral`] Fix loss + nits (#28115)
Generate: speculative decoding (#27979)
Update split string in doctest to reflect #28087 (#28135)
When save a model on TPU, make a copy to be moved to CPU (#27993)
[Doc] Fix token link in What ü§ó Transformers can do (#28123)
Fix a typo in tokenizer documentation (#28118)
[docs] General doc fixes (#28087)
Fix indentation error - semantic_segmentation.md (#28117)
More TF fixes (#28081)
Remove warning if `DISABLE_TELEMETRY` is used (#28113)
Disable jitter noise during evaluation in SwitchTransformers (#28077)
fix ConversationalPipeline docstring (#28091)
in peft finetune, only the trainable parameters need to be saved (#27825)
Spelling correction (#28110)
[`Llava` / `Vip-Llava`] Add SDPA into llava (#28107)
Fix the deprecation warning of _torch_pytree._register_pytree_node (#27803)
4D `attention_mask` support (#27539)
fix resuming from ckpt when using FSDP with FULL_STATE_DICT (#27891)
[docs] MPS (#28016)
[docs] Trainer (#27986)
Fix Vip-llava docs (#28085)
Fix wrong examples in llava usage. (#28020)
Fix `low_cpu_mem_usage` Flag Conflict with DeepSpeed Zero 3 in `from_pretrained` for Models with `keep_in_fp32_modules`" (#27762)
Update fixtures-image-utils (#28080)
Fix bug for checkpoint saving on multi node training setting (#28078)
make torch.load a bit safer (#27282)
Make GPT2 traceable in meta state (#28054)
[LLaVa] Add past_key_values to _skip_keys_device_placement to fix multi-GPU dispatch (#28051)
Skip M4T `test_retain_grad_hidden_states_attentions`  (#28060)
[`Mixtral`]¬†update conversion script to reflect new changes (#28068)
doc: Correct spelling mistake (#28064)
Remove SpeechT5 deprecated argument (#28062)
[Flax LLaMA] Fix attn dropout (#28059)
[Flax BERT] Update deprecated 'split' method (#28012)
[`Modeling` / `Mixtral`] Fix GC + PEFT issues with Mixtral (#28061)
[`FA-2`] Fix fa-2 issue when passing `config` to `from_pretrained` (#28043)
Remove warning when Annotion enum is created (#28048)
Replace build() with build_in_name_scope() for some TF tests (#28046)
Proper build() methods for TF (#27794)
[Seamless] Fix links in docs (#27905)
Generate: Mistral/Mixtral FA2 cache fix when going beyond the context window (#28037)
Fixed spelling error in T5 tokenizer warning message (s/thouroughly/t‚Ä¶ (#28014)
Fix languages covered by M4Tv2 (#28019)
SeamlessM4T: `test_retain_grad_hidden_states_attentions` is flaky (#28035)
Generate: assisted decoding now uses `generate` for the assistant (#28030)
Fix AMD push CI not triggered (#28029)
[`core` / `modeling`] Fix training bug with PEFT + GC (#28031)
[`SeamlessM4TTokenizer`]  Safe import (#28026)
well well well (#28011)
add `modules_in_block_to_quantize` arg in GPTQconfig (#27956)
Add model_docs from cpmant.md to derformable_detr.md (#27884)
Dev version
[Doc] Spanish translation of glossary.md (#27958)
Fix bug with rotating checkpoints (#28009)
[`CI slow`] Fix expected values (#27999)
Fix PatchTSMixer slow tests (#27997)
Adds VIP-llava to transformers (#27932)
[`Whisper`] raise better errors (#27971)
[`Tokenizer Serialization`] Fix the broken serialisation  (#27099)
fix typo in dvclive callback (#27983)
[doc] fix typo (#27981)
Fix SDPA correctness following torch==2.1.2 regression (#27973)
Better key error for AutoConfig (#27976)
Fix link in README.md of Image Captioning (#27969)
Hot-fix-mixstral-loss (#27948)
Generate: `assisted_decoding` now accepts arbitrary candidate generators (#27750)
fixed typos (issue 27919) (#27920)
Support PeftModel signature inspect (#27865)
[docs] Fused AWQ modules (#27896)
Update bounding box format everywhere (#27944)
[`Mixtral`] Change mistral op order (#27955)
fix no sequence length models error (#27522)
Fix for stochastic depth decay rule in the TimeSformer implementation (#27875)
fix bug in mask2former: cost matrix is infeasible (#27897)
Fix a couple of typos and add an illustrative test (#26941)
Add deepspeed test to amd scheduled CI (#27633)
Fix AMD scheduled CI not triggered (#27951)
In PreTrainedTokenizerBase add missing word in error message (#27949)
Fix parameter count in readme for mixtral 45b (#27945)
Update import message (#27946)
Fix test for auto_find_batch_size on multi-GPU (#27947)
Docs for AutoBackbone & Backbone (#27456)
use logger.warning_once to avoid massive outputs (#27428)
Fix PatchTSMixer Docstrings (#27943)
[`Add Mixtral`] Adds support for the Mixtral MoE (#27942)
[`from_pretrained`] Make from_pretrained fast again (#27709)
Fix SDPA dispatch & make SDPA CI compatible with torch<2.1.1 (#27940)
[LLaVa] Some improvements (#27895)
Fix `SeamlessM4Tv2ModelIntegrationTest` (#27911)
Skip `UnivNetModelTest::test_multi_gpu_data_parallel_forward` (#27912)
[BEiT] Fix test (#27934)
[DETA] fix backbone freeze/unfreeze function (#27843)
Fix typo (#27918)
[integration] Update Ray Tune integration for Ray 2.7 (#26499)
[CLAP] Replace hard-coded batch size to enable dynamic ONNX export (#27790)
F.scaled_dot_product_attention support (#26572)
Generate: SinkCache can handle iterative prompts (#27907)
fix typo in image_processing_blip.py Wwhether -> Whether (#27899)
[Doc] Spanish translation of pad_truncation.md (#27890)
Allow `resume_from_checkpoint` to handle `auto_find_batch_size` (#27568)
fix llava (#27909)
Llama conversion script: adjustments for Llama Guard (#27910)
Fix 2 tests in `FillMaskPipelineTests` (#27889)
Fix `notification_service.py` (#27903)
mark `test_initialization` as flaky in 2 model tests (#27906)
Fix CLAP converting script (#27153)
Fix remaining issues in beam score calculation (#27808)
Fix beam score calculation issue for Tensorflow version (#27814)
fix: non-atomic checkpoint save (#27820)
Added passing parameters to "reduce_lr_on_plateau" scheduler (#27860)
Fix: Raise informative exception when `prefix_allowed_tokens_fn` return empty set of tokens (#27797)
[‚ö†Ô∏è removed a default argument] Make `AttentionMaskConverter` compatible with `torch.compile(..., fullgraph=True)` (#27868)
Generate: New `Cache` abstraction and Attention Sinks support (#26681)
Translate `model_doc` files from `clip` to `cpm`  to JP (#27774)
Updates the distributed CPU training documentation to add instructions for running on a Kubernetes cluster (#27780)
[docs] Custom semantic segmentation dataset (#27859)
Generate: All logits processors are documented and have examples (#27796)
Fix TF loading PT safetensors when weights are tied (#27490)
Show new failing tests in a more clear way in slack report (#27881)
Fix device of masks in tests (#27887)
update version of warning notification for `get_default_device` to v4.38 (#27848)
update `create_model_card` to properly save peft details when using Trainer with PEFT (#27754)
Allow `# Ignore copy` (#27328)
[`Llava`]¬†Add Llava to transformers (#27662)
fix: fix gradient accumulate step for learning rate (#27667)
[`FA-2`] Add Flash Attention to `Phi` (#27661)
[i18n-fr] Translate autoclass tutorial to French (#27659)
Fix bug of _prepare_4d_attention_mask (#27847)
Add Llama Flax Implementation (#24587)
Fix beam score calculation issue for JAX version (#27816)
Translating en/model_doc folder docs to Japanese(from `blip` to `clap`) üáØüáµ (#27673)
[`Flash Attention 2`] Add flash attention 2 for GPT-Neo-X (#26463)
Avoid class attribute `_keep_in_fp32_modules` being modified (#27867)
removed the delete doc workflows (#27852)
Update CUDA versions for DeepSpeed (#27853)
[`Docs`] Update broken image on fused modules (#27856)
Documentation: Spanish translation of perplexity.mdx (#27807)
fix(whisper): mutable generation config (#27833)
Update `VitDetModelTester.get_config` to use `pretrain_image_size` (#27831)
‚ö†Ô∏è [VitDet] Fix test (#27832)
[Time series] Add PatchTSMixer (#26247)
Move tensors to same device to enable IDEFICS naive MP training (#27746)
[`ClipVision`] `accelerate` support for clip-vision (#27851)
Generate: Update VisionEncoderDecoder test value (#27850)
Faster generation using AWQ + Fused modules (#27411)
Make image processors more general (#27690)
pin `ruff==0.1.5` (#27849)
Translate `en/tasks` folder docs to Japanese üáØüáµ (#27098)
translate internal folder files to chinese (#27638)
[Seamless v2] Add FE to auto mapping (#27829)
Disallow `pickle.load` unless `TRUST_REMOTE_CODE=True` (#27776)
restructure AMD scheduled CI (#27743)
single word should be set to False (#27738)
[Hot-Fix][XLA] Re-enable broken _tpu_save for XLATensors (#27799)
Flash Attention 2 support for RoCm (#27611)
Added test cases for rembert refering to albert and reformer test_tok‚Ä¶ (#27637)
[Whisper] Fix doctest in timestamp logits processor (#27795)
[Seamless v1] Link to v2 docs (#27827)
Keypoints 0.0 are confusing ../transformers/models/detr/image_processing_detr.py which are fixed (#26250)
Fix `Owlv2ModelIntegrationTest::test_inference_object_detection` (#27793)
Fix `TvpModelIntegrationTests` (#27792)
[`ModelOnTheFlyConversionTester`] Mark as slow for now (#27823)
Add `persistent_workers` parameter to `TrainingArguments` (#27189)
Fix typo in max_length deprecation warnings (#27788)
Improve forward signature test (#27729)
[JAX] Replace uses of jax.devices("cpu") with jax.local_devices(backend="cpu") (#27593)
[MusicGen] Fix audio channel attribute (#27440)
Better error message for bitsandbytes import  (#27764)
Make using safetensors files automated. (#27571)
Fixes for PatchTST Config (#27777)
[i18n-fr] Translate installation to French (#27657)
[SeamlessM4Tv2] Fix links in README (#27782)
Fix unsupported setting of self._n_gpu in training_args on XPU devices (#27716)
Add SeamlessM4T v2 (#27779)
Generate: `GenerationConfig` throws an exception when `generate` args are passed (#27757)
uses dvclive_test mode in examples/pytorch/test_accelerate_examples.py (#27763)
Remove `check_runner_status.yml` (#27767)
Fix precision errors from casting rotary parameters to FP16 with AMP (#27700)
[Time series] Add patchtst (#27581)
[docs] Quantization (#27641)
Docs: Fix broken cross-references, i.e. `~transformer.` -> `~transformers.` (#27740)
CLVP Fixes (#27547)
Trigger corresponding pipeline tests if `tests/utils/tiny_model_summary.json` is modified (#27693)
Enforce pin memory disabling when using cpu only (#27745)
Add madlad-400 MT models (#27471)
Log a warning in `TransfoXLTokenizer.__init__` (#27721)
Update tiny model creation script (#27674)
Add BeitBackbone (#25952)
Fix AMD Push CI not triggered (#27732)
Fixed passing scheduler-specific kwargs via TrainingArguments lr_scheduler_kwargs (#27595)
Translate  `en/model_doc` to JP (#27264)
translation main-class files to chinese (#27588)
Update chat template warnings/guides (#27634)
docs: replace torch.distributed.run by torchrun (#27528)
Fix owlv2 code snippet (#27698)
Modify group_sub_entities in TokenClassification Pipeline to support label with "-" (#27325)
Update forward signature test for vision models (#27681)
fix assisted decoding assistant model inputs (#27503)
Fix oneformer instance segmentation RuntimeError (#27725)
Fix mistral generate for long prompt / response (#27548)
Reorder the code on the Hub to explicit that sharing on the Hub isn't a requirement (#27691)
fix warning (#27689)
Fix Past CI (#27696)
Fix sliding_window hasattr in Mistral (#27041)
Fix `TVPModelTest` (#27695)
Successfully Resolved The ZeroDivisionError Exception. (#27524)
Reflect RoCm support in the documentation (#27636)
[`DocString`] Support a revision in the docstring `add_code_sample_docstrings` to facilitate integrations (#27645)
Fix semantic error in evaluation section (#27675)
Docs/Add conversion code to the musicgen docs (#27665)
Fix typo in warning message (#27055)
Deprecate `TransfoXL` (#27607)
Skip pipeline tests for 2 models for now (#27687)
Refactoring Trainer, adds `save_only_model` arg and simplifying FSDP integration (#27652)
Update tiny model summary file (#27388)
[DPT, Dinov2] Add resources (#27655)
Update TVP arxiv link (#27672)
Extended semantic segmentation to image segmentation (#27039)
[`FA2`] Add flash attention for opt (#26414)
update d_kv'annotation in mt5'configuration (#27585)
update Openai API call method (#27628)
Add UnivNet Vocoder Model for Tortoise TTS Diffusers Integration (#24799)
[Whisper] Add sequential longform decoding (#27492)
Fix `max_steps` documentation regarding the end-of-training condition (#27624)
Simplify the implementation of jitter noise in moe models (#27643)
[`dependency`] update pillow pins (#27409)
Fix `resize_token_embeddings` (#26861) (#26865)
Harmonize HF environment variables + other cleaning (#27564)
Explicitely specify `use_cache=True` in Flash Attention tests (#27635)
TVP model (#25856)
remove the deprecated method `init_git_repo` (#27617)
Fix tracing dinov2 (#27561)
Fix flash attention bugs with Mistral and Falcon (#27625)
Add RoCm scheduled CI & upgrade RoCm CI to PyTorch 2.1 (#26940)
Idefics: Fix information leak with cross attention gate in modeling (#26839)
Generate: Update docs regarding reusing `past_key_values` in `generate` (#27612)
[ConvNext] Improve backbone (#27621)
[`core` / `gradient_checkpointing`] add support for old GC method (#27610)
dvclive callback: warn instead of fail when logging non-scalars (#27608)
Fix torch.fx import issue for torch 1.12 (#27570)
Update Korean tutorial for using LLMs, and refactor the nested conditional statements in hr_argparser.py (#27489)
[Whisper] Add `large-v3` version support (#27336)
timm to pytorch conversion for vit model fix (#26908)
[`FA-2`] Add fa2 support for `from_config` (#26914)
[ examples] fix loading jsonl with load dataset in run translation example (#26924)
docs: fix 404 link (#27529)
Add `convert_hf_to_openai.py` script to Whisper documentation resources (#27590)
Fix idx2sym not loaded from pretrained vocab file in Transformer XL (#27589)
Adding leaky relu in dict ACT2CLS (#27574)
Fix broken distilbert url (#27579)
translate deepspeed.md to chinese (#27495)
Broken links fixed related to datasets docs (#27569)
fixed broken link (#27560)
Generate: update compute transition scores doctest (#27558)
Generate: fix flaky tests (#27543)
Fix AMD CI not showing GPU (#27555)
Skip some fuyu tests (#27553)
translate Trainer.md to chinese (#27527)
Updated albert.md doc for ALBERT model (#27223)
Generate: improve assisted generation tests (#27540)
[`Styling`] stylify using ruff (#27144)
Disable docker image build job `latest-pytorch-amd` for now (#27541)
Raise error when quantizing a quantized model (#27500)
Set `usedforsecurity=False` in hashlib methods (FIPS compliance) (#27483)
Revert "add attention_mask and position_ids in assisted model" (#27523)
Update the TF pin for 2.15 (#27375)
docs: add docs for map, and add num procs to load_dataset (#27520)
[`pytest`] Avoid flash attn test marker warning (#27509)
Support ONNX export for causal LM sequence classifiers (#27450)
translate model.md to chinese (#27518)
Fix offload disk for loading derivated model checkpoint into base model (#27253)
Fix bug for T5x to PyTorch convert script with varying encoder and decoder layers (#27448)
Incorrect setting for num_beams in translation and summarization examples (#27519)
Fixing the failure of models without max_position_embeddings attribute. (#27499)
Translating `en/model_doc` docs to Japanese. (#27401)
Fix wav2vec2 params (#27515)
[ `PretrainedConfig`] Improve messaging  (#27438)
üö®üö® Fix beam score calculation issue for decoder-only models (#27351)
[`tokenizers`] update `tokenizers` version pin (#27494)
Make some jobs run on the GitHub Actions runners (#27512)
[`CircleCI`] skip test_assisted_decoding_sample for everyone (#27511)
Update spelling mistake (#27506)
[Table Transformer] Add Transformers-native checkpoints (#26928)
[Fuyu] Add tests (#27001)
[`CI-test_torch`] skip test_tf_from_pt_safetensors and `test_assisted_decoding_sample`  (#27508)
Track the number of tokens seen to metrics (#27274)
Update processor mapping for hub snippets (#27477)
Have seq2seq just use gather (#27025)
Minor type annotation fix (#27276)
Generate: `GenerationConfig.from_pretrained` can return unused kwargs (#27488)
Update and reorder docs for chat templates (#27443)
Generate: fix `ExponentialDecayLengthPenalty` doctest (#27485)
translate hpo_train.md and perf_hardware.md to chinese (#27431)
Revert "[time series] Add PatchTST (#25927)" (#27486)
[Whisper] Fix pipeline test (#27442)
Clap processor: remove wasteful np.stack operations (#27454)
Add speecht5 batch generation and fix wrong attention mask when padding (#25943)
Fix M4T weights tying (#27395)
[`CI-test_torch`] skip `test_tf_from_pt_safetensors` for 4 models (#27481)
[`Peft`] `modules_to_save` support for peft integration (#27466)
Fix FA2 import + deprecation cycle  (#27330)
[time series] Add PatchTST (#25927)
Fixed typo in pipelines.md documentation (#27455)
Perf torch compile (#27422)
[`AWQ` ] Addresses TODO for awq tests (#27467)
Fix Falcon tokenizer loading in pipeline (#27316)
Add version check for Jinja (#27403)
Add DINOv2 depth estimation (#26092)
Install `python-Levenshtein` for `nougat` in CI image (#27465)
Fix docstring for `gradient_checkpointing_kwargs` (#27470)
OWLv2: bug fix in post_process_object_detection() when using cuda device (#27468)
Fix `from_pt` flag when loading with `safetensors` (#27394)
Default to msgpack for safetensors (#27460)
[`Llama + Mistral`] Add attention dropout (#27315)
Remove-auth-token (#27060)
Fixed typo in error message (#27461)
Fix some Wav2Vec2 related models' doctest (#27462)
Fix line ending in `utils/not_doctested.txt` (#27459)
Make `examples_torch_job` faster (#27437)
Normalize floating point cast (#27249)
Add Phi-1 and Phi-1_5 (#26170)
At most 2 GPUs for CI (#27435)
[`AttentionMaskConverter`] ]Fix-mask-inf (#27114)
Add CLVP (#24745)
update Bark FA2 docs (#27400)
[`Quantization`] Add str to enum conversion for AWQ (#27320)
add attention_mask and position_ids in assisted model (#26892)
Run all tests if `circleci/create_circleci_config.py` is modified (#27413)
Fix `Owlv2` checkpoint name and a default value in `Owlv2VisionConfig` (#27402)
remove failing tests and clean FE files (#27414)
Fix RequestCounter to make it more future-proof (#27406)
Final fix of the accelerate installation issue (#27408)
Use editable install for git deps (#27404)
Fix fuyu checkpoint repo in `FuyuConfig` (#27399)
use `pytest.mark` directly (#27390)
Adds dvclive callback (#27352)
device-agnostic deepspeed testing (#27342)
Skip failing cache call tests (#27393)
Put doctest options back to `pyproject.toml` (#27366)
Change thresh in test (#27378)
[`CodeLlamaTokenizer`] Nit, update __init__ to make sure the AddedTokens are not normalized because they are special (#27359)
Smangrul/fix failing ds ci tests (#27358)
translate debugging.md to chinese (#27374)
Update deprecated `torch.range` in `test_modeling_ibert.py` (#27355)
Add Flash Attention 2 support to Bark (#27364)
translate big_models.md and performance.md to chinese (#27334)
Fix tiny model script: not using `from_pt=True` (#27372)
[Flax Whisper] large-v3 compatibility (#27360)
Remove unused param from example script tests (#27354)
Translate index.md to Turkish (#27093)
MusicGen Update (#27084)
Fix `Kosmos-2` device issue (#27346)
Fix example tests from failing (#27353)
moving example of benchmarking to legacy dir (#27337)
Add numpy alternative to FE using torchaudio (#26339)
translate model_sharing.md and llm_tutorial.md to chinese (#27283)
translate the en tokenizer_summary.md to Chinese (#27291)
Allow scheduler parameters (#26480)
FIx Bark batching feature (#27271)
[`Whisper`]  Nit converting the tokenizer (#27349)
Remove padding_masks from `gpt_bigcode`. (#27348)
Resolve AttributeError by utilizing device calculation at the start of the forward function (#27347)
Remove a redundant variable. (#27288)
[`Whisper`]  Add conversion script for the tokenizer  (#27338)
[`FA2`] Add flash attention for `GPT-Neo` (#26486)
Fix Whisper Conversion Script: Correct decoder_attention_heads and _download function (#26834)
Generate: skip tests on unsupported models instead of passing (#27265)
Fix autoawq docker image (#27339)
[Whisper] Block language/task args for English-only (#27322)
[docs] fixed links with 404 (#27327)
Fix `Kosmos2Processor` batch mode (#27323)
Fix VideoMAEforPretrained dtype error (#27296)
Update sequence_classification.md (#27281)
[`PretrainedTokenizer`] add some of the most important functions to the doc (#27313)
enable memory tracker metrics for npu (#27280)
Remove an unexpected argument for FlaxResNetBasicLayerCollection (#27272)
Update doctest workflow file (#27306)
Fix daily CI image build (#27307)
Fix tokenizer export for LLamaTokenizerFast (#27222)
translate run_scripts.md to chinese (#27246)
translate autoclass_tutorial to chinese (#27269)
[`FA2`] Add flash attention for for `DistilBert` (#26489)
[Docs] Model_doc structure/clarity improvements (#26876)
[`Docs` / `SAM` ] Reflect correct changes to run inference without OOM  (#27268)
Fix switch transformer mixed precision issue (#27220)
Update the ConversationalPipeline docstring for chat templates (#27250)
[docs] Custom model doc update (#27213)
Avoid many failing tests in doctesting (#27262)
[`PEFT` / `Tests` ] Fix peft integration failing tests (#27258)
Refactor: Use Llama RoPE implementation for Falcon (#26933)
Fuyu protection (#27248)
Fixed base model class name extraction from PeftModels (#27162)
Removed the redundant SiLUActivation class. (#27136)
translate peft.md to chinese (#27215)
Dev version
Enrich TTS pipeline parameters naming (#26473)
Remove redundant code from T5 encoder mask creation (#27216)
Generate: return `past_key_values` (#25086)
fix-deprecated-exllama-arg (#27243)
Fixing m4t. (#27240)
Fix safetensors failing tests (#27231)
Wrap `_prepare_4d_causal_attention_mask` as a leaf function (#27236)
Fuyu: improve image processing (#27007)
[`core` / `Quantization`] Fix for 8bit serialization tests (#27234)
Reproducible checkpoint for npu (#27208)
support bf16 (#25879)
[Whisper, Bart, MBart] Add Flash Attention 2 (#27203)
Enable split_batches through TrainingArguments (#26798)
Fix CPU offload + disk offload tests (#27204)
Add exllamav2 better (#27111)
Translate task summary to chinese  (#27180)
improving TimmBackbone to support FrozenBatchNorm2d (#27160)
Fix docstring in get_oneformer_resize_output_image_size func (#27207)
Add TensorFlow implementation of ConvNeXTv2  (#25558)
[WhisperForCausalLM] Add WhisperForCausalLM for speculative decoding (#27195)
Added cache_block_outputs option to enable GPTQ for non-regular models (#27032)
added unsqueeze_dim to apply_rotary_pos_emb (#27117)
Fixing docstring in get_resize_output_image_size function (#27191)
Fix the typos and grammar mistakes in CONTRIBUTING.md. (#27193)
Fix docstring get maskformer resize output image size (#27196)
[`core` / `Quantization` ] AWQ integration (#27045)
device agnostic fsdp testing (#27120)
üåê [i18n-ZH] Translate tflite.md into Chinese (#27134)
Safetensors serialization by default (#27064)
Unify warning styles for better readability (#27184)
device agnostic models testing (#27146)
[docs] Update CPU/GPU inference docs (#26881)
translate traning.md to chinese (#27122)
Fix dropout in `StarCoder` (#27182)
[`Quantization` / `tests` ] Fix bnb MPT test (#27178)
Backward compatibility fix for the Conversation class (#27176)
[FEAT] Add Neftune into transformers Trainer (#27141)
device agnostic pipelines testing (#27129)
Shorten the conversation tests for speed + fixing position overflows (#26960)
Trigger CI if `tiny_model_summary.json` is modified (#27175)
Add support for loading GPTQ models on CPU (#26719)
fix: Fix typical_p behaviour broken in recent change (#27165)
Add flash attention for `gpt_bigcode` (#26479)
Disable CI runner check (#27170)
[doctring] Fix docstring for BlipTextConfig, BlipVisionConfig (#27173)
[docstring] Fix docstring for AltCLIPTextConfig, AltCLIPVisionConfig and AltCLIPConfig (#27128)
Remove broken links to s-JoL/Open-Llama (#27164)
deprecate function `get_default_device` in `tools/base.py` (#26774)
[KOSMOS-2] Update docs (#27157)
Fix import of torch.utils.checkpoint (#27155)
Fix: typos in README.md (#27154)
[`core`/ `GC` / `tests`] Stronger GC tests (#27124)
Device agnostic trainer testing (#27131)
Translating `en/main_classes` folder docs to Japanese üáØüáµ (#26894)
üåê [i18n-ZH] Translate serialization.md into Chinese (#27076)
Remove some Kosmos-2 `copied from` (#27149)
make tests of pytorch_example device agnostic (#27081)
[`tests` / `Quantization`] Fix bnb test (#27145)
Fix some tests using `"common_voice"` (#27147)
Add `Kosmos-2` model (#24709)
remove the obsolete code related to fairscale FSDP (#26651)
[`Trainer` / `GC`] Add `gradient_checkpointing_kwargs` in trainer and training arguments (#27068)
Fix data2vec-audio note about attention mask (#27116)
[`FA2`/ `Mistral`] Revert previous behavior with right padding + forward (#27125)
Fix slack report failing for doctest (#27042)
[Typo fix] flag config in WANDB (#27130)
Fix docstring and type hint for resize (#27104)
translate transformers_agents.md to Chinese (#27046)
Added Telugu [te] translation for README.md in main (#27077)
[Attention Mask] Refactor all encoder-decoder attention mask (#27086)
fix detr device map (#27089)
[`core`/ `gradient_checkpointing`] Refactor GC - part 2 (#27073)
Fix no split modules underlying modules (#27090)
Provide alternative when warning on use_auth_token (#27105)
Add early stopping for Bark generation via logits processor (#26675)
Revert "add exllamav2 arg" (#27102)
[`T5Tokenizer`]  Fix fast and extra tokens (#27085)
Added huggingface emoji instead of the markdown format (#27091)
Save TB logs as part of push_to_hub (#27022)
Correct docstrings and a typo in comments (#27047)
add exllamav2 arg (#26437)
[Llama FA2] Re-add _expand_attention_mask and clean a couple things (#27074)
Add-support for commit description (#26704)
Create SECURITY.md
Remove unneeded prints in modeling_gpt_neox.py (#27080)
Bump`flash_attn` version to `2.1` (#27079)
Bring back `set_epoch` for Accelerate-based dataloaders (#26850)
Bump urllib3 from 1.26.17 to 1.26.18 in /examples/research_projects/lxmert (#26888)
Bump werkzeug from 2.2.3 to 3.0.1 in /examples/research_projects/decision_transformer (#27072)
Handle unsharded Llama2 model types in conversion script (#27069)
Hindi translation of pipeline_tutorial.md (#26837)
üåê [i18n-ZH] Translate custom_models.md into Chinese (#27065)
[`docs`] Add `MaskGenerationPipeline` in docs (#27063)
[DOCS] minor fixes in README.md (#27048)
[docstring] fix incorrect llama docstring: encoder -> decoder (#27071)
Fix TypicalLogitsWarper tensor OOB indexing edge case (#26579)
[`core`] Refactor of `gradient_checkpointing` (#27020)
Skip-test (#27062)
Fix RoPE config validation for FalconConfig + various config typos (#26929)
Add a default decoder_attention_mask for EncoderDecoderModel during training (#26752)
[docs] Performance docs refactor p.2 (#26791)
Fix config silent copy in from_pretrained (#27043)
Device agnostic testing (#25870)
Add fuyu device map (#26949)
add info on TRL docs (#27024)
Safe import of rgb_to_id from FE modules (#27037)
[`TFxxxxForSequenceClassifciation`] Fix the eager mode after #25085 (#25751)
Normalize only if needed (#26049)
Add descriptive docstring to WhisperTimeStampLogitsProcessor (#25642)
Add `default_to_square_for_size` to `CLIPImageProcessor` (#26965)
Register ModelOutput as supported torch pytree nodes (#26618)
Fix key dtype in GPTJ and CodeGen (#26836)
üåê [i18n-ZH] Translate create_a_model.md into Chinese (#27026)
Fix little typo (#27028)
Bugfix device map detr model (#26849)
translate `preprocessing.md` to Chinese (#26955)
üåê [i18n-ZH] Translate multilingual into Chinese (#26935)
Remove ambiguous `padding_mask` and instead use a 2D->4D Attn Mask Mapper (#26792)
Translate `pipeline_tutorial.md` to chinese (#26954)
Remove token_type_ids from default TF GPT-2 signature (#26962)
small typos found (#26988)
[`SeamlessM4T`] fix copies with NLLB MoE int8  (#27018)
[`NLLB-MoE`] Fix NLLB MoE 4bit inference (#27012)
Add Seamless M4T model (#25693)
Change default `max_shard_size` to smaller value (#26942)
Nits in Llama2 docstring (#26996)
skip two tests (#27013)
python falcon doc-string example typo (#26995)
Limit to inferior fsspec version (#27010)
fix logit-to-multi-hot conversion in example (#26936)
Added Telugu [te] translations (#26828)
Update README_hd.md (#26872)
Fix Fuyu image scaling bug (#26918)
fix set_transform link docs (#26856)
[docstring] Fix docstring for speech-to-text config (#26883)
Corrected modalities description in README_ru.md (#26913)
Generate: update basic llm tutorial (#26937)
[`FA-2` / `Mistral`] Supprot fa-2 + right padding + forward (#26912)
Pin Keras for now (#26904)
Fix license (#26931)
[docstring] Fix docstrings for `CodeGen` (#26821)
Fix and re-enable ConversationalPipeline tests (#26907)
[Docs] Make sure important decode and generate method are nicely displayed in Whisper docs (#26927)
[docstring] Fix docstring for `ChineseCLIP` (#26880)
[`FA-2`] Revert suggestion that broke FA2 fine-tuning with quantized models (#26916)
Add fuyu model (#26911)
[`FA-2`] Final fix for FA2 dtype (#26846)
[i18n-ZH] Translated fast_tokenizers.md to Chinese (#26910)
Refactor code part in documentation translated to japanese (#26900)
Add default template warning (#26637)
Emergency PR to skip conversational tests to fix CI (#26906)
[`Tokenizer`] Fix slow and fast serialization (#26570)
Fix Seq2seqTrainer decoder attention mask  (#26841)
Knowledge distillation for vision guide (#25619)
Bump urllib3 from 1.26.17 to 1.26.18 in /examples/research_projects/decision_transformer (#26889)
Bump urllib3 from 1.26.17 to 1.26.18 in /examples/research_projects/visual_bert (#26890)
Generate: improve docstrings for custom stopping criteria (#26863)
Fix TensorFlow pakage check (#26842)
Translating `en/internal` folder docs to Japanese üáØüáµ (#26747)
Fixed a typo in mistral.md (#26879)
[docstring] Fix docstring for LukeConfig (#26858)
üö® üö®  Raise error when no speaker embeddings in speecht5._generate_speech (#26418)
[`FA2`] Fix flash attention 2 fine-tuning with Falcon (#26852)
üö®üö® Generate: change order of ops in beam sample to avoid nans (#26843)
Update logits_process.py docstrings to clarify penalty and reward cases (attempt #2) (#26784)
fix: when window_size is passes as array (#26800)
Chore: Typo fixed in multiple files of docs/source/en/model_doc (#26833)
Fix Mistral OOM again (#26847)
üö®üö®üö® [`Quantization`] Store the original dtype in the config as a private attribute üö®üö®üö® (#26761)
Conversation pipeline fixes (#26795)
[docstring] Fix bert generation tokenizer (#26820)
Better way to run AMD CI with different flavors (#26634)
Llama tokenizer: remove space in template comment (#26788)
Add LLM doc (#26058)
[OWL-ViT, OWLv2] Add resources (#26822)
fix resume_from_checkpoint bug (#26739)
Make fsdp ram efficient loading optional (#26631)
Image-to-Image Task Guide (#26595)
[docstring] Fix docstring for `CodeLlamaTokenizerFast` (#26666)
Add Japanese translation (#26799)
[docstring] Fix docstring for `CanineConfig` (#26771)
Fixed typos (#26810)
translation brazilian portuguese (#26769)
Add CLIP resources (#26534)
[`Flava`] Fix flava doc (#26789)
Fixed KeyError for Mistral (#26682)
Add OWLv2, bis (#26668)
Fix Falcon generation test (#26770)
Disable default system prompt for LLaMA (#26765)
[`core`] Fix fa-2 import (#26785)
[docstring] fix docstring `DPRConfig` (#26674)
Fix num. of minimal calls to the Hub with peft for pipeline  (#26385)
[docstring] Fix docstring for `RwkvConfig` (#26782)
Update expect outputs of `IdeficsProcessorTest.test_tokenizer_padding` (#26779)
üåê [i18n-KO] Translated `big_models.md` to Korean (#26245)
Skip `TrainerIntegrationFSDP::test_basic_run_with_cpu_offload` if `torch < 2.1` (#26764)
chore: fix typos (#26756)
Fix `PerceiverModelIntegrationTest::test_inference_masked_lm` (#26760)
[docstring] Fix docstring for 'BertGenerationConfig' (#26661)
[docstring] Update `GPT2` and  `Whisper` (#26642)
[docstring] Fix `UniSpeech`, `UniSpeechSat`, `Wav2Vec2ForCTC` (#26664)
[docs] LLM prompting guide (#26274)
Fix backward compatibility of Conversation (#26741)
Fix `MistralIntegrationTest` OOM (#26754)
Fix `PersimmonIntegrationTest` OOM (#26750)
Warnings controlled by logger level (#26527)
Add many missing spaces in adjacent strings (#26751)
Fix doctest for `Blip2ForConditionalGeneration` (#26737)
Translated the accelerate.md file of the documentation to Chinese (#26161)
add japanese documentation (#26138)
[docstring] Fix docstring for `CodeLlamaTokenizer` (#26709)
[docstring] Fix docstring for `LlamaTokenizer` and `LlamaTokenizerFast` (#26669)
Revert #20715 (#26734)
Update docker files to use `torch==2.1.0` (#26735)
Fix checkpoint path in `no_trainer` scripts (#26733)
Fix stale bot for locked issues (#26711)
fix the model card issue as `use_cuda_amp` is no more available (#26731)
[docstring] `SwinModel` docstring fix (#26679)
[Assistant Generation] Improve Encoder Decoder (#26701)
`Copied from` for test files (#26713)
Update docs to explain disabling callbacks using report_to (#26155)
In assisted decoding, pass model_kwargs to model's forward call (fix prepare_input_for_generation in all models) (#25242)
Make Whisper Encoder's sinusoidal PE non-trainable by default (#26032)
[JAX] Replace uses of `jnp.array` in types with `jnp.ndarray`. (#26703)
Fix source_prefix default value (#26654)
fix a typo in flax T5 attention - attention_mask variable is misnamed (#26663)
[docstring] Fix docstring for `LlamaConfig` (#26685)
Fix Typo: table in deepspeed.md (#26705)
Control first downsample stride in ResNet (#26374)
[docstring] Fix docstrings for `CLIP` (#26691)
Fix stale bot (#26692)
[docstring] Fix docstring for DonutImageProcessor (#26641)
[docstring] Fix docstring for `CLIPImageProcessor` (#26676)
[docstring] Fix docstring CLIP configs (#26677)
fix typos in idefics.md (#26648)
Avoid CI OOM (#26639)
fix links in README.md for the GPT, GPT-2, and Llama2 Models (#26640)
Fixed malapropism error (#26660)
[DINOv2] Convert more checkpoints (#26177)
docs(zh): review and punctuation & space fix (#26627)
[docstring] Fix docstring for `AlbertConfig` (#26636)
[`LlamaTokenizerFast`] Adds edge cases for the template processor    (#26606)
remove SharedDDP as it is deprecated (#25702)
Fix failing `MusicgenTest .test_pipeline_text_to_audio` (#26586)
fix RoPE t range issue for fp16 (#26602)
Update chat template docs with more tips on writing a template (#26625)
Remove unnecessary unsqueeze - squeeze in rotary positional embedding (#26162)
Update tokenization_code_llama_fast.py (#26576)
Fixed inconsistency in several fast tokenizers (#26561)
Remove unnecessary `view`s of `position_ids` (#26059)
Don't install `pytorch-quantization` in Doc Builder docker file (#26622)
[docs] Update to scripts building index.md (#26546)
Fix `transformers-pytorch-gpu` docker build (#26615)
Don't close ClearML task if it was created externally (#26614)
#26566 swin2 sr allow in out channels (#26568)
[`core`] fix silent bug `keep_in_fp32` modules (#26589)
Make `ModelOutput` serializable (#26493)
Fix failing tests on `main` due to torch 2.1 (#26607)
[Falcon] Set `use_cache=False` before creating `presents` which relies on `use_cache` (#26328)
[`GPTNeoX`] Faster rotary embedding for GPTNeoX (based on llama changes) (#25830)
[ `NougatProcessor`] Fix the default channel (#26608)
add zh translation for installation (#26084)
[Wav2Vec2] Fix tokenizer set lang (#26349)
Update mistral.md to update 404 link (#26590)
skip flaky hub tests (#26594)
Fix encoder->decoder typo bug in convert_t5x_checkpoint_to_pytorch.py (#26587)
Fix embarrassing typo in the doc chat template! (#26596)
Add # Copied from statements to audio feature extractors that use the floats_list function (#26581)
[Mistral] Update config docstring (#26593)
refactor: change default block_size (#26229)
Add add_generation_prompt argument to apply_chat_template (#26573)
Docstring check (#26052)
feat: add trainer label to wandb run upon initialization (#26466)
Extend Trainer to enable Ascend NPU to use the fused Adamw optimizer when training (#26194)
Bump pillow from 9.3.0 to 10.0.1 in /examples/research_projects/decision_transformer (#26580)
docs: feat: add clip notebook resources from OSSCA community (#26505)
[Tokenizers] Skip tests temporarily (#26574)
üåê [i18n-KO] Translated `semantic_segmentation.md` to Korean (#26515)
[Whisper] Allow basic text normalization (#26149)
v4.35.0.dev0
[`Nougat`] from transformers import * (#26562)
[`PEFT`] Final fixes (#26559)
[`Mistral`] Add Flash Attention-2 support for `mistral` (#26464)
Nit-added-tokens (#26538)
[Doctest] Add `configuration_encoder_decoder.py` (#26519)
[AMD] Add initial version for run_tests_multi_gpu (#26346)
[Wav2Vec2 and Co] Update init tests for PT 2.1 (#26494)
Add tokenizer kwargs to fill mask pipeline. (#26234)
[RFC, Logging] Change warning to info (#26545)
Bump urllib3 from 1.26.9 to 1.26.17 in /examples/research_projects/decision_transformer (#26554)
Bump urllib3 from 1.26.5 to 1.26.17 in /examples/research_projects/visual_bert (#26552)
Bump urllib3 from 1.26.5 to 1.26.17 in /examples/research_projects/lxmert (#26551)
[i18n-DE] contribute chapter (#26481)
üåê [i18n-KO] Translated `tokenizer_summary.md` to Korean (#26243)
add build_inputs_with_special_tokens to LlamaFast (#26297)
Code-llama-nit (#26300)
[Doctest] Add configuration_roformer.py  (#26530)
Remove-warns (#26483)
[`PEFT`] Protect `adapter_kwargs` check (#26537)
Fix model integration ci (#26322)
[`core`/  `auto` ] Fix bnb test with code revision + bug with code revision (#26431)
[`PEFT`] Pass token when calling `find_adapter_config` (#26488)
Fix broken link to video classification task (#26487)
Fix issue of canine forward requiring input_ids anyway (#26290)
Fix requests connection error during modelcard creation (#26518)
Fix num_heads in _upad_input (#26490)
Revert falcon exception (#26472)
[ASR Pipe] Improve docs and error messages (#26476)
[Flax Examples] Seq2Seq ASR Fine-Tuning Script (#21764)
Avoid all-zeor attnetion mask used in testing (#26469)
 Skip 2 failing persimmon pipeline tests for now (#26485)
[docs] navigation improvement between text gen pipelines and text gen params (#26477)
[docs] Update offline mode docs (#26478)
[Whisper Tokenizer] Make decoding faster after adding timestamps (#26299)
Esm checkpointing (#26454)
fix_mbart_tied_weights (#26422)
Do not warn about unexpected decoder weights when loading T5EncoderModel and LongT5EncoderModel (#26211)
[`PEFT`]¬†introducing `adapter_kwargs` for loading adapters from different Hub location (`subfolder`, `revision`) than the base model (#26270)
[VITS] Fix speaker_embed device mismatch (#26115)
change mention of decoder_input_ids to input_ids and same with decode_inputs_embeds (#26406)
docs: change assert to raise and some small docs (#26232)
Fix `cos_sin` device issue in Falcon model (#26448)
optimize VRAM for calculating pos_bias in LayoutLM v2, v3 (#26139)
üåê [i18n-KO] Translated `perf_train_gpu_many.md` to Korean (#26244)
üåê [i18n-KO] Translated `debugging.md` to Korean (#26246)
[i18n-DE] Complete first toc chapter (#26311)
Update `runs-on` in workflow files (#26435)
Fix failing doctest (#26450)
[Mistral] Mistral-7B-v0.1 support (#26447)
[`PEFT`] Fix PEFT multi adapters support (#26407)
add bf16 mixed precision support for NPU (#26163)
[`FA` / `tests`] Add use_cache tests for FA models (#26415)
Fixing tokenizer when `transformers` is installed without `tokenizers` (#26236)
Update semantic_segmentation.md (#26419)
Fix padding for IDEFICS  (#26396)
Add torch `RMSProp` optimizer (#26425)
[InternLM] Add support for InternLM (#26302)
Fix DeepSpeed issue with Idefics (#26393)
added support for gradient checkpointing in ESM models (#26386)
Deleted duplicate sentence (#26394)
[ViTMatte] Add resources (#26317)
Add Nougat (#25942)
üåê [i18n-KO] Translated  `audio_classification.mdx` to Korean (#26200)
Add Russian localization for README (#26208)
Update tiny model information and pipeline tests (#26285)
[docs] removed MaskFormerSwin and TimmBackbone from the table on index.md (#26347)
Fix MusicGen logging error (#26370)
Update add_new_model.md (#26365)
Fixed unclosed p tags (#26240)
feat: adding num_proc to load_dataset (#26326)
Add image to image pipeline (#25393)
[TTA Pipeline] Fix MusicGen test (#26348)
[`core` ]¬†Integrate Flash attention 2 in most used models (#25598)
[doc] fixed indices in obj detection example (#26343)
Fix doctest CI (#26324)
Use CircleCI `store_test_results` (#26223)
[QUICK FIX LINK] Update trainer.py (#26293)
More error message fixup, plus some linebreaks! (#26296)
Porting the torchaudio kaldi fbank implementation to audio_utils (#26182)
update hf hub dependency to be compatible with the new tokenizers (#26301)
Fix FSMT weight sharing (#26292)
Keep relevant weights in fp32 when `model._keep_in_fp32_modules` is set even when `accelerate` is not installed (#26225)
add custom RMSNorm to `ALL_LAYERNORM_LAYERS` (#26227)
[`Trainer`] Refactor trainer + bnb logic (#26248)
include changes from llama (#26260)
add bbox input validation (#26294)
fix deepspeed available detection (#26252)
Rewrite for custom code warning messages (#26291)
Integrate AMD GPU in CI/CD environment (#26007)
Update bros checkpoint (#26277)
fix name error when accelerate is not available (#26278)
FSDP tests and checkpointing fixes (#26180)
[FIX] resize_token_embeddings (#26102)
DeepSpeed ZeRO-3 handling when resizing embedding layers (#26259)
Fix `Error` not captured in PR doctesting (#26215)
Add ViTMatte (#25843)
Fix gated repo tests (#26257)
Fix some docstring in image processors (#26235)
Fix the gitlab user mention in issue templates to the correct user (#26237)
[docs] Fix model reference in zero shot image classification example (#26206)
Update add_new_pipeline.md (#26197)
Update README.md (#26198)
[AutoBackbone] Add test (#26094)
Create the return value on device to avoid unnecessary copying from CPU (#26151)
üåê [i18n-KO] Translated `whisper.md` to Korean (#26002)
üö®üö® üö®üö® [`Tokenizer`] attemp to fix add_token issuesüö®üö® üö®üö®  (#23909)
[Check] Fix config docstring (#26222)
[Permisson] Style fix (#26228)
[Wav2Vec2-Conf / LLaMA] Style fix  (#26188)
refactor: change default block_size in block size > max position embeddings (#26069)
refactor decay_parameters production into its own function (#26152)
[FSMT] Fix non-shared weights (#26187)
Fix ConversationalPipeline tests (#26217)
moved `ctrl` to `Salesforce/ctrl` (#26183)
Remove `utils/documentation_tests.txt` (#26213)
No doctest for `convert_bros_to_pytorch.py` (#26212)
[PEFT] Allow PEFT model dict to be loaded (#25721)
[docs] IDEFICS guide and task guides restructure (#26035)
Fix pad to multiple of (#25732)
Update notebook.py to support multi eval datasets (#25796)
[Whisper] Check length of prompt + max new tokens (#26164)
Tweaks to Chat Templates docs (#26168)
[TTA Pipeline] Test MusicGen and VITS (#26146)
IDEFICS: allow interpolation of vision's pos embeddings (#26029)
[BLIP-2] Improve conversion script (#24854)
Add BROS (#23190)
[Whisper] Fix word-level timestamps for audio < 30 seconds (#25607)
[MusicGen] Add sampling rate to config (#26136)
Fix beam search when using model parallel (#24969)
[MusicGen] Add streamer to generate (#25320)
Overhaul Conversation class and prompt templating (#25323)
[`PEFT`] Fix PEFT + gradient checkpointing (#25846)
[Whisper Tokenizer] Encode timestamps (#26054)
Fix eval accumulation when `accelerate` > 0.20.3 (#26060)
Add missing Maskformer dataclass decorator, add dataclass check in ModelOutput for subclasses (#25638)
Flex xpu bug fix (#26135)
[docs] last hidden state vs hidden_states[-1] (#26142)
Update training_args.py - addition of self.distributed_state when using XPU (#25999)
Fix `beam_scores` shape when token scores shape changes after `logits_processor` (#25980)
Falcon: batched generation (#26137)
Fix `test_finetune_bert2bert` (#25984)
Generate: ignore warning when `generation_config.max_length` is set to `None` (#26147)
docs: feat: add llama2 notebook resources from OSSCA community (#26076)
[`RWKV`] Final fix RWMV 4bit (#26134)
Update spectrogram and waveform model mapping for TTS/A pipeline (#26114)
Add missing space in generation/utils.py (#26121)
[`core`] fix 4bit `num_parameters` (#26132)
Fix AutoTokenizer docstring typo (#26117)
fix the deepspeed tests (#26021)
safeguard torch distributed check (#26056)
Fix `MarianTokenizer` to remove metaspace character in `decode` (#26091)
Text2text pipeline: don't parameterize from the config (#26118)
chore: correct update_step and correct gradient_accumulation_steps (#26068)
enable optuna multi-objectives feature (#25969)
üåê [i18n-KO] Translated `contributing.md` to Korean (#25877)
[docs] Updates to TTS task guide with regards to the new TTS pipeline  (#26095)
üåê [i18n-KO] Translated `llama2.md` to Korean (#26047)
Fix ExponentialDecayLengthPenalty negative logits issue (#25594)
Update logits_process.py docstrings (#25971)
Generate: legacy mode is only triggered when `generation_config` is untouched (#25962)
[`core`] Import tensorflow inside relevant methods in `trainer_utils` (#26106)
[`Persimmon`] Add support for persimmon (#26042)
docs: add space to docs (#26067)
[Core] Add lazy import structure to imports (#26090)
docs: update link huggingface map (#26077)
only main process should call _save on deepspeed zero3 (#25959)
[`CITests`] skip failing tests until #26054 is merged (#26063)
[`CodeLlamaTokenizerFast`] Fix fix `set_infilling_processor` to properly reset (#26041)
üåê [i18n-KO] Translated `llama.md` to Korean (#26044)
Skip warning if tracing with dynamo (#25581)
Update missing docs on `activation_dropout` and fix DropOut docs for SEW-D (#26031)
Fix Dropout Implementation in Graphormer (#24817)
Try to fix training Loss inconsistent after resume from old checkpoint (#25872)
Punctuation fix (#26025)
Fix vilt config docstring parameter to match value in init (#26017)
Added HerBERT to README.md (#26020)
[VITS] Fix nightly tests (#25986)
Add `tgs` speed metrics (#25858)
Fix CircleCI config (#26023)
fix _resize_token_embeddings will set lm head size to 0 when enabled deepspeed zero3 (#26024)
Fix err with FSDP (#25991)
modify context length for GPTQ + version bump (#25899)
Remove Falcon from undocumented list (#26008)
üåê[i18n-KO] Translated `llm_tutorial.md` to Korean (#25791)
Fix small typo README.md (#25934)
TF-OPT attention mask fixes (#25238)
Falcon: fix revision propagation (#26006)
Update README.md (#26003)
save space when converting hf model to megatron model. (#25950)
Fix Mega chunking error when using decoder-only model (#25765)
[`VITS`]  tokenizer integration test: fix revision did not exist (#25996)
[`CI`]  Fix red CI and ERROR failed should show (#25995)
Add LLaMA resources (#25859)
[Wav2Vec2 Conformer] Fix inference float16 (#25985)
deepspeed resume from ckpt fixes and adding support for deepspeed optimizer and HF scheduler (#25863)
Add TFDebertaV2ForMultipleChoice (#25932)
PegasusX add _no_split_modules (#25933)
Patch with accelerate xpu (#25714)
Show failed tests on CircleCI layout in a better way (#25895)
Trainer: delegate default generation values to `generation_config` (#25987)
Update training_args.py to remove the runtime error (#25920)
Update RAG README.md with correct path to examples/seq2seq (#25953)
[doc] Always call it Agents for consistency (#25958)
Use main in conversion script (#25973)
fix typo (#25981)
Add `Pop2Piano` space demo. (#25975)
nn.Identity is not required to be compatible with PyTorch < 1.1.0 as the minimum PyTorch version we currently support is 1.10.0 (#25974)
Fix `test_load_img_url_timeout` (#25976)
Fix Detr CI (#25972)
Fix typo (#25966)
v4.34.dev.0
[`Falcon`] Remove SDPA for falcon to support earlier versions of PyTorch (< 2.0) (#25947)
Put Falcon back (#25960)
Add type hints for tf models final batch (#25883)
Fix smart check (#25955)
Fix failing test (#25963)
Add proper Falcon docs and conversion script (#25954)
[VITS] Fix init test (#25945)
Update README.md (#25922)
Import deepspeed utilities from integrations (#25919)
[VITS] Handle deprecated weight norm (#25946)
[MMS] Fix pip install in docs (#25949)
Update README.md (#25941)
Update autoclass_tutorial.md (#25929)
Update community.md (#25928)
Fix typos (#25936)
Skip offload tests for `ViTDet` (#25913)
CI: hotfix (skip VitsModelTest::test_initialization)
Update model_memory_anatomy.md (#25896)
Update-llama-code (#25826)
[VITS] Only trigger tokenizer warning for uroman (#25915)
[MMS] Update docs with HF TTS implementation (#25907)
[VITS] Add to TTA pipeline (#25906)
Revert frozen training arguments (#25903)
Remove broken docs for MusicGen (#25905)
Better error message for pipeline loading (#25912)
Falcon: Add RoPE scaling (#25878)
fix FSDP model resume optimizer & scheduler (#25852)
add VITS model (#24085)
remove torch_dtype override (#25894)
Smarter check for `is_tensor` (#25871)
Update `setup.py` (#25893)
Add type hints for tf models batch 1 (#25853)
[`InstructBlip`] FINAL Fix instructblip test (#25887)
Save image_processor while saving pipeline (ImageSegmentationPipeline) (#25884)
[`CodeLlama`] Fix CI  (#25890)
[`TokenizerFast`] `can_save_slow_tokenizer` as a property for when `vocab_file`'s folder was removed (#25626)
Modify efficient GPU training doc with now-available adamw_bnb_8bit optimizer (#25807)
fix ds z3 checkpointing when  `stage3_gather_16bit_weights_on_model_save=False` (#25817)
For xla tensors, use an alternative way to get a unique id (#25802)
[ViTDet] Fix doc tests (#25880)
Reduce CI output (#25876)
pin pandas==2.0.3 (#25875)
Docs: fix example failing doctest in `generation_strategies.md ` (#25874)
fix max_memory for bnb (#25842)
Fix imports (#25869)
Remote tools are turned off (#25867)
Add Blip2 model in VQA pipeline (#25532)
Add flax installation in daily doctest workflow (#25860)
minor typo fix in PeftAdapterMixin docs (#25829)
Update README.md (#25832)
Generate: models with custom `generate()` return `True` in `can_generate()` (#25838)
Update README.md (#25834)
Support loading base64 images in pipelines (#25633)
MaskFormer,Mask2former - reduce memory load (#25741)
[AutoTokenizer] Add data2vec to mapping (#25835)
update remaining `Pop2Piano` checkpoints (#25827)
ü§¶update warning to If you want to use the new behaviour, set `legacy=‚Ä¶ (#25833)
üåê¬†[i18n-KO] Translated¬†`community.md` to Korean (#25674)
üåê [i18n-KO] Translated `add_new_pipeline.md` to Korean (#25498)
Tests: detect lines removed from "utils/not_doctested.txt" and doctest ALL generation files (#25763)
Error with checking args.eval_accumulation_steps to gather tensors (#25819)
üåê [i18n-KO] `model_memory_anatomy.md` to Korean (#25755)
üåê [i18n-KO] Translated peft.md to Korean (#25706)
fix warning trigger for embed_positions when loading xglm (#25798)
[`LlamaTokenizer`] `tokenize` nits.  (#25793)
Minor wording changes for Code Llama (#25815)
fix register (#25779)
[`Docs`] More clarifications on BT + FA (#25823)
Resolving Attribute error when using the FSDP ram efficient feature (#25820)
[DINOv2] Add backbone class (#25520)
Add ViTDet (#25524)
fixing name position_embeddings to object_queries (#24652)
Fix incorrect Boolean value in deepspeed example (#25788)
Arde/fsdp activation checkpointing (#25771)
[idefics] fix vision's `hidden_act` (#25787)
Add type hints for several pytorch models (batch-4) (#25749)
Add type hints for pytorch models (final batch) (#25750)
Add type hints for several pytorch models (batch-2) (#25557)
[`LlamaFamiliy`] add a tip about dtype (#25794)
Add docstrings and fix VIVIT examples (#25628)
[idefics] small fixes (#25764)
[`CodeLlama`] Add support for `CodeLlama` (#25740)
fix a typo in docsting (#25759)
Correct attention mask dtype for Flax GPT2 (#25636)
üö®üö®üö® [`Refactor`] Move third-party related utility files into `integrations/` folder üö®üö®üö® (#25599)
Add type hints for several pytorch models (batch-3) (#25705)
Docs: fix indentation in `HammingDiversityLogitsProcessor` (#25756)
fix encoder hook (#25735)
[`Sentencepiece`] make sure `legacy` do not require `protobuf` (#25684)
[CLAP] Fix logit scales dtype for fp16 (#25754)
Generate: logits processors are doctested and fix broken doctests (#25692)
[DOCS] Add example for HammingDiversityLogitsProcessor (#25481)
Generate: add missing logits processors docs (#25653)
Add FlaxCLIPTextModelWithProjection (#25254)
fixed typo in speech encoder decoder doc (#25745)
[`PEFT`] Fix PeftConfig save pretrained when calling `add_adapter` (#25738)
üåê [i18n-KO] Translated `visual_question_answering.md` to Korean (#25679)
[ASR Pipe Test] Fix CTC timestamps error message (#25727)
[`from_pretrained`] Fix failing PEFT tests (#25733)
ImageProcessor - check if input pixel values between 0-255 (#25688)
[idefics] idefics-9b test use 4bit quant (#25734)
[`from_pretrained`]  Simpler code for peft (#25726)
Generate: nudge towards `do_sample=False` when `temperature=0.0` (#25722)
[`AutoGPTQ`] Add correct installation of GPTQ library + fix slow tests (#25713)
Fix number of minimal calls to the Hub with peft integration (#25715)
[`PEFT`] Fix peft version (#25710)
Fix failing `test_batch_generation`  for bloom (#25718)
docs: Resolve typos in warning text (#25711)
Update list of persons to tag (#25708)
[`LlamaTokenizer`] make unk_token_length a property (#25689)
fix ram efficient fsdp init (#25686)
Skip broken tests
Fix typo in `configuration_gpt2.py` (#25676)
Generate: general test for decoder-only generation from `inputs_embeds`  (#25687)
correct resume training steps number in progress bar (#25691)
[DOCS] Added docstring example for EpsilonLogitsWarper #24783 (#25378)
Fix `pad_token` check condition (#25685)
Sets the stalebot to 10 AM CEST (#25678)
‚ö†Ô∏è [CLAP] Fix dtype of logit scales in init (#25682)
Prevent Dynamo graph fragmentation in GPTNeoX with torch.baddbmm fix (#24941)
Remove `utils/documentation_tests.txt` (#25680)
fix wrong path in some doc (#25658)
[`GPTNeo`]  Add input_embeds functionality to gpt_neo Causal LM  (#25664)
[`SPM`] Patch `spm` Llama and T5 (#25656)
Add Llama2 resources (#25531)
Update doc toctree (#25661)
Add input_embeds functionality to gpt_neo Causal LM (#25659)
stringify config (#25637)
Adds `TRANSFORMERS_TEST_BACKEND` (#25655)
removing unnecesssary extra parameter (#25643)
Fix bloom add prefix space (#25652)
TF 2.14 compatibility (#25630)
Put IDEFICS in the right section of the doc (#25650)
Pass the proper token to PEFT integration in auto classes (#25649)
[MINOR:TYPO] (#25646)
[DOCS] MusicGen Docs Update (#25510)
Add Number Normalisation for SpeechT5 (#25447)
Support specifying revision in push_to_hub (#25578)
Add Pop2Piano (#21785)
fix documentation for CustomTrainer (#25635)
üö®üö®üö® changing default threshold and applying threshold before the rescale (#25608)
Skip doctest for some recent files (#25631)
fix ACT_FN (#25627)
correct TTS pipeline docstrings snippet (#25587)
Added paper links in logitprocess.py (#25482)
v4.33.0.dev0
Fix test_modeling_mpt typo in model id (#25606)
Run doctest for new files (#25588)
Fix PEFT integration failures on nightly CI (#25624)
Ignore all exceptions from signal in dynamic code (#25623)
Hotfix
reattach hooks when using `resize_token_embeddings` (#25596)
new model: IDEFICS via HuggingFaceM4 (#24796)
üåê [i18n-KO] Translated `perf_train_tpu_tf.md` to Korean (#25433)
Make TTS automodels importable (#25595)
[`PEFT`] Peft integration alternative design  (#25077)
[`TokenizerFast`] Fix setting prefix space in __init__ (#25563)
fix z3 init when using accelerate launcher (#25589)
[Time series Informer] fix dtype of cumsum (#25431)
[`Llama`] remove prompt and fix prefix finetuning (#25565)
[`split_special_tokens`] Add support for `split_special_tokens` argument to encode (#25081)
Replaces calls to `.cuda` with `.to(torch_device)` in tests (#25571)
Added missing parenthesis in call to is_fsdp_enabled (#25585)
[`Docs` / `BetterTransformer` ] Added more details about flash attention + SDPA (#25265)
Suggestions on Pipeline_webserver (#25570)
Fix typo in example code (#25583)
add warning for 8bit optimizers (#25575)
Skip `test_contrastive_generate` for `TFXLNet` (#25574)
Add Text-To-Speech pipeline (#24952)
add util for ram efficient loading of model when using fsdp (#25107)
Revert "change version (#25387)" (#25573)
[`Tests`] Fix failing 8bit test (#25564)
[`NllbMoe`] Update code to properly support loss computation (#25429)
Inconsistency in PreTrainedModel.resize_token_embeddings When ZeRO3 Is Enabled (#25394)
üö®üö®üö® [`SPM`] Finish fix spm models üö®üö®üö® (#25224)
[`SwitchTransformers`] Remove unused module (#25427)
[`resize_embedding`] Introduce `pad_to_multiple_of` and guidance (#25088)
Skip `test_beam_search_xla_generate_simple` for `T5` (#25566)
Adds `TRANSFORMERS_TEST_DEVICE` (#25506)
[`Docs`] Fix un-rendered images (#25561)
Skip `test_onnx_runtime_optimize` for now (#25560)
YOLOS - reset default return_pixel_mask value (#25559)
üö®üö®üö® Vivit update default rescale_factor value (#25547)
Fix `torch.fx` tests on nightly CI (#25549)
Fix MPT CI (#25548)
Add documentation to dynamic module utils (#25534)
Update trainer.py (#25553)
[i18n-KO] Translated docs: ko: pr_checks.md to Korean (#24987)
More utils doc (#25457)
[ASR Pipeline] Fix init with timestamps (#25438)
Input data format (#25464)
More frozen args (#25540)
Fix `MaskFormerModelIntegrationTest` OOM (#25544)
fix vit hybrid test (#25543)
Generate: fix default max length warning (#25539)
Document the test fetcher (#25521)
Marian: post-hack-fix correction (#25459)
Fix nested configs of Jukebox (#25533)
[TYPO] fix typo/format in quicktour.md (#25519)
Use dynamic past key-values shape in TF-Whisper (#25523)
Make training args fully immutable (#25435)
add __repr__  to the BitsAndBytesConfig class (#25517)
Bump tornado from 6.3.2 to 6.3.3 in /examples/research_projects/lxmert (#25511)
Bump tornado from 6.3.2 to 6.3.3 in /examples/research_projects/visual_bert (#25512)
Check for case where `auxiliary_head` is `None` in `UperNetPreTrainedModel` (#25514)
Conditional DETR type hint fix (#25505)
üö®üö®üö® Remove softmax for EfficientNetForImageClassification üö®üö®üö® (#25501)
fix gptq nits (#25500)
MaskFormer post_process_instance_segmentation bug fix convert out side of loop (#25497)
Set can_generate for SpeechT5ForTextToSpeech (#25493)
Add type hints to Blip2QFormer,  BigBirdForQA and ConditionalDetr family models (#25488)
Remove logging code in TF Longformer that fails to compile (#25496)
fix : escape key of start_token from special characters before search end_token in token2json function of DonutProcessor  (#25472)
Bump gitpython from 3.1.30 to 3.1.32 in /examples/research_projects/decision_transformer (#25467)
Bump gitpython from 3.1.30 to 3.1.32 in /examples/research_projects/distillation (#25468)
import required torch and numpy libraries (#25483)
Revert "Reuse the cache created for latest `main` on PRs/branches" (#25466)
Mark flaky tests (#25463)
Add input_data_format argument, image transforms (#25462)
Update run_translation.py broken link example Pytoch (#25461)
Reuse the cache created for latest `main` on PRs/branches if `setup.py` is not modified (#25445)
Switch Transformers: remove overwritten beam sample test (#25458)
Refactor image processor testers (#25450)
Fix for #25437 (#25454)
GPTQ integration (#25062)
docs: add LLaMA-Efficient-Tuning to awesome-transformers (#25441)
Fix issue with ratio evaluation steps and auto find batch size (#25436)
Add `examples`  to tests to run when `setup.py` is modified (#25437)
Fix rendering for `torch.compile()` docs (#25432)
Generate: Load generation config when `device_map` is passed (#25413)
[WavLM] Fix Arxiv link and authors (#25415)
Generation: strict generation config validation at save time (#25411)
Doc checks (#25408)
üåê [i18n-KO] Translated `philosophy.md` to Korean (#25010)
[DINOv2] Update pooler output (#25392)
Bark: flexible generation config overload (#25414)
Enable passing number of channels when inferring data format (#25412)
aligned sample_beam output selection with beam_search (#25375)
Update Bark generation configs and tests (#25409)
üåê [i18n-KO] Translated `model_summary.md` to Korean (#24625)
üåê [i18n-KO] Translated `add_new_model.md` to Korean (#24957)
VQA task guide (#25244)
Generate: lower severity of parameterization checks (#25407)
16059 - Add extra type hints for AltCLIPModel (#25399)
Generate: generation config validation fixes in docs (#25405)
Improve training args (#25401)
Generate: length validation (#25384)
Docs: introduction to generation with LLMs (#25240)
YOLOS - Revert default return_pixel_mask value (#25404)
Fix path for dynamic module creation (#25402)
rm useless condition since the previous condition contains it. (#25403)
16059 - Add missing type hints for ASTModel (#25364)
üåê [i18n-KO] Translated `perf_train_cpu_many.md` to Korean (#24923)
[DOCS] Add example for `TopPLogitsWarper`  (#25361)
change version (#25387)
Add copied from for image processor methods (#25121)
Use small config for `OneFormerModelTest.test_model_with_labels` (#25383)
Fix missing usage of `token` (#25382)
Generate: add config-level validation (#25381)
Fix `torch_job` worker(s) crashing (#25374)
üåê [i18n-KO] Translated `add_tensorflow_model.md` to Korean (#25017)
Enable tests to run on third-party devcies (#25327)
Fix `token` in example template (#25351)
Load state in else (#25318)
MaskFormer, Mask2Former - replace einsum for tracing (#25297)
[ASR Pipeline] Clarify return timestamps (#25344)
Add warning for missing attention mask when pad tokens are detected (#25345)
Fix `test_model_parallelism` (#25359)
Register ModelOutput subclasses as supported torch.utils._pytree nodes (#25358)
[DOCS] Add descriptive docstring to MinNewTokensLength (#25196)
Add mask2former fp16 support (#25093)
Docs: Added benchmarks for `torch.compile()`¬†for vision models (#24748)
[DOCS] Add `NoRepeatNGramLogitsProcessor` Example for `LogitsProcessor` class (#25186)
Adding more information in help parser on train_file and validation_file (#25324)
Migrate Trainer from `Repository` to `upload_folder` (#25095)
Fix more offload edge cases (#25342)
Generate: remove Marian hack (#25294)
Allow `trust_remote_code` in example scripts (#25248)
Loosen output shape restrictions on GPT-style models (#25188)
Generalize CFG to allow for positive prompts (#25339)
Update TF pin in docker image (#25343)
üåê [i18n-KO] Translated `perf_infer_gpu_one.md` to Korean (#24978)
add CFG for .generate() (#24654)
Remove jnp.DeviceArray since it is deprecated. (#24875)
[Whisper] Better error message for outdated generation config (#25298)
Document toc check and doctest check scripts (#25319)
Make `bark` could have tiny model (#25290)
Document check copies (#25291)
Deal with nested configs better in base class (#25237)
Add offline mode for agents (#25226)
Generate: get generation mode as an enum (#25292)
Give more memory in test_disk_offload (#25315)
Move usage of deprecated logging.warn to logging.warning (#25310)
Fix typo: Roberta -> RoBERTa (#25302)
[small] llama2.md typo (#25295)
[JAX] Bump min version (#25286)
Add timeout parameter to load_image function (#25184)
add generate method to SpeechT5ForTextToSpeech (#25233)
Update bark doc (#25234)
Docs: separate generate section (#25235)
Update InstructBLIP & Align values after rescale update (#25209)
Docs: Update list of `report_to` logging integrations in docstring (#25281)
CI with `pytest_num_workers=8` for torch/tf jobs (#25274)
CI with `num_hidden_layers=2` üöÄüöÄüöÄ (#25266)
[MMS] Fix mms (#25267)
recommend DeepSpeed's Argument Parsing documentation (#25268)
üåê [i18n-KO] Translated `perf_infer_gpu_many.md` to Korean (#24943)
Remove `pytest_options={"rA": None}` in CI (#25263)
Fix return_dict_in_generate bug in InstructBlip generate function (#25246)
[DOCS] Add example and modified docs of EtaLogitsWarper (#25125)
Fix some bugs for two stage training of deformable detr (#25045)
Update rescale tests - cast to float after rescaling to reflect #25229 (#25259)
resolving zero3 init when using accelerate config with Trainer (#25227)
Add `token` arugment in example scripts (#25172)
add pathname and line number to logging formatter in debug mode (#25203)
fix get_keys_to_not_convert() to return correct modules for full precision inference (#25105)
Fix set of model parallel in the Trainer when no GPUs are available (#25239)
Move rescale dtype recasting to match torchvision ToTensor (#25229)
[`Detr`] Fix detr BatchNorm replacement issue (#25230)
[`MPT`] Add  `require_bitsandbytes` on MPT integration tests (#25201)
[`Docs`/`quantization`] Clearer explanation on how things works under the hood. + remove outdated info (#25216)
[`Pix2Struct`] Fix pix2struct cross attention (#25200)
make build_mpt_alibi_tensor a method of MptModel so that deepspeed co‚Ä¶ (#25193)
Fix docker image build failure (#25214)
Update tiny model info. and pipeline testing (#25213)
[`pipeline`] revisit device check for pipeline (#25207)
[quantization.md] fix (#25190)
Fix `all_model_classes` in `FlaxBloomGenerationTest` (#25211)
[`PreTrainedModel`] Wrap `cuda` and `to` method correctly (#25206)
Better error message in `_prepare_output_docstrings` (#25202)
Musicgen: CFG is manually added  (#25173)
üö®üö®üö®  Fix rescale ViVit Efficientnet (#25174)
[MusicGen] Fix integration tests (#25169)
Fix beam search to sample at least 1 non eos token (#25103) (#25115)
üåê¬†[i18n-KO] Translated¬†`transformers_agents.md` to Korean (#24881)
[`InstructBlip`] Fix instructblip slow test (#25171)
[`Mpt`] Fix mpt slow test (#25170)
Update `use_auth_token` -> `token` in example scripts (#25167)
added compiled model support for inference (#25124)
make run_generation more generic for other devices (#25133)
Represent query_length in a different way to solve jit issue (#25164)
override .cuda() to check if model is already quantized (#25166)
Add test when downloading from gated repo (#25039)
Fix `.push_to_hub` and cleanup `get_full_repo_name` usage (#25120)
Add new model in doc table of content (#25148)
Add bloom flax (#25094)
More `token` things (#25146)
Add offload support to Bark (#25037)
[`MptConfig`] support from pretrained args (#25116)
üö®üö®üö®Change default from `adamw_hf` to `adamw_torch` üö®üö®üö® (#25109)
Clarify 4/8 bit loading log message (#25134)
[`T5/LlamaTokenizer`] default legacy to `None` to not always warn (#25131)
fix delete all checkpoints when save_total_limit is set to 1 (#25136)
fix deepspeed load best model at end when the model gets sharded (#25057)
Move center_crop to BaseImageProcessor (#25122)
MaskFormer - enable return_dict in order to compile (#25052)
Fix ViT docstring regarding default dropout values. (#25118)
Move common image processing methods to BaseImageProcessor (#25089)
Fix past CI after #24334 (#25113)
update `use_auth_token` -> `token` (#25083)
fix "UserWarning: Creating a tensor from a list of numpy.ndarrays is ‚Ä¶ (#24772)
Add descriptive docstring to TemperatureLogitsWarper (#24892)
Fix `PvtModelIntegrationTest::test_inference_fp16` (#25106)
üåê[i18n-KO] Translated pipeline_webserver.md to Korean (#24828)
documentation for llama2 models (#25102)
fix tied_params for meta tensor (#25101)
Bump certifi from 2022.12.7 to 2023.7.22 in /examples/research_projects/visual_bert (#25097)
Bump certifi from 2022.12.7 to 2023.7.22 in /examples/research_projects/decision_transformer (#25098)
Bump certifi from 2022.12.7 to 2023.7.22 in /examples/research_projects/lxmert (#25096)
Fix doctest (#25031)
[`T5`, `MT5`, `UMT5`] Add [T5, MT5, UMT5]ForSequenceClassification (#24726)
Hotfix for failing `MusicgenForConditionalGeneration` tests (#25091)
[ `PreTrainedTokenizerFast`] Keep properties from fast tokenizer (#25053)
Edit err message and comment in `test_model_is_small` (#25087)
[`TF`]  Also apply patch to support left padding (#25085)
[ `ForSequenceClassification`] Support `left` padding (#24979)
Allow generic composite models to pass more kwargs (#24927)
üåê [i18n-KO] Translated `perf_infer_cpu.md` to Korean (#24920)
[DOCS] add example NoBadWordsLogitsProcessor (#25046)
[`MPT`] Add MosaicML's `MPT` model to transformers (#24629)
Fix: repeat per sample for SAM image embeddings (#25074)
üåê [i18n-KO] Translated `hpo_train.md` to Korean (#24968)
[`generate`]  Only warn users if the `generation_config`'s `max_length` is set to the default value (#25030)
replace `per_gpu_eval_batch_size` with `per_device_eval_batch_size` in readme of multiple-choice task (#25078)
Fix broken link in README_hd.md (#25067)
Set `TF32` flag for PyTorch cuDNN backend (#25075)
fix: add TOC anchor link (#25066)
Fix last models for common tests that are too big. (#25058)
üåê [i18n-KO] Translated `perf_hardware.md` to Korean (#24966)
üåê [i18n-KO] Translated `<tf_xla>.md` to Korean (#24904)
[Docs] fix rope_scaling doc string (#25072)
Generate - add beam indices output in contrained beam search (#25042)
[`RWKV`] Add note in doc on `RwkvStoppingCriteria` (#25055)
Better error message when signal is not supported on OS (#25049)
üåê [i18n-KO] Translated `perf_train_cpu.md` to Korean (#24911)
[`8bit`] Fix 8bit corner case with Blip2 8bit (#25047)
compute_loss in trainer failing to label shift for PEFT model when label smoothing enabled. (#25044)
Pvt model (#24720)
Comment again print statement
Make more test models smaller (#25005)
Fix typo in LlamaTokenizerFast docstring example (#25018)
Add dispatch_batches to training arguments (#25038)
üåê [i18n-KO] Translated `testing.md` to Korean (#24900)
üåê[i18n-KO] Translated performance.md to Korean (#24883)
Better handling missing SYS in llama conversation tokenizer (#24997)
Support GatedRepoError + use raise from (#25034)
[docs] Performance docs tidy up, part 1  (#23963)
fix(integrations): store serialized `TrainingArgs` to `wandb.config` without sanitization. (#25035)
[`logging.py`] set default `stderr`  path if `None` (#25033)
[check_config_docstrings.py] improve diagnostics (#25012)
üåê [i18n-KO] Updated Korean `serialization.md` (#24686)
Move template doc file to md (#25004)
improve from_pretrained for zero3 multi gpus mode (#24964)
[`Llama`] remove persistent  `inv_freq` tensor (#24998)
[`bnb`] Add simple check for bnb import (#24995)
Fix `llama` tokenization doctest (#24990)
Use main_input_name for include_inputs_for_metrics (#24993)
Fix type annotation for deepspeed training arg (#24988)
Avoid importing all models when instantiating a pipeline (#24960)
Remove tokenizers from the doc table (#24963)
[`LlamaConfig`] Nit: pad token should be None by default (#24958)
Fix missing spaces in system prompt of Llama2 tokenizer (#24930)
fsdp fixes and enhancements (#24980)
üåê [i18n-KO] Fixed Korean and English `quicktour.md` (#24664)
fix: cast input pixels to appropriate dtype for image_to_text pipelines (#24947)
fix fsdp checkpointing issues (#24926)
Fallback for missing attribute `Parameter.ds_numel` (#24942)
Contrastive Search peak memory reduction (#24120)
Change logic for logging in the examples (#24956)
[`RWKV`] Add Gradient Checkpointing support for RWKV (#24955)
Bump aiohttp from 3.8.1 to 3.8.5 in /examples/research_projects/decision_transformer (#24954)
fix type annotations for arguments in training_args (#24550)
[DOCS] Example for `LogitsProcessor` class (#24848)
Fix `main_input_name` in `src/transformers/keras_callbacks.py` (#24916)
Update processing_vision_text_dual_encoder.py (#24950)
Bump pygments from 2.11.2 to 2.15.0 in /examples/research_projects/decision_transformer (#24949)
Generate: sequence bias can handle same terminations (#24822)
replace no_cuda with use_cpu in test_pytorch_examples (#24944)
Deprecate unused OpenLlama architecture (#24922)
Add multi-label text classification support to pytorch example (#24770)
üåê [i18n-KO] Translated`tasks/document_question_answering.md` to Korean (#24588)
[doc] `image_processing_vilt.py` wrong default documented (#24931)
[`Llama2`] replace `self.pretraining_tp` with `self.config.pretraining_tp` (#24906)
Fix minor llama2.md model doc typos (#24909)
fix typo in BARK_PRETRAINED_MODEL_ARCHIVE_LIST (#24902)
Fixed issue where ACCELERATE_USE_CPU="False" results in bool(True) (#24907)
Fix `test_model_parallelism` for `FalconModel` (#24914)
Update tested versions in READMEs (#24895)
Avoid some pipeline tasks to use `use_cache=True` (#24893)
Check for accelerate env var when doing CPU only (#24890)
Disable ipex env var if false (#24885)
[`Llama2`]  Add support for Llama 2 (#24891)
Separate CircleCI cache between `main` and `pull` (or other branches) (#24886)
check if eval dataset is dict (#24877)
[`Blip`] Fix blip output name (#24889)
[`InstructBlip`] Fix int8/fp4 issues (#24888)
Add DINOv2 (#24016)
Enable `ZeroShotAudioClassificationPipelineTests::test_small_model_pt` (#24882)
add ascend npu accelerator support (#24879)
Fix CircleCI cache (#24880)
[`Docs`] Clarify 4bit docs (#24878)
Remove `tests/onnx` (#24868)
Skip Add model like job (#24865)
Skip failing `ZeroShotAudioClassificationPipelineTests::test_small_model_pt` for now (#24867)
deprecate no_cuda (#24863)
Remove deprecated codes (#24837)
Make CLIP model could use new added tokens with meaningful pooling (#24777)
Replace assert statements with exceptions (#24856)
Fix the fetch of all example tests (#24864)
4.32.0.dev0
Fix token pass (#24862)
Add bark (#24086)
Add TAPEX to the list of deprecated models (#24859)
fix broken links in READMEs (#24861)
Fix comments for `_merge_heads` (#24855)
Fix `is_vision_available` (#24853)
Add Multimodal heading and Document question answering in task_summary.mdx (#23318)
Bump cryptography from 41.0.0 to 41.0.2 in /examples/research_projects/decision_transformer (#24833)
Remove unused code in GPT-Neo (#24826)
üåê¬†[i18n-KO] Translated¬†`custom_tools.mdx` to Korean  (#24580)
deprecate `sharded_ddp` training argument (#24825)
[üîó Docs] Fixed Incorrect Migration Link (#24793)
Check models used for common tests are small (#24824)
set correct model input names for gptsw3tokenizer (#24788)
Fixing double `use_auth_token.pop` (preventing private models from being visible). (#24812)
Copy code when using local trust remote code (#24785)
Run hub tests (#24807)
Use _BaseAutoModelClass's register method (#24810)
Update setup.py to be compatible with pipenv (#24789)
Remove Falcon docs for the release until TGI is ready (#24808)
Fix typo 'submosules' (#24809)
Add accelerate version in transformers-cli env (#24806)
Llama/GPTNeoX: add RoPE scaling  (#24653)
Deprecate models (#24787)
Skip torchscript tests for `MusicgenForConditionalGeneration` (#24782)
Fix MobileVitV2 doctest checkpoint (#24805)
Upgrade jax/jaxlib/flax pin versions (#24791)
[DOC] Clarify relationshi load_best_model_at_end and save_total_limit (#24614)
[fix] Change the condition of ValueError in "convert_checkpoint_from_transformers_to_megatron" (#24769)
Removing unnecessary `device=device` in modeling_llama.py (#24696)
Revert "Unpin protobuf in docker file (for daily CI)" (#24800)
Rm duplicate pad_across_processes (#24780)
Remove WWT from README (#24672)
gpt-bigcode: avoid `zero_` to support Core ML (#24755)
Fix pad across processes dim in trainer and not being able to set the timeout (#24775)
Update default values of bos/eos token ids in `CLIPTextConfig` (#24773)
Replacement of 20 asserts with exceptions (#24757)
Docs: Update logit processors __call__ docs (#24729)
Add MobileVitV2 to doctests (#24771)
Fix eval_accumulation_steps leading to incorrect metrics (#24756)
Unpin protobuf in docker file (for daily CI) (#24761)
Allow existing configs to be registered (#24760)
:bug: Handle empty gen_kwargs for seq2seq trainer prediction_step function (#24759)
Fix lr scheduler not being reset on reruns (#24758)
Skip some slow tests for doctesting in PRs (Circle)CI (#24753)
[InstructBLIP] Fix bos token of LLaMa checkpoints (#24492)
Fix non-deterministic Megatron-LM checkpoint name (#24674)
Skip keys not in the state dict when finding mismatched weights (#24749)
add gradient checkpointing for distilbert (#24719)
Docs: add `kwargs` type to fix formatting (#24733)
fix: Text splitting in the BasicTokenizer (#22280)
Fix typo in LocalAgent (#24736)
Add ViViT (#22518)
[Patch-t5-tokenizer] Patches the changes on T5 to make sure previous behaviour is still valide for beginning of words (#24622)
Falcon port (#24523)
add link to accelerate doc (#24601)
Docs: change some `input_ids` doc reference from `BertTokenizer` to `AutoTokenizer`  (#24730)
[`T5`] Adding model_parallel = False to `T5ForQuestionAnswering` and `MT5ForQuestionAnswering` (#24684)
Add Multi Resolution Analysis (MRA) (New PR) (#24513)
Enable `conversational` pipeline for `GPTSw3Tokenizer` (#24648)
Whisper: fix prompted max length (#24666)
Fix flaky `test_for_warning_if_padding_and_no_attention_mask` (#24706)
[`MT5`] Fix CONFIG_MAPPING issue leading it to load umt5 class (#24678)
Fix integration with Accelerate and failing test (#24691)
Avoid import `sentencepiece_model_pb2` in `utils.__init__.py` (#24689)
DeepSpeed/FSDP ckpt saving utils fixes and FSDP training args fixes (#24591)
Add dropouts to GPT-NeoX (#24680)
LlamaTokenizer should be picklable (#24681)
Add Nucleotide Transformer notebooks and restructure notebook list (#24669)
Fix model referenced and results in documentation. Model mentioned was inaccessible (#24609)
Unpin `huggingface_hub` (#24667)
Add `is_torch_mps_available` function to utils (#24660)
Fix `VisionTextDualEncoderIntegrationTest` (#24661)
Fix `EncodecModelTest::test_multi_gpu_data_parallel_forward` (#24663)
Make warning disappear for remote code in pipelines (#24603)
Add `finetuned_from` property in the autogenerated model card (#24528)
Update warning messages reffering to post_process_object_detection (#24649)
documentation_tests.txt - sort filenames alphabetically (#24647)
llama fp16 torch.max bug fix (#24561)
Fix audio feature extractor deps (#24636)
precompiled_charsmap checking before adding to the normalizers' list for XLNetTokenizerFast conversion. (#24618)
Generate: force cache with `inputs_embeds` forwarding (#24639)
Generate: multi-device support for contrastive search (#24635)
Fix loading dataset docs link in run_translation.py example (#24594)
Pin `Pillow` for now (#24633)
[Time-Series] Added blog-post to tips (#24482)
üåê [i18n-KO] Translated `perplexity.mdx` to Korean (#23850)
[`Umt5`]  Add google's umt5 to `transformers` (#24477)
fix pydantic install command
Limit Pydantic to V1 in dependencies (#24596)
Use protobuf 4 (#24599)
[several models] improve readability (#24585)
Speed up TF tests by reducing hidden layer counts (#24595)
Make (TF) CI faster (test only a subset of model classes) (#24592)
Show a warning for missing attention masks when pad_token_id is not None (#24510)
Udate link to RunHouse hardware setup documentation. (#24590)
‚ö†Ô∏è‚ö†Ô∏è[`T5Tokenize`] Fix T5 family tokenizers‚ö†Ô∏è‚ö†Ô∏è (#24565)
fix peft ckpts not being pushed to hub  (#24578)
Fix annotations (#24582)
Check all objects are equally in the main `__init__` file (#24573)
Fix ESM models buffers (#24576)
Removal of deprecated vision methods and specify deprecation versions (#24570)
Update some torchscript tests after #24505 (#24566)
Add Musicgen (#24109)
Revert "Fix typing annotations for FSDP and DeepSpeed in TrainingArguments" (#24574)
Docs: 4 bit doc corrections (#24572)
Fix annotations (#24571)
Fix Typo (#24559)
Update old existing feature extractor references (#24552)
Fixed OwlViTModel inplace operations (#24529)
Update masked_language_modeling.md (#24560)
Make PT/Flax tests could be run on GPU (#24557)
Update PT/Flax weight conversion after #24030 (#24556)
[`InstructBlip`] Add instruct blip int8 test (#24555)
Fix processor __init__ bug if image processor undefined (#24554)
[`gpt2-int8`] Add gpt2-xl int8 test (#24543)
Update `EncodecIntegrationTest` (#24553)
Update PT/TF weight conversion after #24030 (#24547)
Fix typing annotations for FSDP and DeepSpeed in TrainingArguments (#24549)
Allow for warn_only selection in enable_full_determinism (#24496)
Unpin DeepSpeed and require DS >= 0.9.3 (#24541)
‚ö†Ô∏è Time to say goodbye to py37 (#24091)
Add bitsandbytes support for gpt2 models (#24504)
Finishing tidying keys to ignore on load (#24535)
Fix Typo (#24530)
Allow backbones not in backbones_supported - Maskformer Mask2Former (#24532)
Clean load keys (#24505)
[Mask2Former] Remove SwinConfig (#24259)
Fix LR scheduler based on bs from auto bs finder (#24521)
Find module name in an OS-agnostic fashion (#24526)
Update `huggingface_hub` commit sha (#24527)
set model to training mode before accelerate.prepare (#24520)
[`T5`] Add T5ForQuestionAnswering and MT5ForQuestionAnswering (#24481)
Update hyperparameter_search.py (#24515)
use accelerate autocast in jit eval path, since mix precision logic is‚Ä¶ (#24460)
üåê [i18n-KO] Translated `tflite.mdx` to Korean (#24435)
Fix poor past ci (#24485)
Fix TypeError: Object of type int64 is not JSON serializable (#24340)
Generate: `min_tokens_to_keep` has to be `>= 1` (#24453)
Generate: `group_beam_search` requires `diversity_penalty>0.0` (#24456)
üö®üö® Fix group beam search (#24407)
Fix link in utils (#24501)
Compute `dropout_probability` only in training mode (SpeechT5) (#24498)
Fix 'local_rank' AttiributeError in Trainer class (#24297)
Compute `dropout_probability` only in training mode (#24486)
[`InstructBlip`] Add accelerate support for instructblip (#24488)
Add support for for loops in python interpreter (#24429)
Update token_classification.md (#24484)
Update `InstructBlipModelIntegrationTest` (#24490)
deepspeed z1/z2 state dict fix (#24489)
when resume from peft checkpoint, the model should be trainable (#24463)
[`pipeline`] Fix str device issue (#24396)
Update AlbertModel type annotation (#24450)
Fix tpu_metrics_debug (#24452)
add missing alignment_heads to Whisper integration test (#24487)
Add InstructBLIP (#23460)
Improved keras imports (#24448)
Update `JukeboxConfig.from_pretrained` (#24443)
Allow dict input for audio classification pipeline (#23445)
fixes issue when saving fsdp via accelerate's FSDP plugin (#24446)
Fix some `TFWhisperModelIntegrationTests` (#24428)
Fix typo (#24440)
Replace python random with torch.rand to enable dynamo.export (#24434)
fix the grad_acc issue at epoch boundaries (#24415)
[`Trainer`] Fix `.to` call on 4bit models (#24444)
[AutoModel] Add AutoModelForTextEncoding (#24305)
[llama] Fix comments in weights converter (#24436)
Save `site-packages` as cache in CircleCI job (#24424)
Clarify batch size displayed when using DataParallel (#24430)
Refactor hyperparameter search backends (#24384)
TF CI fix for Segformer (#24426)
Update RayTune doc link for Hyperparameter tuning (#24422)
Fix `save_cache` version in `config.yml` (#24419)
Revert "Fix gradient checkpointing + fp16 autocast for most models" (#24420)
[`bnb`]¬†Fix bnb serialization issue with new release (#24416)
Skip `test_conditional_generation_pt_pix2struct` in Past CI (torch < 1.11) (#24417)
TF safetensors reduced mem usage (#24404)
[ASR pipeline] Check for torchaudio (#23953)
Explicit arguments in `from_pretrained` (#24306)
Remove redundant code from TrainingArgs (#24401)
add word-level timestamps to Whisper (#23205)
Check auto mappings could be imported via `from transformers` (#24400)
Clean up dist import (#24402)
Fix gradient checkpointing + fp16 autocast for most models (#24247)
[Trainer] Fix optimizer step on PyTorch TPU (#24389)
fix type annotation for debug arg (#24033)
byebye Hub connection timeout - Recast (#24399)
Generate: add SequenceBiasLogitsProcessor (#24334)
Add `ffmpeg` for `doc_test_job` on CircleCI (#24397)
[docs] Fix NLLB-MoE links (#24388)
Update deprecated torch.ger (#24387)
Migrate doc files to Markdown. (#24376)
[Wav2Vec2 - MMS] Correct directly loading adapters weights (#24335)
[GPTNeoX] Nit in config (#24349)
[Whisper Docs] Nits (#24367)
Skip a tapas (tokenization) test in past CI (#24378)
Better test name and enable pipeline test for `pix2struct` (#24377)
style: add BitsAndBytesConfig __repr__ function (#24331)
[Tokenizer doc] Clarification about `add_prefix_space` (#24368)
Add a check in `ImageToTextPipeline._forward` (#24373)
Rename test to be more accurate (#24374)
Remove print statement
[Whisper] Make tests faster (#24105)
[modelcard] add audio classification to task list (#24363)
Update tiny models for pipeline testing. (#24364)
TensorFlow CI fixes (#24360)
Fix resuming PeftModel checkpoints in Trainer  (#24274)
Allow passing kwargs through to TFBertTokenizer (#24324)
Respect explicitly set framework parameter in pipeline (#24322)
Fix the order in `GPTNeo`'s docstring (#24358)
[Doc Fix] Fix model name path in the transformers doc for AutoClasses (#24329)
docs: add BentoML to awesome-transformers (#24344)
Fix link to documentation in Install from Source (#24336)
Fix ImageGPT doctest (#24353)
Make `AutoFormer` work with previous torch version (#24357)
Update MMS integration docs  (#24311)
Fix device issue in `SwitchTransformers` (#24352)
Fix `KerasMetricCallback`: pass `generate_kwargs` even if `use_xla_generation` is False (#24333)
Clean up disk sapce during docker image build for `transformers-pytorch-gpu` (#24346)
byebye Hub connection timeout (#24350)
pin `apex` to a speicifc commit (for DeepSpeed CI docker image) (#24351)
üåê [i18n-KO] Fixed `tutorial/preprocessing.mdx` (#24156)
error bug on saving distributed optim state when using data parallel (#24108)
Adding ddp_broadcast_buffers argument to Trainer (#24326)
Add test for proper TF input signatures (#24320)
Fix ImageGPT doc example (#24317)
Tied weights load (#24310)
Fix ner average grouping with no groups (#24319)
Big TF test cleanup (#24282)
Byebye pytorch 1.9 (#24080)
Fix functional TF Whisper and modernize tests (#24301)
[`SwitchTransformers`] Fix return values (#24300)
Update test versions on README.md (#24307)
Make `can_generate` as class method (#24299)
Beam search type (#24288)
Update tokenizer_summary.mdx (grammar) (#24286)
[Docs] Fix the paper URL for MMS model (#24302)
[EnCodec] Changes for 32kHz ckpt (#24296)
deepspeed init during eval fix (#24298)
Update README_zh-hans.md (#24181)
[Docs] Improve docs for MMS loading of other languages (#24292)
Fix image segmentation tool bug (#23897)
[fix] bug in BatchEncoding.__getitem__ (#24293)
Split common test from core tests (#24284)
remove unused is_decoder parameter in DetrAttention (#24226)
Fix LLaMa beam search when using parallelize (#24224)
Fix `check_config_attributes`: check all configuration classes (#24231)
Fix bug in slow tokenizer conversion, make it a lot faster (#24266)
Add MMS CTC Fine-Tuning (#24281)
[WIP] add EnCodec model (#23655)
Clean up old Accelerate checks (#24279)
Fix Debertav2 embed_proj (#24205)
`Pix2StructImageProcessor` requires `torch>=1.11.0` (#24270)
Update check of core deps (#24277)
Adapt Wav2Vec2 conversion for MMS lang identification (#24234)
TF: CTRL with native embedding layers (#23456)
Skip some `TQAPipelineTests` tests in past CI (#24267)
QA doc: import torch before it is used (#24228)
Fix URL in comment for contrastive loss function (#24271)
update FSDP save and load logic (#24249)
docs wrt using accelerate launcher with trainer (#24250)
Skip `GPT-J` fx tests for torch < 1.12 (#24256)
Stop storing references to bound methods via tf.function (#24146)
Fix how we detect the TF package (#24255)
Update urls in warnings for rich rendering (#24136)
Add `torch >=1.12` requirement for `Tapas` (#24251)
Generate: GenerationConfig can overwrite attributes at from_pretrained time (#24238)
TF: standardize `test_model_common_attributes` for language models (#23457)
[Time Series] use mean scaler when scaling is a boolean True (#24237)
Tied params cleanup (#24211)
deprecate `use_mps_device` (#24239)
fix overflow when training mDeberta in fp16 (#24116)
Safely import pytest in testing_utils.py (#24241)
Improving error message when using `use_safetensors=True`. (#24232)
Update `(TF)SamModelIntegrationTest` (#24199)
fix: TextIteratorStreamer cannot work with pipeline (#23641)
Fix README copies
Add the number of `model` test failures to slack CI report (#24207)
Finish dataloader integration (#24201)
Update `WhisperForAudioClassification` doc example (#24188)
Remove unnecessary aten::to overhead in llama (#24203)
Skip RWKV test in past CI (#24204)
Fix steps bugs in no trainer examples (#24197)
Fix `_load_pretrained_model` (#24200)
üö®üö®üö® Replace DataLoader logic for Accelerate in Trainer, remove unneeded tests üö®üö®üö® (#24028)
üåê [i18n-KO] Translated tasks_summary.mdx to Korean (#23977)
Generate: detect special architectures when loaded from PEFT (#24198)
typo: fix typos in CONTRIBUTING.md and deepspeed.mdx (#24184)
Update `GPTNeoXLanguageGenerationTest` (#24193)
Fix device issue in `OpenLlamaModelTest::test_model_parallelism` (#24195)
Generate: force caching on the main model, in assisted generation (#24177)
[i18n]Translated "attention.mdx" to korean (#23878)
Change ProgressCallback to use dynamic_ncols=True (#24101)
Fix push to hub (#24187)
Fix `Wav2Vec2` CI OOM  (#24190)
Avoid OOM in doctest CI (#24139)
[tests] fix bitsandbytes import issue (#24151)
Tool types (#24032)
Fix typo in streamers.py (#24144)
[documentation] grammatical fixes in image_classification.mdx (#24141)
Fix Pipeline CI OOM issue (#24124)
[BlenderBotSmall] Update doc example (#24092)
[lamaTokenizerFast] Update documentation (#24132)
[`SAM`] Fix sam slow test (#24140)
Fix XGLM OOM on CI (#24123)
Fix SAM OOM issue on CI (#24125)
Fix TF Rag OOM issue (#24122)
fix bugs with trainer (#24134)
Generate: PT's `top_p` enforces `min_tokens_to_keep` when it is `1` (#24111)
Correctly build models and import call_context for older TF versions (#24138)
[`bnb`] Fix bnb config json serialization (#24137)
PLAM => PaLM (#24129)
[Lllama] Update tokenization code to ensure parsing of the special tokens [core] (#24042)
Avoid `GPT-2` daily CI job OOM (in TF tests) (#24106)
Fix typo in Llama docstrings (#24020)
add trust_remote_code option to CLI download cmd (#24097)
[`GPT2`] Add correct keys on `_keys_to_ignore_on_load_unexpected` on all child classes of `GPT2PreTrainedModel` (#24113)
fix get_keys_to_not_convert function (#24095)
Update the pin on Accelerate (#24110)
[`Trainer`] Correct behavior of `_load_best_model` for PEFT models (#24103)
reset accelerate env variables after each test (#24107)
Fix a tiny typo in `WhisperForConditionalGeneration::generate` docstring (#24045)
v4.31.0.dev0
Add AzureOpenAiAgent (#24058)
Up pinned accelerate version (#24089)
fix accelerator prepare during eval only mode (#24014)
Do not prepare lr scheduler as it as the right number of steps (#24088)
fix executable batch size issue (#24067)
Update delete_doc_comment_trigger.yml (#24084)
Fix expected value in tests of the test fetcher (#24077)
[doc build] Use secrets (#24079)
Make the TF dummies even smaller (#24071)
Be nice to TF (#24076)
[`bnb`] Fix bnb skip modules (#24043)
Fix `is_optimum_neuron_available` (#23961)
[`Hub`] Add `safe_serialization` in push_to_hub (#24074)
Support PEFT models when saving the model using trainer (#24073)
Add support for non-rust implemented tokenization for `__getitem__` method. (#24039)
[Wav2Vec2] Fix torch srcipt (#24062)
Generate: increase left-padding test atol (#23448)
Remote code improvements (#23959)
Fix device placement for model-parallelism in generate for encoder/de‚Ä¶ (#24025)
bring back `filtered_test_list_cross_tests.txt` (#24055)
Use new parametrization based weight norm if available (#24030)
Move TF building to an actual build() method (#23760)
Oops, missed one (#24054)
Reduce memory usage in TF building (#24046)
Act on deprecations in Accelerate no_trainer examples (#24053)
Tiny fix for `check_self_hosted_runner.py` (#24052)
Add TimmBackbone model (#22619)
Modification of one text example file should trigger said test (#24051)
Prevent ZeroDivisionError on `trainer.evaluate` if model and dataset are tiny  (#24049)
Use TruncatedNormal from Keras initializers (#24036)
Fixing single candidate_label return. (#24023)
Add check for tied parameters (#24029)
üåê [i18n-KO] Translated `bertology.mdx` to Korean (#23968)
üåê [i18n-KO] Translated `language-modeling.mdx` (#23969)
Pin `deepspeed` to `0.9.2` for now (#24024)
Fix `MobileViTV2` checkpoint name (#24018)
üåê [i18n-KO] Translated `tasks_explained.mdx` to Korean (#23844)
TensorBoard callback no longer adds hparams (#23999)
Pix2Struct: fix wrong broadcast axis of attention mask in visual encoder (#23976)
expose safe_serialization argument in the pipeline API (#23775)
Auto tokenizer registration (#23965)
Update README.md (#24022)
Skip `test_multi_gpu_data_parallel_forward` for `MobileViTV2ModelTest` (#24017)
fix trainer slow tests related to hyperparam search (#24011)
Fix typo in doc comment of BitsAndBytesConfig (#23978)
Bump cryptography from 39.0.1 to 41.0.0 in /examples/research_projects/decision_transformer (#23964)
Added time-series blogs to the models (#23857)
Add an option to reduce compile() console spam (#23938)
[Whisper Tokenizer] Skip special tokens when decoding with timestamps (#23945)
Trainer: fixed evaluate raising `KeyError` for ReduceLROnPlateau (#23952)
üåê [i18n-KO] Translated object_detection.mdx to Korean (#23164)
add new mms functions to doc (#23954)
Add MobileViTv2 (#22820)
[MMS] Scaling Speech Technology to 1,000+ Languages | Add attention adapter to Wav2Vec2 (#23813)
Fix `ReduceLROnPlateau` object has no attribute 'get_last_lr' (#23944)
use _make_causal_mask in clip/vit models (#23942)
Modify device_map behavior when loading a model using from_pretrained (#23922)
#23675 Registering Malay language (#23689)
Revert "Update stale.yml to use HuggingFaceBot" (#23943)
Make TF ESM inv_freq non-trainable like PyTorch (#23940)
Update stale.yml to use HuggingFaceBot (#23941)
rename DocumentQuestionAnsweringTool parameter input to match docstring (#23939)
Pin rhoknp (#23937)
Fix doc string nits (#23929)
Effectively allow `encoder_outputs` input to be a tuple in pix2struct (#23932)
[Flax Whisper] Update decode docstring (#23908)
Skip device placement for past key values in decoder models (#23919)
[PushToHub] Make it possible to upload folders (#23920)
Update the update metadata job to use upload_folder (#23917)
Re-enable squad test (#23912)
remove the extra `accelerator.prepare`  (#23914)
Bug fix - flip_channel_order for channels first images (#23701)
Empty circleci config (#23913)
Raise error if loss can't be calculated - ViT MIM  (#23872)
add conditional statement for auxiliary loss calculation (#23899)
[`RWKV`] Fix RWKV 4bit (#23910)
Upgrade safetensors version (#23911)
fix: Replace `add_prefix_space` in `get_prompt_ids` with manual space for FastTokenizer compatibility (#23796)
Move import check to before state reset (#23906)
[`bnb`] add warning when no linear  (#23894)
Unpin numba (#23162)
ensure banned_mask and indices in same device (#23901)
Support shared tensors (#23871)
Fix Trainer when model is loaded on a different GPU (#23792)
fix(configuration_llama): add `keys_to_ignore_at_inference` to `LlamaConfig` (#23891)
Skip failing test for now
accelerate deepspeed and gradient accumulation integrate (#23236)
Add TensorFlow implementation of  EfficientFormer (#22620)
Fix last instances of kbit -> quantized (#23797)
Fix bug leading to missing token in GPTSanJapaneseTokenizer (#23883)
shift torch dynamo handling to accelerate (#23168)
move fsdp handling to accelerate (#23158)
üåê [i18n-KO] Translated `pad_truncation.mdx` to Korean (#23823)
Smangrul/accelerate ddp integrate (#23151)
Smangrul/accelerate mp integrate (#23148)
Adds AutoProcessor.from_pretrained support for MCTCTProcessor (#23856)
Editing issue with pickle def with lambda function (#23869)
[from_pretrained] imporve the error message when `_no_split_modules` is not defined (#23861)
#23388 Issue: Update RoBERTa configuration (#23863)
[LlamaTokenizerFast] nit update `post_processor` on the fly (#23855)
Update collating_graphormer.py (#23862)
Adds a FlyteCallback (#23759)
üåê [i18n-KO] Translated `troubleshooting.mdx` to Korean (#23166)
[i18n-KO] Translated video_classification.mdx to Korean (#23026)
üåê [i18n-KO] Translated `fast_tokenizers.mdx` to Korean (#22956)
fix Whisper tests on GPU (#23753)
TF SAM shape flexibility fixes (#23842)
add type hint in pipeline model argument (#23740)
[Time-Series] Autoformer model (#21891)
Enable code-specific revision for code on the Hub (#23799)
Log the right train_batch_size if using auto_find_batch_size and also log the adjusted value seperately. (#23800)
Fix no such file or directory error (#23783)
no_cuda does not take effect in non distributed environment (#23795)
Update trainer.mdx class_weights example (#23787)
Fix RWKV backward on GPU (#23774)
[OPT] Doc nit, using fast is fine (#23789)
[`Nllb-Moe`] Fix nllb moe accelerate issue (#23758)
Bump tornado from 6.0.4 to 6.3.2 in /examples/research_projects/visual_bert (#23767)
Bump tornado from 6.0.4 to 6.3.2 in /examples/research_projects/lxmert (#23766)
Fix is_ninja_available() (#23752)
[LongFormer] code nits, removed unused parameters  (#23749)
Revamp test selection for the example tests (#23737)
Fix psuh_to_hub in Trainer when nothing needs pushing (#23751)
Add LlamaIndex to awesome-transformers.md (#23484)
Fix `pip install --upgrade accelerate` command in modeling_utils.py (#23747)
Remove the last few TF serving sigs (#23738)
Enable prompts on the Hub (#23662)
Fix sagemaker DP/MP (#23681)
Fix the regex in `get_imports` to support multiline try blocks and excepts with specific exception types (#23725)
[Whisper] Reduce batch size in tests (#23736)
Overhaul TF serving signatures + dummy inputs (#23234)
fix: Whisper generate, move text_prompt_ids trim up for max_new_tokens calculation (#23724)
fix: delete duplicate sentences in `document_question_answering.mdx` (#23735)
TF SAM memory reduction (#23732)
Minor awesome-transformers.md fixes (#23453)
Better TF docstring types (#23477)
fix gptj could not jit.trace in GPU (#23317)
fix: use bool instead of uint8/byte in Deberta/DebertaV2/SEW-D to make it compatible with TensorRT (#23683)
Export to ONNX doc refocused on using optimum, added tflite (#23434)
Paged Optimizer + Lion Optimizer for Trainer (#23217)
4-bit QLoRA via bitsandbytes (4-bit base model + LoRA) (#23479)
add GPTJ/bloom/llama/opt into model list and enhance the jit support (#23291)
Fix some docs what layerdrop does (#23691)
fix: load_best_model_at_end error when load_in_8bit is True (#23443)
Skip `TFCvtModelTest::test_keras_fit_mixed_precision` for now (#23699)
is_batched fix for remaining 2-D numpy arrays (#23309)
[`Blip`] Fix blip doctest (#23698)
TF version compatibility fixes (#23663)
[`SAM`]¬†Fixes pipeline and adds a dummy pipeline test (#23684)
Fix a `BridgeTower` test (#23694)
üåê [i18n-KO] Translated `tasks/monocular_depth_estimation.mdx` to Korean (#23621)
Making `safetensors` a core dependency. (#23254)
Fix PyTorch SAM tests (#23682)
Fix typo in a parameter name for open llama model (#23637)
Add PerSAM [bis] (#23659)
Bump requests from 2.22.0 to 2.31.0 in /examples/research_projects/lxmert (#23668)
Bump requests from 2.22.0 to 2.31.0 in /examples/research_projects/visual_bert (#23670)
Bump requests from 2.27.1 to 2.31.0 in /examples/research_projects/decision_transformer (#23673)
small fix to remove unused eos in processor when it's not used. (#23408)
[image-to-text pipeline] Add conditional text support + GIT (#23362)
Update workflow files (#23658)
Update all no_trainer with skip_first_batches (#23664)
Fix SAM tests and use smaller checkpoints (#23656)
changing the requirements to a cpu torch version that works (#23483)
Fix wav2vec2 is_batched check to include 2-D numpy arrays (#23223)
Bugfix: LLaMA layer norm incorrectly changes input type and consumers lots of memory (#23535)
Muellerzr fix deepspeed (#23657)
Fix accelerate logger bug (#23650)
Fix tensor device while attention_mask is not None (#23538)
Remove erroneous `img` closing tag (#23646)
Debug example code for MegaForCausalLM (#23382)
Fix `tests/repo_utils/test_get_test_info.py` (#23485)
Fix confusing `transformers` installation in CI (#23465)
Fix DeepSpeed stuff in the nightly CI (#23478)
[`Blip`] Remove redundant shift right (#23153)
Fix: Change tensors to integers for torch.dynamo and torch.compile compatibility (#23475)
Fix PretrainedConfig `min_length` docstring (#23471)
Fix parallel mode check (#23409)
Fix `transformers`' DeepSpeed CI job (#23463)
Use config to set name and description if not present (#23473)
[`RWKV`] Rwkv fix for 8bit inference (#23468)
TF port of the Segment Anything Model (SAM) (#22970)
Remove .data usages in optimizations.py (#23417)
README: Fix affiliation for MEGA (#23394)
feat: Whisper prompting (#22496)
fix bug in group_texts function, that was inserting short batches (#23429)
Clean up CUDA kernels (#23455)
Add an option to log result from the Agent (#23454)
add cleanlab to awesome-transformers tools list (#23440)
Properly guard PyTorch stuff (#23452)
Update tiny models and pipeline tests (#23446)
Less flaky `test_assisted_decoding_matches_greedy_search` (#23451)
Make `RwkvModel` accept `attention_mask` but discard it internally (#23442)
Add local agent (#23438)
TF: GPT2 with native embedding layers (#23436)
Fix DecisionTransformerConfig doctring (#23450)
Fix (skip) a pipeline test for `RwkvModel` (#23444)
üåê [i18n-KO] Translated `tasks/zero_shot_object_detection.mdx` to Korean (#23430)
remove unnecessary print in gpt neox sequence classifier (#23433)
Generate: skip left-padding tests on old models (#23437)
Fix device issue in `SwiftFormerModelIntegrationTest::test_inference_image_classification_head` (#23435)
Remove hardcoded prints in Trainer (#23432)
Encoder-Decoder: add informative exception when the decoder is not compatible (#23426)
Update Bigbird Pegasus tests (#23431)
TF: embeddings out of bounds check factored into function (#23427)
Update error message when Accelerate isn't installed (#23373)
Small fixes and link in the README (#23428)
Top 100 (#22912)
Add Missing tokenization test [electra] (#22997)
[Reland] search model buffers for dtype as the last resort (#23319)
Return early once stop token is found. (#23421)
[`SAM`] fix sam slow test (#23376)
Update 3 docker files to use cu118 (#23406)
Use dict.items to avoid unnecessary lookups. (#23415)
Fix a typo in HfAgent docstring. (#23420)
Update `ConvNextV2ModelIntegrationTest::test_inference_image_classification_head` (#23402)
Run doctest (in PRs) only when some doc example(s) are modified (#23387)
Why crash the whole run when HFHub gives a 50x error? (#23320)
Fix smdistributed check (#23414)
Replace appends with list comprehension. (#23359)
Generate: add test to check KV format (#23403)
Build with non Python files (#23405)
Docs: add link to assisted generation blog post (#23397)
[AutoModel] fix `torch_dtype=auto` in `from_pretrained` (#23379)
Fix translation no_trainer (#23407)
Generate: faster `can_generate` check on TF and Flax (#23398)
[`Pix2Struct`] Add conditional generation on docstring example (#23399)
Minor fixes in transformers-tools (#23364)
üåê [i18n-KO] Translated `asr.mdx` to Korean (#23106)
Fix chat prompt in HFAgent (#23335)
OPT/BioGPT: Improved attention mask shape exception (#23270)
Update `test_batched_inference_image_captioning_conditioned` (#23391)
Fix `RwkvModel` (#23392)
Use `mkstemp` to replace deprecated `mktemp` (#23372)
Replace NumPy Operations with JAX NumPy Equivalents for JIT Compilation Compatibility (#23356)
Added type hints for `Graphormer` pytorch version (#23073)
Fix test typos - audio feature extractors (#23310)
Skip failing `AlignModelTest::test_multi_gpu_data_parallel_forward` (#23374)
[Bugfix] `OPTDecoderLayer` does not return attentions when `gradient_checkpointing` and `training` is enabled. (#23367)
Revert "Only add files with modification outside doc blocks" (#23371)
Fix `OwlViTForObjectDetection.image_guided_detection` doc example (#23370)
Fix `BigBirdForMaskedLM` doctest (#23369)
Fix some `is_xxx_available` (#23365)
Typo suggestion (#23360)
Fix issue introduced in PR #23163 (#23363)
Removing one of the twice defined position_embeddings in LongFormer (#23343)
Use cu118 with cudnn >= 8.6 in docker file (#23339)
replaced assert with raise ValueError for t5, switch_transformers, pix2struct, mt5, longt5, gptsan_japanese. (#23273)
Handle padding warning in generation when using `inputs_embeds` (#23131)
OR am I crazy? (#23295)
[docs] Fix Agents and Tools docstring (#23313)
Only add files with modification outside doc blocks (#23327)
Compute the mask in-place, with less memory reads, and on CUDA on `XLNetLMHeadModel` (#23332)
Fix docker image (caused by `tensorflow_text`) (#23321)
Add swiftformer (#22686)
Remove `LanguageIdentificationTool` in `__init__.py` as we don't have it yet (#23326)
Revert "search buffers for dtype" (#23308)
unpin tf prob (#23293)
Style
Fix image segmentation tool test (#23306)
Fix typo in gradio-tools docs (#23305)
Fix broken links in the agent docs (#23297)
Agents extras (#23301)
Add gradient_checkpointing parameter to FlaxWhisperEncoder (#23300)
Better check for packages availability (#23163)
skip `test_run_squad_no_trainer` for now (#23302)
Fix doctest files fetch issue (#23277)
Convert numpy arrays to lists before saving the evaluation metrics as json (#23268)
Update transformers_agents.mdx (#23289)
Update custom_tools.mdx: fix link (#23292)
Added missing " in CHAT_PROMPT_TEMPLATE (#23287)
Temporarily increase tol for PT-FLAX whisper tests (#23288)
`transformers-cli` -> `huggingface-cli` (#23276)
Add `top_k` argument to post-process of conditional/deformable-DETR (#22787)
Temporary tolerance fix for flaky whipser PT-TF equiv. test (#23257)
[`gpt`] Gpt2 fix half precision causal mask (#23256)
Bring back the PR `Refactor doctests + add CI` to `main` (#23271)
Remove missplaced test file (#23275)
Fix link displayed for custom tools (#23274)
chore: allow protobuf 3.20.3 requirement (#22759)
Render custom tool docs a bit better (#23269)
Fix new line bug in chat mode for agents (#23267)
Refine documentation for Tools (#23266)
pin `tensorflow-probability` in docker files (#23260)
Update Image segmentation description (#23261)
Metadata update (#23259)
Improve Docs of Custom Tools and Agents (#23255)
[docs] Audio task guides fixes (#23239)
CTC example: updated trainer parameters to save tokenizer (#23243)
Test composition (#23214)
Fix `from_config` (#23246)
Revert "[Doctests] Refactor doctests + add CI" (#23245)
v4.30.0.dev0
[Doctests] Refactor doctests + add CI (#22987)
Support ratios for `logging_steps`, `eval_steps`, and `save_steps` (#23235)
Proposed fix for TF example now running on safetensors. (#23208)
Add RWKV-4 (#22797)
Add Japanese translation to accelerate.mdx (#23232)
fix: Update run_qa.py to work with deepset/germanquad (#23225)
Fix typo ; Update output.mdx (#23227)
make opt checkpoint dir name correct (#21660)
audio_utils improvements (#21998)
[SAM] Add resources (#23224)
Pin tensorflow-probability (#23220)
docs: Fix broken link in 'How to add a model...'  (#23216)
New version of Accelerate for the Trainer (#23204)
Skip failing test
Fixing class embedding selection in owl-vit (#23157)
Generate: starcoder ü§ú ü§õ assisted generation (#23182)
Fix hf_argparser.parse_json_file to open file with utf-8 encoding, close file when finished (#23194)
fix random attention for pytorch's bigbird/pegasus_bigbird (#23056)
Update LLaMA docs with arxiv link (#23191)
search buffers for dtype (#23159)
Add FlaxWhisperForAudioClassification model (#23173)
Add `no_trainer` scripts to pre-train Vision Transformers (#23156)
fix: Passing language as acronym to Whisper generate (#23141)
üåê [i18n-KO] docs: ko: Translate `multiple_choice.mdx` (#23064)
fixed whisper positional encoding (#23167)
Add TrOCR resources (#23142)
Revert "Add FlaxWhisperForAudioClassification model" (#23154)
Generate: text generation pipeline no longer emits `max_length` warning when it is not set (#23139)
[docs] Text to speech task guide (#23107)
Add FlaxWhisperForAudioClassification model (#22883)
Pin urllib3
[`GPT-J`] Fix causal mask dtype (#23147)
GPTNeoXForQuestionAnswering (#23059)
gpt2 multi-gpu fix (#23149)
fix resume fsdp (#23111)
Remove typo in perf_train_gpu_many.mdx (#23144)
fix spelling error (#23143)
Add methods to update and verify out_features out_indices (#23031)
GPTNeoForQuestionAnswering (#23057)
Tidy Pytorch GLUE benchmark example (#23134)
Remove redundant print statements (#23133)
Enable to use custom tracer in FX `symbolic_trace` (#23105)
Add focalnet backbone (#23104)
[doc] Try a few ‚â† ways of linking to Papers, users, and org profiles (#22611)
docs: ko: update `_toctree.yml` (#23112)
Add support for beam search's num_return_sequencs flag in flax  (#23082)
Support union types `X | Y` syntax for `HfArgumentParser` for Python 3.10+ (#23126)
Fix ConvNext V2 paramater naming issue (#23122)
Add resources for LayoutLmV2 and reformat documentation resources (#23115)
Generate: better warnings with pipelines (#23128)
improve unclear documentation (#23123)
Generate: correct beam search length on score calculation for multi batch generation (#23127)
Generate: slow assisted generation test (#23125)
[`Doctest`]¬†Fix pix2struct doctest (#23121)
Pin numba for now (#23118)
Fixed default config for `Pix2Struct` model to set `Pix2StructTextModel` to `is_decoder=True` (#23051)
num_noise_spans should be <= num_items #22246 (#22938)
[ONNX] Sam fix (#23110)
[`Flava`] Fix flava `torch.distributed.nn.functional import all_gather` issue (#23108)
Fix check for backword_pos (#23075)
üåê [i18n-KO] Translated `torchscript.mdx` to Korean (#23060)
GPT2ForQuestionAnswering (#23030)
Save the tokenizer and image preprocessor after training a model with the contrastive image-text example (#23035)
added type hints for blip_text pytorch model (#23071)
Bump flask from 2.0.3 to 2.3.2 in /examples/research_projects/decision_transformer (#23094)
üåê [i18n-KO] Translated `tasks/zero_shot_image_classification.mdx` to Korean (#23065)
üåê [i18n-KO] Translated `tasks/question_answering.mdx` to Korean (#23012)
üåê [i18n-KO] Translated `tasks/image_classification.mdx` to Korean (#23048)
Depricate xpu_backend for ddp_backend (#23085)
Fix `convnext` __init__ (#23078)
Add `BioGPTForSequenceClassification` (#22253)
Fix string syntax error in logger warning message (additional comma) (#23083)
Fix grammar error in summarization pipeline (#23080)
Generate: prepare assisted generation for release (#23052)
extend the test files (#23043)
Fix model parallelism for `BridgeTower` (#23039)
üö®üö®üö® [`Blip`] remove labels masking (#23024)
add open-llama model with ckpt (#22795)
Skip pt/flax equivalence tests in pytorch `bigbird` test file (#23040)
Cuda rng_state_all is used when saving in distributed mode so same should also be used when loading (#23045)
[docs] Doc TOC updates (#23049)
üåê [i18n-KO] Translated `model_sharing.mdx` to Korean (#22991)
Add Trainer support for ReduceLROnPlateau (#23010)
Make `_test_xla_generate` less flaky (#22996)
Fix CLAP link across all READMEs (#23032)
Fix bigbird random attention (#21023)
Update `BridgeTowerModelTester` (#23029)
added GPTNeoForTokenClassification (#22908)
added GPTNeoXForTokenClassification (#23002)
[MEGA] nit size test (#23028)
Fix the expected error in `test_offline_mode_pipeline_exception` (#23022)
üåê [i18n-KO] Translated `multilingual.mdx` to Korean (#23008)
[`Pix2Struct`] Fix pix2struct doctest (#23023)
Add methods to PreTrainedModel to use PyTorch's BetterTransformer (#21259)
üö®üö®üö® Use default ignore index in Luke (#23014)
Bring back PartialState DeepSpeed (#22921)
Fix None value when adding info to auto_map (#22990)
[Llama Tokenizer] Fast llama template (#22959)
[`PEFT`] Add HFTracer support for PEFT (#23006)
üö®üö®üö® [`Pix2Struct`] Attempts to fix training issues üö®üö®üö® (#23004)
Add gradient checkpointing to Whisper Flax (#22954)
Remove a failing ONNX test (#23011)
Add TensorFlow Wav2Vec2 for sequence classification (#22073)
üåê [i18n-KO] Translated `token_classification.mdx` to Korean (#22945)
üåê [i18n-KO] Translated `tasks/image_captioning.mdx` to Korean (#22943)
Fix typo in mega.mdx (#22998)
üåê [i18n-KO] Translated `serialization.mdx` to Korean (#22806)
[`DocTest`] Fix correct checkpoint (#22988)
Avoid invalid escape sequences, use raw strings (#22936)
fixed small typo in code example (#22982)
Neptune fix bug init run (#22836)
[`SAM`]¬†Add sam doc (#22984)
üåê [i18n-KO] Fixed `tasks/masked_language_modeling.mdx` (#22965)
Fix `DeepSpeed` CI job link in Past CI (#22967)
Install `accelerete@main` in PyTorch Past CI jobs (#22963)
Generate: assisted generation with sample (take 2) (#22949)
üåê [i18n-KO] translate `create_a_model` doc to Korean (#22754)
Update feature selection in to_tf_dataset (#21935)
Fix TF example in quicktour (#22960)
fix ValueError message in LlamaAttention (#22966)
Reverting Deta cloning mecanism. (#22656)
üåê [i18n-KO] Translated `run_scripts.mdx` to Korean (#22793)
Prepare tests for hfh 0.14 (#22958)
[Fix Bugs] Fix keys in `_load_pretrained_model` (#22947)
Raise error if `stride` is too high in `TokenClassificationPipeline` (#22942)
Decorate `test_codegen_sample_max_time` as flaky (#22953)
Add an attribute to disable custom kernels in deformable detr in order to make the model ONNX exportable (#22918)
üåê [i18n-KO] Translated `tasks/summarization.mdx` to Korean (#22783)
üåê [i18n-KO] Translated `tasks/masked_language_modeling.mdx` to Korean (#22838)
Update tiny models and a few fixes (#22928)
Generate: Add exception path for Donut (#22955)
[CLAP] Doc nits  (#22957)
[i18n-KO] Translated `accelerate.mdx` to Korean (#22830)
Add FocalNet (#21532)
vilt_model (#22930)
Feature to convert videomae huge and small finetuned on kinetics and ssv2 added to the videomae to pytorch converter (#22788)
Small sam patch (#22920)
Fix a minor bug in CI slack report (#22906)
tests: Fix flaky test for NLLB-MoE (#22880)
ddp fixes for training (#22874)
[CI] clap patch fusion test values (#22922)
Hardcode GELU as the intermediate activation for ESM (#22892)
Remove broken test_data symlink in legacy s2s examples (#22876)
fix: GPTNeoX half inference error (#22888)
Expose AutoModelForMaskGeneration (#22910)
Make sam ONNX exportable (#22915)
Fix: Seq2SeqTrainingArgs overriding to_dict for GenerationConfig json support (#22919)
fix bug of CLAP dataloader  (#22674)
Update Swin MIM output class (#22893)
Fix `FillMaskPipelineTests` (#22894)
Add inputs_embeds functionality when generating with GPT-Neox  (#22916)
fix CLAP integration tests (#22834)
Fix Slack report for Nightly CI and Past CI (#22901)
Fix counting in Slack report for some jobs (#22913)
Moved labels to enable parallelism pipeline in Luke model (#22909)
Skip a failing test on main for now (#22911)
moved labels to the same device as logits for LILT model (#22898)
[tensorflow] Add support for the `is_symbolic_tensor` predicate (#22878)
Revert DeepSpeed stuff from accelerate integration (#22899)
Add `automatic-mask-generation` pipeline for Segment Anything Model (SAM) (#22840)
Pin flax & optax version (#22895)
Fix weight tying in TF-ESM (#22839)
Include decoder_attention_mask in T5 model inputs (#22835)
moved labels to the same device as logits for OTP, CODEGEN ,gptj and pixel2struct model (#22872)
[Examples/TensorFlow] minor refactoring to allow compatible datasets to work (#22879)
[`SAM`] Change to `facebook/sam-vit-base` (#22891)
fix warning function call creating logger error (max_length and max_new_tokens) (#22889)
Change schedule CI time (#22884)
Generation: only search for eos_token if set (#22875)
fix: Correct small typo in docstring (#22857)
Fix SAM example in documentation (#22887)
Patching clip model to create mask tensor on the device (#22711)
[`SAM`] Correct arxiv link (#22886)
XGLM: Fix left-padding (PT and TF) (#22828)
Add Segment Anything Model (SAM) (#22654)
Fix to removing ESM special tokens (#22870)
Fixup multigpu local_rank (#22869)
Remove some pipeline skip cases (#22865)
Show diff between 2 CI runs on Slack reports (#22798)
Remove 'main' from doc links (#22860)
use `accelerate@main` in CI (#22859)
feat(model parallelism): move labels to the same device as logits for M2M100 (#22850)
move preprocess_logits_for_metrics before _nested_gather in trainer.e‚Ä¶ (#22603)
fix SpeechT5 doc comments (#22854)
Make ClipSeg compatible with model parallelism (#22844)
Raise err if minimum Accelerate version isn't available (#22841)
Fix from_pretrained when model is instantiated on the meta device (#22837)
Use code on the Hub from another repo (#22814)
Update accelerate version + warning check fix (#22833)
Generate: Add assisted generation (#22211)
Fix `test_eos_token_id_int_and_list_top_k_top_sampling` (#22826)
Fix Past CI not running against the latest `main` (#22823)
üåê [i18n-KO] Fix anchor links for docs `auto_tutorial`, `training` (#22796)
TTS fine-tuning for SpeechT5 (#21824)
Mark auto models as important (#22815)
Introduce `PartialState` as the device handler in the `Trainer` (#22752)
Revert "Use code on the Hub from another repo" (#22813)
Simplify update metadata job (#22811)
Remove accelerate from tf test reqs (#22777)
Fix squeeze into torch 1.x compatible form in llama model (#22808)
Don't use `LayoutLMv2` and `LayoutLMv3` in some pipeline tests (#22774)
Use code on the Hub from another repo (#22698)
üåê [i18n-KO] Translated `tasks/translation.mdx` to Korean (#22805)
Fix sneaky torch dependency in TF example (#22804)
improve(llama): Faster apply_rotary_pos_emb (#22785)
[i18n-KO] fix: docs: ko: sagemaker anchors and  `_toctree.yml` (#22549)
üåê [i18n-KO] Translated `custom_models.mdx` to Korean (#22534)
Fix `test_word_time_stamp_integration` for `Wav2Vec2ProcessorWithLMTest` (#22800)
Generate: add CJK support to TextStreamer (#22664)
Move labels to the same device as logits for Whisper (#22779)
Indexing fix - CLIP checkpoint conversion (#22776)
Seq2SeqTrainer: Evict decoder_input_ids only when it is created from labels (#22772)
Fix word_ids hyperlink (#22765)
Tweak ESM tokenizer for Nucleotide Transformer (#22770)
[WIP]üåê [i18n-KO] Translated `tutorial/proprecssing.mdx` to Korean (#22578)
Fix failing torchscript tests for `CpmAnt` model  (#22766)
Fix a mistake in Llama weight converter log output. (#22764)
Generate: pin number of beams in BART test (#22763)
Pix2struct: doctest fix (#22761)
[Examples] TPU-based training of a language model using TensorFlow (#21657)
üåê [i18n-KO] Translated `sequence_classification.mdx` to Korean (#22655)
Fix `serving_output` for TF composite models (encoder-decoder like models) (#22743)
Revert (for now) the change on `Deta` in #22437 (#22750)
Generate: handle text conditioning with multimodal encoder-decoder models (#22748)
fix(llama): fix LlamaTokenzier (#22746)
[trainer] update url (#22747)
Remove `DS_BUILD_AIO=1` (#22741)
`DocumentQuestionAnsweringPipeline` only for fast ‚ö° tokenizers (#22745)
üåê [i18n-KO] Translated `training.mdx` to Korean (#22670)
Change `torch_dtype` to `str` when `saved_model=True` in `save_pretrained` for TF models (#22740)
[Pix2struct] Simplify generation (#22527)
Make vilt, switch_transformers compatible with model parallelism (#22703)
Indexing fix for gpt_bigcode (#22737)
[Doctest] Add configuration_mvp.py (#22735)
[Doctest] Add configuration_m2m_100.py (#22733)
v4.29.0.dev0
Fix docstrings for TF BLIP (#22618)
Update warning levels (#22727)
add fast support and option (#22724)
`torch.distributed` group initialization for `torch_neuron` disabled when `optimum-neuron` is installed (#22728)
[tests] switch to torchrun (#22712)
Modify pipeline_tutorial.mdx (#22726)
[`bnb`] Let's make serialization of int8 models possible (#22177)
add model resources for CPMAnt (new) (#20906)
Added parallel device usage for GPT-J (#22713)
remove wrong doc in readme (#22723)
Update input values for docstring (#22631)
Fix decorator order (#22708)
Replace -100s in predictions by the pad token (#22693)
Remove 2 failing ONNX conversion tests (#22660)
Clarify stride option (#22684)
Enable naive Pipeline Parallelism training for Gpt neox japanese and san japanese (#22702)
Make it easier to develop without a dev install (#22697)
Update some `MarkupLM` tests' expected values (#22667)
Model parallelism: Moving labels to same devices as the logits are (#22691)
add GPTNeoXForSequenceClassification (#22671)
use __func__ to check can_generate (#22643)
Fix quantization docs typo (#22666)
Make dynamic code work with offline mode (#22661)
(feat): Moving labels to same device as logits for Deit (#22679)
Model parallelism: Moving labels to the same device as logits for BridgeTower models (#22676)
Add GPTBigCode model (Optimized GPT2 with MQA from Santacoder & BigCode) (#22575)
moved labels to the same device as logits for BLOOM, GPT Neo, GPT NeoX, RoBERTa and VIT models (#22663)
Revert migration of setup to pyproject.toml (#22658)
Generate: add API warning to streamers (#22659)
[OPT] Fix default attention mask size (#22649)
[tokenization] do not push special file (#22657)
Small nit, (#22653)
üåê [i18n-KO] Translated `pipeline_tutorial.mdx` to Korean (#22508)
Fix `MegaModel` CI (#22652)
Fix typo (#22650)
Move labels to the same device as logits for LlamaForSequenceClassification and Blip2 (#22596)
üåê[i18n-KO] Translate `autoclass_tutorial` to Korean and Fix the typo of `quicktour` (#22533)
fix FSDP version related issues (#22489)
Update tiny model summary file for recent models (#22637)
[`Blip`] Fix slow tests and doctests with correct values (#22632)
LlamaTokenizerFast Fix (.., from_slow=True). (#22630)
[`bnb`] 8bit models should not be converted to `DDP` (#22628)
A script to add/update `pipeline_model_mapping` systematically (#22180)
update_pip_test_mapping (#22606)
docs: Fix broken link to generation strategies (#22623)
Make tiny model creation + pipeline testing more robust (#22500)
Backbone add mixin tests (#22542)
Seq2SeqTrainer: use unwrapped model to retrieve the generation config (#22584)
Revert error back into warning for byte fallback conversion. (#22607)
Adding Llama FastTokenizer support. (#22264)
feat(model parallelism): moving the labels to the same device as the logits for gpt2 and bart (#22591)
Use native TF checkpoints for the BLIP TF tests (#22593)
Add DePlot + MatCha on `transformers` (#22528)
Adding support for BPE merge creation from scores instead of ids. (#22582)
Fix a typo in one of the BLIP pretrained checkpoint names (#22588)
Sync preprocesses before loading the processor at run_speech_recognition_ctc.py (#21926)
docs: ko: complete `_toctree.yml` (#22581)
Add thousands separator in training summary (#22583)
Fix PT-TF equivalence test for GPT1 (#22586)
Tests: disable `accelerate_tests` mark warnings (#22585)
Move back doctest instructions to setup.cfg (#22587)
Generate: `TextIteratorStreamer` timeout (#22576)
Skip failing test
Fix inverted conditional in TF common test! (#22540)
fix `_no_split_modules` for Whisper model (#22486)
Flax Regnet (#21867)
corrected the code comment for the output of find_pruneable_heads_and_indices  (#22557)
Add TF port of BLIP (#22090)
Soft error whisper. (#22475)
Add id2label and label2id to model's config in run_xnil (#22558)
[`bnb`] Fix typo (#22556)
Remove hack for dynamic modules and use Python functions instead (#22537)
Implemented safetensors checkpoints save/load for Trainer (#22498)
üö®üö®üö® `[NLLB Tokenizer]` Fix the prefix tokens üö®üö®üö® (#22313)
[Roformer] Fixing a bug in RoFormerEncoder where it was ignoring the length of past_key_values when generating as a decoder (#22416)
Generate: Add text streamer decoding options (#22544)
Fix OPTForQuestionAnswering doc string (#22481)
Update test_image_processing_pix2struct.py (#22543)
Skip failing test
[setup] migrate setup script to `pyproject.toml` (#22539)
Generate: Enable easier TextStreamer customization (#22516)
[setup] drop deprecated `distutils` usage (#22531)
Fix missing metrics with multiple eval datasets (#22536)
[`T5`] Enable naive Pipeline Parallelism training for T5 (#22535)
[`Trainer`] Force `is_model_parallel` when model is loaded in multiple GPUs using `accelerate` (#22532)
[BLIP] fix cross attentions for BlipTextEncoder (#22515)
fix LayoutLMv3TokenizerFast subword label after 'ƒ†' token (#21695)
llama docs: fix conversion script url (#22514)
Fix convert_opt_original_pytorch_checkpoint_to_pytorch.py typo (#22526)
Generate: `TextIteratorStreamer` (streamer for gradio) (#22501)
added biogpt token classifier (#22447)
[WIP] docs: ko: sagemaker.mdx (#22509)
Fix llama tokenizer (#22402)
[Time-Series] fix past_observed_mask type (#22076)
Backbone add out indices (#22493)
Update convert_llama_weights_to_hf.py (#22525)
Test fetch v2 (#22367)
Update Neptune callback docstring (#22497)
Bump redis from 4.5.3 to 4.5.4 in /examples/research_projects/decision_transformer (#22494)
Making sure we can use safetensors to serialize all the time. (#22437)
Update `Wav2Vec2ProcessorWithLM` doc example (#22474)
Relax `eos_token_id < 0` checks in `generate()` from `ValueError` to warning (#22472)
(Re-)Enable Nightly + Past CI (#22393)
Docs fix: Multinomial sampling decoding needs "num_beams=1", since by default it is usually not 1. (#22473)
Llama: support for `max_position_embeddings` (#22471)
[NLLB-MoE] `model_type` update for auto mapping (#22470)
Guard imports of PreTrainedTokenizerFast on is_tokenizers_available (#22285)
üö®üö®üö®   Fix ordering of height, width for BLIP image processor (#22466)
Generate: basic token streaming (#22449)
Skip flaky NLLB Moe test for now (#22463)
Rescale image back if it was scaled during PIL conversion (#22458)
Move common properties to BackboneMixin (#21855)
Update: ignore padding support for TransfoXL training when n_clusters==0 (#22457)
Pin ruff (#22455)
Update release instructions (#22454)
Avoid using personal HF token in CI (#22453)
Update Neptune docs (#22452)
Revert "Fix --bf16 option support for Neuron after PR #22300" (#22451)
[`Pix2Struct`] Fix slow test (#22448)
Revert "Error (also in original) model, scaling only q matrix not qk.T dot product (qk.T/sqrt(dim_per_head))" (#22444)
Use real tokenizers if tiny version(s) creation has issue(s) (#22428)
Don't hard error when cache version can't be converted to int (#22427)
[`Generate`] Add conditional generation for multimodal models (#22424)
[`bnb`] fix bnb failing test (#22439)
Hyperparameter search reporting to W&B (#22440)
Add clean_up_tokenization_spaces to config (#22341)
MBart: Fix docs and doctests (#22422)
[performance] ensure `causal_mask` is created directly on device (#22378)
Fix bug in perplexity guide calculations and update perplexity numbers. Fixes #22348 (#22411)
Bump redis from 4.1.4 to 4.5.3 in /examples/research_projects/decision_transformer (#22410)
[neptune] fix checkpoint bug with relative out_dir (#22102)
[WIP]`NLLB-MoE` Adds the moe model (#22024)
Fix quality
Hardware Auto-Setup for Examples (#22319)
Trainer: missing None check (#22404)
Trainer: move Seq2SeqTrainer imports under the typing guard (#22401)
[Pix2Struct] Add support to resize embeddings (#22394)
Transformers env safetensors (#22400)
[`bnb`] Force `requires_grad` to be `False` (#22396)
Generate: support for left-padding on GPTNeoX and Llama (#22382)
Seq2seq trainer generation config arg (#22323)
Wav2Vec2ProcessorWithLM can return N best hypotheses now (#22235)
load_in_8bit now respects 'balanced' device maps in multi-gpu environments (#22377)
Adapt find_tied_parameters to handle breaking change in Accelerate (#22360)
Translated documentation in italian (#22388)
Changed world_size() to get_world_size() bugfix (#22381)
TensorFlow: additional missing `cmake` dependencies in CI (#22383)
[safetensors] don't use in `torch<1.10` (#22370)
Fix TF pipeline job
[Trainer] add disclaimer that full_determinism is slow (#22368)
Resnet flax (#21472)
TensorFlow: pin maximum version to 2.12 (#22364)
Improve error message (#22361)
Pin tensorflow-text to go with tensorflow (#22362)
Update docker files to use official torch 2.0.0 (#22357)
Add Mega: Moving Average Equipped Gated Attention (#21766)
Generate: Add GPTNeoX integration test (#22346)
Fix typo in Greedy Search Description (#22345)
[HFTracer] Make embeddings ops take on the dtype of the weight (#22347)
Automatically create/update tiny models (#22275)
Enable training Llama with model or pipeline parallelism (#22329)
Generate: add test for left-padding support (#22322)
Fix --bf16 option support for Neuron after PR #22300 (#22307)
Added type hints to TFDeiTModel (#22327)
Minor typo in pipeline FillMaskPipeline's documentation. (#22339)
Fix various imports (#22281)
Mention why one needs to specify max_steps in Trainer (#22333)
Fixed gradient checkpoint bug for TimeSeriesTransformer (#22272)
[`MBart`] Add `accelerate` support for MBart (#22309)
[gptj] support older pytorch version (#22325)
Really fix quality due to ruff release
Fix quality due to ruff release
[deepspeed zero3] need `generate(synced_gpus=True, ...)` (#22242)
Fix PipelineTests skip conditions (#22320)
Chunkable token classification pipeline (#21771)
docs: Resolve incorrect type typo in trainer methods (#22316)
Add Pix2Struct (#21400)
Beef up Llama tests (#22314)
Generate: Export TF generate with a TF tokenizer (#22310)
Enforce `max_memory` for device_map strategies (#22311)
Fixed bug to calculate correct xpath_sub_list in MarkupLMTokenizer (#22302)
Fix position embeddings for GPT-J and CodeGen (#22069)
fix: Allow only test_file in pytorch and flax summarization (#22293)
add low_cpu_mem_usage option in run_clm.py example which will benefit‚Ä¶ (#22288)
Enable traced model for text-generation task (#22265)
Add MaskedImageModelingOutput  (#22212)
Final update of doctest (#22299)
[deepspeed] offload + non-cpuadam optimizer exception doc (#22044)
Correct NATTEN function signatures and force new version (#22298)
Restore fp16 support on xla gpu device (#22300)
Time to Say Goodbye, torch 1.7 and 1.8 (#22291)
Add translation perf_infer_gpu_one for it (#22296)
fix more doctests (#22292)
More doctests (#22268)
Fix error in mixed precision training of `TFCvtModel` (#22267)
replace_8bit_linear modules_to_not_convert default value fix (#22238)
Update vision docstring bool masked pos (#22237)
Example of pad_to_multiple_of for padding and truncation guide & docstring update (#22278)
Move torch.compile() wrapping after DDP/FSDP wrapping to ensure correct graph breaks during training (#22279)
Fix doc links (#22274)
Proper map location for optimizer load (#22273)
Rework a bit the LLaMA conversion script (#22236)
Fix balanced and auto device_map (#22271)
Fix the gradient checkpointing bug of the llama model (#22270)
[Trainer] Add optional communication backends for torch.distributed when using GPU (#22247)
Italian translation perf_infer_cpu (#22243)
[Docs] fix typos in some tokenizer docs (#22256)
Update training_args.py -- a nightly install is not required anymore for torch.compile (#22266)
[trainer] param count for deepspeed zero3 (#22193)
Fix Unnecessary move of tensors from CPU to GPU in LlamaRotaryEmbedding (#22234)
Revert "Use `dash==2.8.1` for now for daily CI" (#22233)
Fix natten (#22229)
fix(docs): fix task guide links in model docs (#22226)
Removed .mdx extension in two links (#22230)
Add LlamaForSequenceClassification (#22209)
fix AutoTP in deepspeed could not work for bloom (#22196)
LLaMA house-keeping (#22216)
Depth estimation task guide (#22205)
Use `dash==2.8.1` for now for daily CI (#22227)
fix code example in mgp-str doc (#22219)
fix typos in llama.mdx (#22223)
Hotfix for natten issue with torch 2.0.0 on CircleCI (#22218)
üî•py38 + torch 2 üî•üî•üî•üöÄ (#22204)
fixes a typo in WhisperFeatureExtractor docs. (#22208)
[`XGLM`] Add `accelerate` support for XGLM (#22207)
Temporarily fix ONNX model exporting error (#21830)
Update tiny model creation script (#22202)
LLaMA Implementation (#21955)
LLaMA Implementation (#21955)
Italian Translation of migration.mdx (#22183)
Update expected values in `MgpstrModelIntegrationTest` (#22195)
Fix typo in  Align docs  (#22199)
Fix DeepSpeed CI (#22194)
t5 remove data dependency (#22097)
Update BridgeTowerForContrastiveLearning (#22145)
Regression pipeline device (#22190)
Revert 22152 MaskedImageCompletionOutput changes (#22187)
Fix: unfinished_sequences with correct device  (#22184)
Run all tests by default (#22162)
Load optimizer state on CPU to avoid CUDA OOM (#22159)
v4.28.0.dev0
Revert "Enforce same behavior as PyTorch 2.0 for older versions" (#22163)
[trainer] add `--optim adamw_torch_fused` for pt-2.0+ (#22144)
to_pil - don't rescale if int and in range 0-255 (#22158)
Create MaskedImageCompletionOutput and fix ViT docs (#22152)
Fix big model inference for T5 models in float16 (#22095)
Translation Italian: perf_train_cpu and perf_train_cpu_many (#22151)
Update 2 doctest expected values for torch 2.0.0 (#22148)
Add ConvNeXT V2 (#21679)
Move `is_pipeline_test_to_skip` to specific model test classes (#21999)
[üõ†Ô∏è] Fix-whisper-breaking-changes  (#21965)
docs:  New terms and updates to glossary (#21982)
Prepare daily CI for torch 2.0.0 (#22135)
[Safetensors] Add explicit  flag to from pretrained (#22083)
Remove backend check for torch.compile (#22140)
[deepspeed docs] Activation Checkpointing (#22099)
[trainer] fix bug in grad accum with multiple epochs (#22098)
Enforce same behavior as PyTorch 2.0 for older versions (#22136)
Trainer: let generate pick its inputs (#22108)
[`Whiper`] add `get_input_embeddings` to `WhisperForAudioClassification` (#22133)
Update configuration_align.py (projected_dim=640) (#22139)
Add a new script to check model testers' config (#22063)
Adding Type Hints to TF_Pegasus model (#21941)
Fix doc link for MGP-STR (#22138)
Zero-shot image classification task guide (#22132)
Fix gradient checkpointing bug in trocr (#22126)
Fix gradient checkpointing bug in LongT5 (#22130)
Fix gradient checkpointing bug in xmod (#22129)
[`Blip2`] skip accelerate test (#22124)
 Added big_models.mdx italian translation #17600  (#22115)
Fix gradient checkpointing bug in xlm_roberta_xl (#22128)
Fix gradient checkpointing bug in Trajectory Transformer (#22125)
Fix gradient checkpointing bug in xglm (#22127)
Add pr_checks.mdx Italian translation (#17459) (#22116)
add new model of MGP-STR (#21418)
Add AutoModelForZeroShotImageClassification (#22087)
[Whisper] Remove embed_tokens from encoder docstring (#21996)
Revert "[GPT2] Propose fix for #21080" (#22093)
Fix imports of TF MobileViT (#22065)
GPT-J specific half precision on CPU note (#22086)
handle numpy inputs in whole word mask data collator (#22032)
Fix hint in src/transformers/modeling_utils.py (#22074)
Fix gradient checkpointing bug in Speecht5 (#22080)
Generate - Fix broken documentation links (#22078)
Fix small typo in flan-ul2.mdx (#22068)
[GPT2] Propose fix for #21080 (#21853)
Fix gradient checkpointing bug in switch transformer (#22081)
Fix gradient checkpointing bug in Speech2Text (#22079)
Add a progress bar for the total download of shards (#22062)
Fix case when using --gradient_accumulation_steps with DDP disabled. (#22007)
Update tiny model creation script (#22058)
Add setters by type of args to TrainingArguments (#21570)
Skip 3 tests for `WhisperEncoderModelTest` (#22060)
Edit the docstring of `image_processing_donut` to match code (#22033)
[deepspeed] offload + non-cpuadam optimizer exception (#22043)
rm $ symbol from code block from contributing.md (#22057)
pt-to-tf model architecture override (#22055)
Return analysis for hyperparameter_search with Ray backend (#22040)
Show the number of `huggingface_hub` warnings in CI report (#22054)
Remove set_access_token usage + fail tests if FutureWarning (#22051)
Can't install tf2 on M1 Chip by default (#22046)
Docs Improvement - In ZSH, not using ' ' around pip install fails, fix it (#22045)
[21737][T5]: Fix gradient checkpoint bug (#22036)
Update ALIGN docs (#22025)
Bug fix: token classification pipeline while passing offset_mapping (#22034)
Mark all `BridgeTower` tests slow for now (#22039)
Avoid `text_config_dict` and `vision_config_dict` being saved  for CLIP-like models (#22035)
fixes the gradient checkpointing of whisper (#22019)
[examples/speech-recognition] Add SpecAugment to run_speech_recognition_seq2seq.py (#21942)
Add tokenize_kwargs parameter definition in the FeatureExtractionPipeline (#22031)
Fix test for torchneuroncore in Trainer (#22028)
[WIP] Add BridgeTowerForContrastiveLearning (#21964)
[`bnb`] Fix bnb error message (#22026)
Update `AudioClassificationPipelineTests::test_small_model_pt` for PT 2.0.0 (#22023)
update: bertology paper (#22012)
VideoMAE doctest - use valid dummy pixel values (#22022)
Generate - add 1 to cur_len to make up the new beam length (#21993)
Update tiny model creation script and some others files (#22006)
[Time-Series] informer model (#21099)
[DETR and friends] Remove is_timm_available (#21814)
[TF] Fix creating a PR while pushing in TF framework (#21968)
Stop requiring Torch for our TF examples! (#21997)
[Whisper] Add model for audio classification (#21754)
Skip `test_multi_gpu_data_parallel_forward` for some model tests (#21991)
Update `notification_service.py` (#21992)
Remove unneeded casts to bool (#21983)
[DETR, YOLOS] Fix device bug (#21974)
Fix MinNewTokensLengthLogitsProcessor when used with a list of eos tokens (#21959)
Add check before int casting for PIL conversion (#21969)
Update `Jukebox` tests (#21984)
docs: improve clarity for language modeling (#21952)
Fix gradient checkpointing bug in ESM (#21980)
Fix gradient checkpointing bug in Codegen (#21979)
Fix gradient checkpointing bug in BlipText (#21978)
Fix gradient checkpointing bug in Blenderbot Small (#21977)
Fix gradient checkpointing bug in BigBird Pegasus (#21976)
Update expected values for `test_xglm_sample` (#21975)
Add TF contrastive image text finetuning example (#21939)
Use larger atol in `torch.allclose` for some tests (#21966)
Add missing parameter definition in layoutlm config (#21960)
[Generate] Fix gradient_checkpointing and use_cache bug for BLOOM (#21956)
Fix bert issue (#21963)
Disable DDP for neuron (#21953)
[CI] Fix ci  (#21940)
Update expected values in `XLMProphetNetModelIntegrationTest` (#21957)
Fixed gradient_checkpointing/use_cache bug in blenderbot (#21833)
Fix gradient checkpointing bug in Roformer (#21946)
Fix gradient checkpointing bug in Rembert (#21945)
Fix gradient checkpointing bug in Pegasus (#21944)
Fix gradient checkpointing bug in OPT (#21943)
[Whisper] Fix feature normalization in `WhisperFeatureExtractor` (#21938)
[CLAP] Support batched inputs for CLAP. Fixes pipeline issues (#21931)
Update README logo (#21933)
[Flan-UL2] Add-flan-ul2 (#21929)
Fix wrong documentation about DataCollator padding defaults (#21919)
Avoid failure in `check_repo.py` due to missing backends (#21930)
Fix `AlignModelTest` tests (#21923)
feat: filter try/except when looking at custom code (#21914)
Cleanup more auto mapping names  (#21909)
Use large VM for `repo_utils_job` (#21928)
Update `model_split_percents` for `WhisperModelTest` (#21922)
Fix gradient checkpointing megatron bert (#21921)
Fix gradient checkpointing bug in mvp (#21920)
Fix gradient checkpointing bug in MBart (#21918)
faster forward following what is done for images (#21906)
Fix doctests for TFVisionTextDualEncoder (#21910)
Avoid modeling tests run in pipeline CI jobs (#21911)
[time series] Add Time series inputs tests (#21846)
Faster zero shot image (#21897)
Temporarily skip 3 tests in `BridgeTowerModelTest` (#21908)
Add Blip and Blip2 for pipeline tests (#21904)
Refactor whisper asr pipeline to include language too. (#21427)
Make schedulers picklable by making lr_lambda fns global (#21768)
Prophetnet batch dimension inversion fix (#21870)
Clean up auto mapping names (#21903)
Mark pipeline tests to skip them easily (#21887)
Fix gradient checkpointing bug marian (#21842)
Fix gradient checkpointing bug M2M 100 (#21841)
Fix gradient checkpointing bug LED (#21840)
fsdp bf16 enable autocast (#21847)
[GPT-J] add deprecation warning (#21869)
fix typo in Bart's attention (#21898)
[Whisper] Add rescaling function with `do_normalize` (#21263)
[T5 doc] Fix confusing documentation about `d_kv` (#21896)
Add `inputs_embeds` functionality when generating with BioGPT  (#21889)
Use PyAV instead of Decord in examples (#21572)
[ZAC] fix ci daily  (#21893)
[Refactor] Relative imports wherever we can (#21880)
fix checkpoint (#21874)
Fix `test_load_default_pipelines_pt` for `ClapModel` (#21886)
Fix `WhisperModelTest`  (#21883)
Fix Gradient checkpointing bug BigBird (#21882)
Add ALIGN to transformers (#21741)
Add TFVisionTextDualEncoder (#21873)
Make loading of pretrained gpt2 faster by avoiding initialization of Conv1D's weights (#21879)
Add check for different embedding types in examples (#21881)
Add an utility file to get information from test files (#21856)
[doc] deepspeed tests (#21859)
update FSDP and add XLA-FSDP documentation (#21812)
Removed BLIP mention from the troubleshooting guide (#21872)
[`Blip`] Fix blip doctest (#21868)
Italian translation of community.mdx (#21871)
Change the way tensor is reshaped in BartAttention (from .view to .reshape) (#21860)
[deepspeed] check whether model is NLP one instead of counting on input type (#21800)
Fix gradient checkpointing bug Bart (#21866)
Flax beam search fix (#21857)
[ConvBert] Fix #21523 (#21849)
prepare for "__floordiv__ is deprecated  and its behavior will change in a future version of pytorch" (#20211)
Fix flaky test for log level (#21776)
Improve TF weight loading, especially PT crossloading (#21792)
üî•Rework pipeline testing by removing `PipelineTestCaseMeta` üöÄ (#21516)
Add loss for BridgeTowerForMaskedLM and BridgeTowerForImageAndTextRetrieval (#21684)
[`Blip2`] Fix Blip-2 multi gpu (#21707)
Make Slack CI reporting stronger (#21823)
Add: task guide for zero shot object detection (#21829)
[GPTJ] Fix gradient checkpointing bug  (#21794)
Fix the issue of blip model returning loss even when the label is not provided.  (#21811)
[`Blip2`] Add `Blip2Model` (#21817)
[`T5`] Fix torchquant issue (#21843)
Fix tf random token masking probability in data collator (#21834)
Fix gradient checkpointing imagegpt (#21816)
Fix gradient checkpointing bug in git (#21818)
check for None forced tokens (#21793)
Fix gradient checkpointing bug BioGpt (#21844)
Rename `MobileViTModelTest` to `TFMobileViTModelTest` (#21825)
introduce `logger.warning_once` and use it for grad checkpointing code (#21804)
Fix quality with `ruff==0.0.253` (#21828)
Inheritance-based framework detection (#21784)
Fix gradient checkpointing bug in gptneox (#21815)
Fix nn.init.trunc_normal_ call on torch.float16 data (#21789)
Fix PyTorch Perceiver `PerceiverFourierPositionEncoding` with fp16 (#21787)
[`tests`] add `accelerate` marker (#21743)
[torch] remove deprecated uint8 in favor of bool (#21384)
[Pipeline] Add zero shot audio classificatoin pipeline (#21600)
[FX tracer] Make `concrete_args` from outside available (#21775)
Fix en documentation typos (#21799)
Fix type in gpt2 config docstring (#21782)
[examples/summarization] deal with `max_length` and `num_beams` (#21740)
Fix resume_from_checkpoint for deepspeed (#21735)
[SpeechT5] Fix HiFiGAN tests (#21788)
[GPT2, ProphetNet] Fix gradient checkpointing bug (#21772)
[time series] updated expected values for integration test. (#21762)
Generate - update cookie cutters to not initialize cache with training and gradient checkpointing (#21759)
Fix-ci-whisper (#21767)
[Whisper] Add SpecAugment (#21298)
[Flax] Fix erroneous kwargs being passed to generate config (#21765)
Different behavior in DistilBERT when using "inputs_embeds" (#21752)
[Examples] Generalise run audio classification for log-mel models (#21756)
[Flax] adding support for batch norm layers (#21581)
fix: Change is_last chunk calc and add conditional break in chunk_iter (#21612)
Graphormer fix  (#21699)
[deepspeed tests] fix issues introduced by #21700 (#21769)
Auto api Value Error addition to Troubleshoot (#21708)
Added Type Hints for modeling_tf_encoder_decoder.py (#21673)
Skip test_log_level for now
Generate: Fix GIT batched captioning (#21738)
[`GPTNeo`] Fix gradient checkpointing bug (#21733)
Fix 2 quicktour file doctest (#21742)
Update doctest GH workflow file (#21744)
Make ImageProcessorMixin compatible with subfolder kwarg (#21725)
typos in french documentation (#21750)
Added "Open in Colab" to task guides  (#21729)
Fix to KerasMetricCallback when the model returns unstructured output (#21727)
[SpeechT5HifiGan] Handle batched inputs (#21702)
Fix `GPTSanJapaneseModel` (#21731)
Fix `ErnieMEmbeddings` device issue (#21726)
Change doc example for `BigBirdForQuestionAnswering` (#21723)
Remove `gptsan_japanese` from doctest list to avoid GPU OOM (#21722)
Respect documentation on passive log level (#21700)
Fix quality
[`MBart`] Fix cross attention mask check (#21730)
Apply ruff flake8-comprehensions (#21694)
Time series transformer: input projection and Std scaler (#21020)
Adding type hints to call() functions in this file (#21548)
Adding task guides to resources (#21704)
Fix TVLT (torch device issue) (#21710)
Fix `get_class_in_module` (#21709)
Fix typo in `PROCESSOR_MAPPING_NAMES` and add tests (#21703)
remove position ids and token type ids from forward args in docstring (#21701)
Fix axial positional encoding calculations for  reformer.mdx (#21649)
Add WhisperTokenizerFast (#21222)
Pass along revision in dynamic code fetch (#21698)
Fix-rag-finetune-project-requirement (#21697)
Add EfficientNet (#21563)
[`bnb`] fix `bnb` decoders bug (#21688)
add GPTSAN model (reopen) (#21291)
Fix quality
Fix for non-contiguous label tensors in VisonEncoderDecoder (#21582)
add flax whisper implementation (#20479)
Enable PyTorch/XLA Fully Sharded Data Parallel (FSDP) (#21406)
Fix dynamic module import error (#21646)
[`BLIP`] update blip path on slow tests (#21476)
[`ImageProcessor`] Refactor default `mean` & `std` to `OPENAI_CLIP_MEAN` & `OPENAI_CLIP_STD` (#21425)
Generate: eta sampling numerical stability (#21676)
Fix multi-gpu training error for LayoutLMv2 (#21675)
[`CLAP`] Fix few broken things (#21670)
[`bnb`] Introducing `BitsAndBytesConfig` (#21579)
Adapt PerceiverIO Multimodal class to work with arbitrary modalities (#20054)
[CLAP] Add CLAP to the library (#21370)
Sort deps alphabetically
Add OPT resources to the transformers documentation (#21625)
[bloom] gradient_checkpointing fix (#21655)
refactor: Make direct_transformers_import util (#21652)
[WhisperModel] fix bug in reshaping labels (#21653)
Bump werkzeug from 2.0.3 to 2.2.3 in /examples/research_projects/decision_transformer (#21658)
Update document of WhisperDecoderLayer (#21621)
[WIP] Move X-MOD models to facebook organization (#21640)
Fix typos in contrastive-image-text example README (#21665)
Update deprecated load_module (#21651)
Generate: PT Dynamo without graph breaks in the main greedy/sample loop (#21648)
Refactor model summary (#21408)
Add TVLT (#20725)
Pass parent exception as context exception to provide clearer stack trace (#21636)
Skipping more high mem tests - Wav2Vec2 Hubert (#21647)
Add Ernie-M Model to huggingface (#21349)
Fix passing kwargs to TFBertTokenizer (#21619)
Skip wav2vec2 hubert high mem tests (#21643)
Fix Blip-2 CI again (#21637)
Remove extra "`max_length` is reached." from InfNaNLogitsProcessor documentation (#21634)
fix: Race Condition when using Sagemaker Checkpointing and Model Repository (#21614)
Fix typo in QA task guide (#21608)
Error (also in original) model, scaling only q matrix not qk.T dot product (qk.T/sqrt(dim_per_head)) (#21627)
Fix typo in documentation. (#21632)
Removes duplicate computations in DETR post processing (#21592)
Fix generation config for empty state dict (#21630)
Fix the real failing test
Remove Niels from templates (#21564)
Final cleanup of TOKENIZER_FOR_DOC (#21565)
Skip failing test
Generate: input expansion for any model input (#21624)
Generate: filter encoder inputs when its signature does not accept wildcards (#21603)
Enable `requires_grad` on input embedding to train on top of frozen layers (#21598)
Add in big model inference to issue template (#21611)
Fix TF CTC tests (#21606)
Fix env. variable type issue in testing (#21609)
Clarify available pipelines in quicktour (#21607)
[deepspeed] performance docs (#21573)
Update setup.py (#21584)
[i18n-fr] Translate quicktour page to French (#21589)
Generate: correct default model input creation for decoder-only models (#21580)
Fix Blip-2 CI (#21595)
Add missing arguemtn to run_clip.py (#21588)
Correct Markdown bullets indentation (#21583)
Bump ipython from 8.1.1 to 8.10.0 in /examples/research_projects/decision_transformer (#21577)
annotated TFvisionEncoderDecoder input type hints (#21432)
[`bnb`] Let's make the daily CI green üçè  (#21597)
Generate: Fix flaky indexing error in `test_constrained_beam_search_generate_dict_output` (#21561)
Add `inputs_embeds` support when generating with GPT-J (#21575)
[MINOR] Fix link in timeseries transformer docs (#21602)
Remove trailing 'extractive' word from en documentation (#21594)
CI: skip failing TF hubert test (#21601)
Add: document question answering task guide (#21518)
Generate: TF supports multiple eos tokens (#21571)
Fix quality on main (ruff release)
[`Blip2`] Add int8 support for `blip2-flan-t5-xxl`  (#21574)
Remove more unused attributes in config classes (#21543)
Added timesformer configuration (#21446)
Replace input_values_processing with unpack_inputs (#21502)
improving contributing tests section (#21569)
[deepspeed] deal with models w/o `config.hidden_size` (#21504)
Goodbye to Blip-2 doctests (#21566)
[Tasks] Adds image captioning  (#21512)
[from_pretrained] extend `torch_dtype="auto"` to look up `config.torch_dtype` first, expand docs (#21524)
[Tests] Improve flax test_attention_outputs (#21486)
Add _mp_fn to run_mae.py for XLA testing (#21551)
[Variant] Make sure variant files are not incorrectly deleted (#21562)
Replace inefficient torch.sqrt taking scalar input with numpy.sqrt (#21496)
Add X-MOD (#20939)
Fix stuff related to the causal_mask in CodeGen. (#21527)
Remove CLI spams with Whisper FeatureExtractor (#21267)
adding a tip for deepspeed integration in multi-node environment (#21459)
Added with torch.no_grad() to Camembert integration test (#21544)
[`pipeline`] A simple fix for half-precision & 8bit models  (#21479)
Skip failing test for now
Added with torch.no_grad() to XLM-Roberta integration test  (#21547)
üö®üö®üö® Enforce single model initialization (#21431)
Fix from_pretrained API with config and state_dict (#21542)
Fix inclusion of non py files in package (#21546)
Align BLIP-2 winit with others
Add BLIP-2 (#21441)
fix typo in  run_speech_recognition_ctc.py (#21528)
Tag tests as slow ‚åõ (#21537)
Fix ClearML Integration to run in ClearML pipelines and external Tasks. (#21531)
Fix missing unfinished_sequences (#21529)
Generate: TF `.generate()` can now be exported with dynamic length (#21474)
Generate: make TF `.generate()` signature == PT `.generate()` signature  (#21525)
Add `__len__` method to `_LazyAutoMapping`  (#21522)
Fix multiple `eos_token_id`s in model.generate(...) (#21461)
Fixing backward compatiblity `image_processor` in pipeline. (#21513)
[tests] add missing `report_to none` (#21505)
Update OPT conversion script to work for OPT-IML (#21519)
no more dummies for speech processors (#21517)
Generate: TF `compute_transition_scores` (#21341)
[Doc] Minor URL fixes in PyTorch Text Classification Readme (#21511)
Bump cryptography from 36.0.2 to 39.0.1 in /examples/research_projects/decision_transformer (#21507)
Exclude the madeup words from M2M100Tokenizer.vocab_size (#20976)
Wrap RemBert integration test forward passes with torch.no_grad() (#21503)
Fix import in Accelerate for find_exec_bs (#21501)
Check for mapping/dict in distributed_concat function (#21500)
Add XLM-V to Model Doc (#21498)
Add inverse sqrt learning rate scheduler (#21495)
[tokenizer] sanitize saved config (#21483)
Cleanup quality (#21493)
Add limit_all_gathers option to fsdp_config and fix forward_prefetch bug (#21489)
A new test to check config attributes being used (#21453)
[OPT] Adds `GPT2TokenizerFast` to the list of tokenizer to use for OPT. (#20823)
Sanity check the type of id2label and label2id arguments of from_pretrained for TokenClassification models (#21490)
Typos/fixes to link syntax (#21450)
:pen: fix typo in pytorch semantic segmentation readme (#21492)
changed "ot" to "to" (#21488)
[`Doc`] Fix int8 docs (#21487)
Generate: TF can now generate from embeddings in encoder-decoder models (#21475)
[CI ] Remove `past` in favor of `pat_key_values` (#21443)
Deprecate parallelize API (#21448)
Fix epoch number when resuming training (#21478)
Bump oauthlib from 3.2.1 to 3.2.2 in /examples/research_projects/decision_transformer (#21481)
Update quality tooling for formatting (#21480)
Add tips for generation with Int8 models (#21424)
OPT: BLIP2-ready `prepare_inputs_for_generation` (#21477)
[i18n-fr] Translate index page to French (#21458)
[examples] improve block_size warning message (#21463)
Removing `more_itertools` dependency. (#21473)
Generate: TF can now accept custom logits processors (#21454)
make SpeechT5 doc examples deterministic (#21470)
Fixed RAG script which was failing on dummy example (#21416)
Fix `PushToHubCallback` import in Share a model docs (#21457)
Added documentation for DagsHubCallback (#21452)
Add perf numbers for perf_train_cpu (#20974)
Fix `SpeechT5ForSpeechToSpeechIntegrationTests` device issue (#21460)
Avoid flaky generation sampling tests (#21445)
For IterableDataset, return DataLoader using self._train_batch_size. ‚Ä¶ (#21447)
Add tutorial doc for TF + TPU (#21429)
exclude deleted files in the fixup script (#21436)
[WIP] add SpeechT5 model (#18922)
do not scale gradient in bf16 mode (#21428)
Fix device issue in a `ConvBertModelTest` test (#21438)
Added model resources for LayoutLM Issue#19848 (#21377)
Remove more unused attributes in config classes (#21392)
Add `inputs_embeds` support for `.generate()` with BLOOM models (#21430)
üö®üö® Generate: standardize beam search behavior across frameworks (#21368)
Add VQGAN-CLIP research project (#21329)
Update task summary  (#21067)
Fixes bug in the creation of ExponentialDecayLengthPenalty (#21423)
Fix task guide formatting (#21409)
Fix some pipeline tests (#21401)
Allow to add more information in `is_flaky` (#21426)
[`bnb`] Fine-tuning HF 8-bit models (#21290)
Fix Graphormer test suite (#21419)
Add the GeLU activation from pytorch with the tanh approximation (#21345)
Add distinct section names for PyTorch and TF (#21422)
Fix image_processor_class bug (#21410)
Use torch `1.13.1` in push/schedule CI (#21421)
Generate: decoder-only models can generate with `inputs_embeds` (#21405)
Add TF image classification example script (#19956)
Added DagshubCallback (#21404)
Skip batches fast with accelerate (#21390)
Fix the issue of using only inputs_embeds in convbert model (#21398)
Moved LiLT under multimodal models in TOC (#21393)
Add variant to transformers (#21332)
Update `Graphormer` and fix its `torchscript` test failures (#21380)
Generate: fix TF XLA tests on models with `max_position_embeddings` or `max_target_positions` (#21389)
Remove more unused attributes in config classes (#21327)
Add support of backward_prefetch and forward_prefetch (#21237)
Simplify column_names in run_clm/mlm (#21382)
[Docs] Minor fixes (#21383)
Do not log the generation config for each prediction step in TrainerSeq2Seq (#21385)
Cleanup the usage of `layer_norm_eps` in some models (#21336)
Template for framework-agnostic tests (#21348)
Add DETA (#20983)
[`run_(clm|mlm).py` examples] add streaming dataset support (#21343)
translate index to zh(#20095) (#21351)
Adding resource section to GPT-J docs (#21270)
Fixes path for Graphormer checkpoint (#21367)
Generate: Relaxed `max_length` and `max_new_tokens` coexistence (#21347)
Add cPython files in build (#21372)
Fix DETR tests after #21144 (#21365)
Remove duplicate declarations in dummy inputs for TFLongformer (#21352)
Corrected (#21350)
fix the issue that the output dict of jit model could not get [0] (#21354)
Pipeline testing - using tiny models on Hub (#20426)
Fix `GitModelIntegrationTest.test_batched_generation` device issue (#21362)
Automated compatible models list for task guides (#21338)
Little cleanup: let huggingface_hub manage token retrieval (#21333)
[Whisper] another patch (#21324)
Fix `RobertaPreLayerNorm` doctest (#21337)
Bump onnx from 1.11.0 to 1.13.0 in /examples/research_projects/decision_transformer (#21331)
Fix M2M100 positional embedding creation for ONNX (#21328)
Update Hebrew language code to he per IANA registry (#21310)
[Doctest] Fix `Perceiver` doctest (#21318)
Generate: better `compute_transition_scores` examples (#21323)
Fix `TFEncoderDecoder` tests (#21301)
check paths in `utils/documentation_tests.txt` (#21315)
Small QoL for qa. (#21316)
[i18n-KO] Translated quicktour page to Korean (#20946)
Fix 2 paths in the doctest list (#21314)
Use `model_class.__name__` and compare against `XXX_MAPPING_NAMES` (#21304)
Accept batched tensor of images as input to image processor (#21144)
[WHISPER] Small patch (#21307)
Small fix to ExponentialDecayLengthPenalty docstring (#21308)
Add BridgeTower model (#20775)
[CI-Daily] replace `past` in prepare inputs for generation (#21296)
Documentation code sample fixes (#21302)
[Doctest] Fix `Blenderbot` doctest (#21297)
Update `OneFormerModelIntegrationTest` expected values (#21295)
[Hubert] Fix Hubert processing auto (#21299)
Fix `EfficientFormer` (#21294)
Moving to cleaner tokenizer version or `oneformer`. (#21292)
[Whisper] Refactor whisper (#21252)
[Mask2Former] Add doc tests (#21232)
Supporting `ImageProcessor` in place of `FeatureExtractor` for pipelines (#20851)
[GIT] Add test for batched generation (#21282)
Update expected values for doctest (#21284)
Fix `TrainingArguments.label_names` docs to reflect the correct default value behaviour (#21288)
[W2V2 with LM] Fix decoder test with params (#21277)
[GenerationConfig] add additional kwargs handling (#21269)
[examples/deepspeed] fix renamed api (#21283)
[`t5`] Fix T5 inference in `float16` + `bnb` error (#21281)
Fix MaskFormerImageProcessor.post_process_instance_segmentation (#21256)
Use `logger.info` instead of `print` to emit a logging message in `hub.py` (#21273)
Hotifx remove tuple for git config image processor. (#21278)
Use return_tensors="np" instead of "tf" (#21266)
[Doc] fix broken link (#21276)
Skip `test_multi_gpu_data_parallel_forward` for `UperNetModelTest` (#21216)
v4.27.0.dev0
Models docstring (#21225)
Supported pipeline tasks update (#21268)
[Whisper] fix all issues with unk token (#21250)
Add class properties with warnings (#21195)
[ci-daily] Fix pipeline tests (#21257)
Add: TensorFlow example for semantic segmentation task guide (#21223)
Notebook examples grouping and update (#21265)
Update tests: replace feature extractor tests with image processor (#20768)
Replace reduce_labels with do_reduce_labels (#21218)
Generate: save generation config with the models' `.save_pretrained()` (#21264)
Add missing checkpoint for doctest (#21258)
Add scikit-learn dependency to train langage-modeling (#21229)
Add Japanese translation installation.mdx (#21241)
Fix reformer CI (#21254)
Optimize by not computing gradients for parameters set to requires_grad=False (#21236)
[GIT] Convert more checkpoints (#21245)
Add test_image_processing_common.py (#20785)
Extend Script to enable conversion of Encoder Only  T5x Models to Pytorch (#20907)
[DETR and friends] Use AutoBackbone as alternative to timm (#20833)
Generate: precision fix in compute_transition_scores doctests (#21251)
[`BLIP`] fix doctest (#21217)
Skip failing test for now (#21226)
[`BLIP`] fix docstring for `BlipTextxxx` (#21224)
Microphone live inference catching up when inference is too slow (whisper). (#21219)
Remove all hf-internal-testing checkpoints that can be removed (#21199)
Fix task summary doctest  (#21200)
Fix OneFormer Docstrings (#21215)
Make `parallelism` for CircleCI jobs work - but keep it `1` for now (#21157)
Fix code example in training tutorial (#21201)
Declare __len__ method in PreTrainedTokenizerBase (#21210)
Fix `GPTJ` doctest (#21213)
Fix `CONFIG_ARCHIVE_MAP_MAPPING_NAMES` (#21207)
Update `huggingface_hub` version (#21212)
deleted references of self.vocab_size and self.type_vocab_size for multiple models [TF implementation] (#21164)
Generate: documented function to compute the transition scores (#21191)
Update modeling doc strings FE -> IP (#21106)
[Whispe]  Fix pipeline after timestamp merges (#21198)
Enabling live `automatic-speech-recognition` asr for Whisper. (#21196)
Efficientformer (#20459)
Add disclaimer for necessary fake models (#21178)
Graphormer model for Graph Classification (#20968)
revert Copyright 2023
Add Japanese translation index.mdx (#21186)
Flax dtype-dependent numerical masking (#21197)
[`CVT`] Fix module initialization issue (#21193)
Add hallucination filter (#18675)
[Whisper] Fix timestamp processor (#21187)
hertz is already per second (#21188)
Update examples with image processors (#21155)
Rename GLPN image processor tests (#21194)
Updates to computer vision section of the Preprocess doc (#21181)
Fix device issue in `UperNetModelIntegrationTest` (#21192)
Trigger CI
workaround documentation rendering bug (#21189)
Update year 2020 to 2023 in one file (#21190)
Fix `Mask2FormerForUniversalSegmentation` (#21175)
Add OneFormer Model (#20577)
[issues template] update deepspeed owners (#21027)
Rewrite a couple of lines in the TF XLA doc (#21177)
Add AWS Neuron torchrun support (#20806)
Bump future from 0.18.2 to 0.18.3 in /examples/research_projects/visual_bert (#21173)
Bump future from 0.18.2 to 0.18.3 in /examples/research_projects/lxmert (#21169)
Adapt repository creation to latest hf_hub (#21158)
Fix doctest CI (#21166)
using raw string for regex to search <extra_id> (#21162)
fix the issue that the output dict of jit model could not get [:2] (#21146)
Fix git model for generate with beam search. (#21071)
OPT: Fix batched generation with FLAX (#21150)
Fix typos in documentation (#21160)
Remove Roberta Dependencies from XLM Roberta Flax and Tensorflow models (#21047)
`blip` support for training (#21021)
Make `test_save_pretrained_signatures` slow test (#21105)
Add Japanese translation to multilingual.mdx (#21084)
üåê [i18n-KO] Translated `installation.mdx` to Korean (#20948)
Fixed num_channels!=3 normalization training (#20630)
Add Epsilon- and Eta-Sampling (#21121)
Refactoring of the text generate API docs (#21112)
Add: An introductory guide for text generation (#21090)
Add: tensorflow example for image classification task guide (#21038)
Add resources (#20872)
CLI: update hub PR URL (#21154)
Change variable name to prevent shadowing (#21153)
Add batch of resources (#20647)
Whisper Timestamp processor and prediction (#20620)
Fixing offline mode for pipeline (when inferring task). (#21113)
Clarify and add missing typical_p argument docstring. (#21095)
feat: add standalone guide on XLA support. (#21141)
Small simplification to TopKLogitsWarper (#21130)
Rename test_feature_extraction files (#21140)
Generate: TF contrastive search must pop `use_cache` from `model_kwargs` (#21149)
TF: serializable hubert (#20966)
Fixes to TF collators (#21143)
Add Mask2Former (#20792)
[GIT] Fix training (#21133)
Update `TFTapasEmbeddings` (#21107)
Added clefourrier as ref point for graph models in bug reports (#21139)
Fix `RealmModelIntegrationTest.test_inference_open_qa` (#21136)
Fixed issue #21053 (#21065)
Fixing batching pipelines on single items for ChunkPipeline (#21132)
Add `min_new_tokens` argument in generate() (implementation based on `MinNewTokensLengthLogitsProcessor`) (#21044)
[LongT5] Remove duplicate encoder_attention_mask default value check (#21124)
[VideoMAE] Fix docstring (#21111)
Add UperNet (#20648)
Fixed typo in docstring (#21115)
Use raw string for regex in tokenization_t5_fast.py (#21125)
[CI-doc-daily] Remove RobertaPreLayernorm random tests (#20992)
Rework automatic code samples in docstrings (#20757)
Add Spanish translation to community.mdx (#21055)
Update task summary part 1 (#21014)
[Tokenizers] Fix a small typo (#21104)
Fix `torchscript` tests for `AltCLIP` (#21102)
Fix past CI (#20967)
[bnb optim] fixing test (#21030)
Remove more unused attributes in config classes (#21000)
Fixed issue #21039 (#21062)
Optimize inference only mode memory if ipex is used (#21083)
fix typo in comment (#21088)
Update docstring for CLIPConfig (#21066)
Fix header level (#21072)
feature: update wandb callback to upload checkpoints (#21035)
Make the attention_head_size in distilbert an object attribute (#20970)
Patch-past-refactor (#21050)
remove flax file from `documentation_tests.txt` (#21036)
Fix warning for MCTC model (#21049)
Skip failing test until Athur looks at it.
Replace `past` with `past_key_values` (#20944)
fix typo (#21048)
fix typo (#21042)
fix levit timm conversion file (#20938)
fix parameter name in docstring (#21032)
Support turning off the model uploading in ClearML (#20969)
Fix arguments passed to predict function in QA Seq2seq training script (#21026)
[NumPy] Remove references to deprecated NumPy type aliases (#21022)
Added mask_time_prob and mask_time_length arguments to wav2vec2 pretraining script (#20985)
Generate: FLAX uses `GenerationConfig` as the basis for `.generate()` parametrization (#21007)
[CLIPSeg] Fix integration test (#20995)
Make sure dynamic objects can be saved and reloaded (#21008)
[`BLIP`] Fix daily CI failing test (#20877)
Generate: FLAX infers pad token in its absence and has functional example (#21009)
Generate: post-generate config TF doctest fix  (#21018)
Fix callback docstrings (#21005)
Bump gitpython from 3.0.2 to 3.1.30 in /examples/research_projects/distillation (#21011)
Bump gitpython from 3.1.18 to 3.1.30 in /examples/research_projects/decision_transformer (#21010)
Fix (DeepSpeed) docker image build issue (#21002)
Generate: Fix CI related to #20727  (#21003)
add: task guide on video classification model fine-tuning. (#20827)
Update PR template (#21006)
Fix repo consistency
Remove T5 dependency from mT5 model (#20949)
Update bug report template (#21004)
Generate: TF uses `GenerationConfig` as the basis for `.generate()` parametrization (#20994)
Refactor the function get_results (#20999)
Fix model hub link (#20998)
Don't call deprecated method (#20904)
Fix bug in segmentation postprocessing (#20198)
Update image processor parameters if creating with kwargs (#20866)
auxiliary_loss works for Deformable Detr (#20959)
Add: doc page for the object detection task (#20925)
update template (#20885)
Add AltCLIP (#20446)
Add custom stop token ids for generation (#20727)
Fix race condition on cleaning checkpoints when save_total_limit set to 1 (#20989)
Improve OWL-ViT postprocessing (#20980)
Fix for LXMERT (#20986)
Avoid CI runs under users' own CircleCI personal account (#20981)
Ignore errors when deleting old checkpoints in trainer (#20984)
Enable `decoder_attention_mask` in `generate` function (#20726)
Fix valid ratio for Deformable Detr (#20958)
[run_clm example] add torch_dtype option for model load. (#20971)
Remove more unused attributes in config classes (#20858)
Add GIT (GenerativeImage2Text) (#20295)
Fix post_process_object_detection method descriptions (#20977)
`MinNewTokensLengthLogitsProcessor` for `.generate` method #20814 (#20892)
Generate: delete unused TF `_reorder_cache` (#20964)
Fix T5 docstring (#20957)
Generate: TF XLA beam sample (#20927)